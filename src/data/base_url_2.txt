URL: https://tutorial.gigalogy.com/
Skip to content
Gigalogy Tutorial
Overview
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
Table of contents
Tutorial
API Reference
Overview

The Gigalogy AI platform (GAIP) introduces cutting-edge solutions, designed to craft personalized product offerings. These solutions address specific user needs while propelling business growth. The platform provides solutions for personalization, Custom GPT creation and CV based solution. Within this documentation, you will discover comprehensive guidance on integrating the solutions, understanding its operational framework, and leveraging its capabilities within your application.

Let's have a rundown of the documentation:

Tutorial

To seamlessly integrate your product with our solutions, this section serves as your comprehensive guide. It provides a detailed, step-by-step walkthrough for integrating an API with our solutions. Additionally, you'll gain insights into how the solution function and discover strategies for effectively building your application with it. For an optimal integration process, it is advisable to thoroughly review this section prior to integrating your product.

Get started

API Reference

This section offers developer-friendly API documentation designed to guide you through accessing and utilizing the recommender API effectively.

API Reference

Next
Account and Project creation
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/#overview
Skip to content
Gigalogy Tutorial
Overview
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
Table of contents
Tutorial
API Reference
Overview

The Gigalogy AI platform (GAIP) introduces cutting-edge solutions, designed to craft personalized product offerings. These solutions address specific user needs while propelling business growth. The platform provides solutions for personalization, Custom GPT creation and CV based solution. Within this documentation, you will discover comprehensive guidance on integrating the solutions, understanding its operational framework, and leveraging its capabilities within your application.

Let's have a rundown of the documentation:

Tutorial

To seamlessly integrate your product with our solutions, this section serves as your comprehensive guide. It provides a detailed, step-by-step walkthrough for integrating an API with our solutions. Additionally, you'll gain insights into how the solution function and discover strategies for effectively building your application with it. For an optimal integration process, it is advisable to thoroughly review this section prior to integrating your product.

Get started

API Reference

This section offers developer-friendly API documentation designed to guide you through accessing and utilizing the recommender API effectively.

API Reference

Next
Account and Project creation
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/en/
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/ja/
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/00_account_project_creation/
Skip to content
Gigalogy Tutorial
Account and Project creation
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
Table of contents
Account creation
Project creation
Account and Project creation
Account creation

Go to platform.gigalogy.com. Click on Get started on the top right and follow the instructions to create an account.

After account creation, you will receive an Email for account verification. Click the link in the Email to complete the account verification.

Note: If you don't see the email, check your spam folder or contact support.

After creating account and logging in, you will see the solutions segments on the left side navigation bar, and detail solutions under those segments.

Project creation

From this page, click on the button Create a project to create your first project.

You will need to fill in the following details:

Project name: Can be anything, recommended to use something easier to track later.

Language: Can be english or Japanese, based on the content of your catalog information

Industry: Select the industry the project is intended for.

Enabled solutions: Select the solutions you want to use.

Next, you will see a confirmation page, check the details and click and your project creation is completed.

In projects page, now you can see the project you just created.

Click on the project to see the project Detail, Settings and Insights page.

If it is your first project, this will be a trial project. The trial period is 90 days. In the project detail page, you can see the susbscription status, and related information. If you wish to end the trial period and convert to standard plan, click the button at the bottom of the page to request the change. Please note this cannot be reversed

If it is not your first project, this will be a standard project. In the project detail page, you can see the susbscription status, and the API rate limits for the project. If you wish to increase the API rate limits, please contact us at support@gigalogy.com.

Previous
Overview
Next
Credentials
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/00_account_project_creation/#account-creation
Skip to content
Gigalogy Tutorial
Account and Project creation
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
Table of contents
Account creation
Project creation
Account and Project creation
Account creation

Go to platform.gigalogy.com. Click on Get started on the top right and follow the instructions to create an account.

After account creation, you will receive an Email for account verification. Click the link in the Email to complete the account verification.

Note: If you don't see the email, check your spam folder or contact support.

After creating account and logging in, you will see the solutions segments on the left side navigation bar, and detail solutions under those segments.

Project creation

From this page, click on the button Create a project to create your first project.

You will need to fill in the following details:

Project name: Can be anything, recommended to use something easier to track later.

Language: Can be english or Japanese, based on the content of your catalog information

Industry: Select the industry the project is intended for.

Enabled solutions: Select the solutions you want to use.

Next, you will see a confirmation page, check the details and click and your project creation is completed.

In projects page, now you can see the project you just created.

Click on the project to see the project Detail, Settings and Insights page.

If it is your first project, this will be a trial project. The trial period is 90 days. In the project detail page, you can see the susbscription status, and related information. If you wish to end the trial period and convert to standard plan, click the button at the bottom of the page to request the change. Please note this cannot be reversed

If it is not your first project, this will be a standard project. In the project detail page, you can see the susbscription status, and the API rate limits for the project. If you wish to increase the API rate limits, please contact us at support@gigalogy.com.

Previous
Overview
Next
Credentials
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/00_account_project_creation/#project-creation
Skip to content
Gigalogy Tutorial
Account and Project creation
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
Table of contents
Account creation
Project creation
Account and Project creation
Account creation

Go to platform.gigalogy.com. Click on Get started on the top right and follow the instructions to create an account.

After account creation, you will receive an Email for account verification. Click the link in the Email to complete the account verification.

Note: If you don't see the email, check your spam folder or contact support.

After creating account and logging in, you will see the solutions segments on the left side navigation bar, and detail solutions under those segments.

Project creation

From this page, click on the button Create a project to create your first project.

You will need to fill in the following details:

Project name: Can be anything, recommended to use something easier to track later.

Language: Can be english or Japanese, based on the content of your catalog information

Industry: Select the industry the project is intended for.

Enabled solutions: Select the solutions you want to use.

Next, you will see a confirmation page, check the details and click and your project creation is completed.

In projects page, now you can see the project you just created.

Click on the project to see the project Detail, Settings and Insights page.

If it is your first project, this will be a trial project. The trial period is 90 days. In the project detail page, you can see the susbscription status, and related information. If you wish to end the trial period and convert to standard plan, click the button at the bottom of the page to request the change. Please note this cannot be reversed

If it is not your first project, this will be a standard project. In the project detail page, you can see the susbscription status, and the API rate limits for the project. If you wish to increase the API rate limits, please contact us at support@gigalogy.com.

Previous
Overview
Next
Credentials
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/01_Credential/
Skip to content
Gigalogy Tutorial
Credentials
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
Table of contents
Create credentials
Credentials

After project is created, you can create credentials for your project. Multiple credentials can be created for a project, depending on your organization's needs.

There are two types of credentials.

Project key and API key: Used for server to server communication.
Client key: Used for client to server communication.

We will dive deeper in the use cases of two types of key in the integration steps.

Note

Please note that Gigalogy does not keep a copy of your API key, if you lose the key, create a new credential and Stop the credential with lost key.

Create credentials

Navigate to your project Settings page. (Got to your project and click on SETTINGS tab).

Click on the blue “Plus” (⊕) button on the right to create credentials. Follow the on-screen instructions to create the credentials. Notice that you will have option to choose the Key Type, as mentioned above. For Client key, you can optionally limit the domain of the request origin for extra security.

Save your credentials carefully.

Now you can start setting up the project with your data, test and integrate the endpoints.

Previous
Account and Project creation
Next
Project setup
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/01_Credential/#create-credentials
Skip to content
Gigalogy Tutorial
Credentials
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
Table of contents
Create credentials
Credentials

After project is created, you can create credentials for your project. Multiple credentials can be created for a project, depending on your organization's needs.

There are two types of credentials.

Project key and API key: Used for server to server communication.
Client key: Used for client to server communication.

We will dive deeper in the use cases of two types of key in the integration steps.

Note

Please note that Gigalogy does not keep a copy of your API key, if you lose the key, create a new credential and Stop the credential with lost key.

Create credentials

Navigate to your project Settings page. (Got to your project and click on SETTINGS tab).

Click on the blue “Plus” (⊕) button on the right to create credentials. Follow the on-screen instructions to create the credentials. Notice that you will have option to choose the Key Type, as mentioned above. For Client key, you can optionally limit the domain of the request origin for extra security.

Save your credentials carefully.

Now you can start setting up the project with your data, test and integrate the endpoints.

Previous
Account and Project creation
Next
Project setup
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/project_setup_from_ui/
Skip to content
Gigalogy Tutorial
Project setup
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Project setup from UI
Upload data
Mapping your data
Rank Setting
Setup completion
Project setup

Once the project is created, now it is time to setup the project.

In the coming sections, we will guide you through various steps such as "data mapping" and "creating indexes" (found under the Environment Setup section), "integrating data" (explained in the Integration of Catalogue Information and User Behavior Data section), and "training your data" (in the Training Your Data section). These sections will cover you how to perform these actions by directly using our endpoints from the sandbox.

In this section, we will show you how to complete all these steps directly through our platform UI.

Project setup from UI
Upload data

After creating project, go inside the project. At the top, there will be four tabs DETAILS, SETTINGS, INSIGHTS, SETUP. Click and go to the SETUP. Here you will find a box to upload your data file. Currently we accept CSV files only. Select your CSV file and click upload.

Mapping your data

In the next page, you will see a suggested mapping done for your data. Gigalogy personalization engine relies on specific default keys to operate. To integrate your item catalogue with our solution, it's essential to map you data source keys (such as item name, item description, tags, ingredients, category, etc.) with keys of Gigalogy personalization engine. Our personalizer then can understand your data.

You can update the suggested mapping as you find necessary. This can also be modified later. When satisfied, click continue to proceed to the next step

Rank Setting

In the Rank settings step, you can modify and set the settings for the rank training, which will impact the "trending items" recommendation. If you are not sure about this, we highly recommend to keep it as it is (Default settings). Click continue to proceed to the next step.

Setup completion

In the next page you will see the status of the project setup. If there is any issue, it should give you the reason and you will be asked to restart the process. Please contact us if you fail to resolve the issue at support@gigalogy.com.

If all above steps are completed successfully, your project setup is done.

Next go to the SETTINGS tab, and you will find the section "Train data". Hit the "Train" button for each (Item, image, rank) to train your data. This is recommended to do once when you first setup your project. After this, it will run automatically every 3 hours (Default setting is 3 hours, can be changed based on requirement).

Below you will find the training history with their status. You can update the rank settings and mapper.

If you want to re-upload your data, go to the DATASET tab again to re-upload. This will not replace your old data, only add new data.

Previous
Credentials
Next
Environment setup
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/project_setup_from_ui/#project-setup-from-ui
Skip to content
Gigalogy Tutorial
Project setup
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Project setup from UI
Upload data
Mapping your data
Rank Setting
Setup completion
Project setup

Once the project is created, now it is time to setup the project.

In the coming sections, we will guide you through various steps such as "data mapping" and "creating indexes" (found under the Environment Setup section), "integrating data" (explained in the Integration of Catalogue Information and User Behavior Data section), and "training your data" (in the Training Your Data section). These sections will cover you how to perform these actions by directly using our endpoints from the sandbox.

In this section, we will show you how to complete all these steps directly through our platform UI.

Project setup from UI
Upload data

After creating project, go inside the project. At the top, there will be four tabs DETAILS, SETTINGS, INSIGHTS, SETUP. Click and go to the SETUP. Here you will find a box to upload your data file. Currently we accept CSV files only. Select your CSV file and click upload.

Mapping your data

In the next page, you will see a suggested mapping done for your data. Gigalogy personalization engine relies on specific default keys to operate. To integrate your item catalogue with our solution, it's essential to map you data source keys (such as item name, item description, tags, ingredients, category, etc.) with keys of Gigalogy personalization engine. Our personalizer then can understand your data.

You can update the suggested mapping as you find necessary. This can also be modified later. When satisfied, click continue to proceed to the next step

Rank Setting

In the Rank settings step, you can modify and set the settings for the rank training, which will impact the "trending items" recommendation. If you are not sure about this, we highly recommend to keep it as it is (Default settings). Click continue to proceed to the next step.

Setup completion

In the next page you will see the status of the project setup. If there is any issue, it should give you the reason and you will be asked to restart the process. Please contact us if you fail to resolve the issue at support@gigalogy.com.

If all above steps are completed successfully, your project setup is done.

Next go to the SETTINGS tab, and you will find the section "Train data". Hit the "Train" button for each (Item, image, rank) to train your data. This is recommended to do once when you first setup your project. After this, it will run automatically every 3 hours (Default setting is 3 hours, can be changed based on requirement).

Below you will find the training history with their status. You can update the rank settings and mapper.

If you want to re-upload your data, go to the DATASET tab again to re-upload. This will not replace your old data, only add new data.

Previous
Credentials
Next
Environment setup
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/project_setup_from_ui/#upload-data
Skip to content
Gigalogy Tutorial
Project setup
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Project setup from UI
Upload data
Mapping your data
Rank Setting
Setup completion
Project setup

Once the project is created, now it is time to setup the project.

In the coming sections, we will guide you through various steps such as "data mapping" and "creating indexes" (found under the Environment Setup section), "integrating data" (explained in the Integration of Catalogue Information and User Behavior Data section), and "training your data" (in the Training Your Data section). These sections will cover you how to perform these actions by directly using our endpoints from the sandbox.

In this section, we will show you how to complete all these steps directly through our platform UI.

Project setup from UI
Upload data

After creating project, go inside the project. At the top, there will be four tabs DETAILS, SETTINGS, INSIGHTS, SETUP. Click and go to the SETUP. Here you will find a box to upload your data file. Currently we accept CSV files only. Select your CSV file and click upload.

Mapping your data

In the next page, you will see a suggested mapping done for your data. Gigalogy personalization engine relies on specific default keys to operate. To integrate your item catalogue with our solution, it's essential to map you data source keys (such as item name, item description, tags, ingredients, category, etc.) with keys of Gigalogy personalization engine. Our personalizer then can understand your data.

You can update the suggested mapping as you find necessary. This can also be modified later. When satisfied, click continue to proceed to the next step

Rank Setting

In the Rank settings step, you can modify and set the settings for the rank training, which will impact the "trending items" recommendation. If you are not sure about this, we highly recommend to keep it as it is (Default settings). Click continue to proceed to the next step.

Setup completion

In the next page you will see the status of the project setup. If there is any issue, it should give you the reason and you will be asked to restart the process. Please contact us if you fail to resolve the issue at support@gigalogy.com.

If all above steps are completed successfully, your project setup is done.

Next go to the SETTINGS tab, and you will find the section "Train data". Hit the "Train" button for each (Item, image, rank) to train your data. This is recommended to do once when you first setup your project. After this, it will run automatically every 3 hours (Default setting is 3 hours, can be changed based on requirement).

Below you will find the training history with their status. You can update the rank settings and mapper.

If you want to re-upload your data, go to the DATASET tab again to re-upload. This will not replace your old data, only add new data.

Previous
Credentials
Next
Environment setup
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/project_setup_from_ui/#mapping-your-data
Skip to content
Gigalogy Tutorial
Project setup
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Project setup from UI
Upload data
Mapping your data
Rank Setting
Setup completion
Project setup

Once the project is created, now it is time to setup the project.

In the coming sections, we will guide you through various steps such as "data mapping" and "creating indexes" (found under the Environment Setup section), "integrating data" (explained in the Integration of Catalogue Information and User Behavior Data section), and "training your data" (in the Training Your Data section). These sections will cover you how to perform these actions by directly using our endpoints from the sandbox.

In this section, we will show you how to complete all these steps directly through our platform UI.

Project setup from UI
Upload data

After creating project, go inside the project. At the top, there will be four tabs DETAILS, SETTINGS, INSIGHTS, SETUP. Click and go to the SETUP. Here you will find a box to upload your data file. Currently we accept CSV files only. Select your CSV file and click upload.

Mapping your data

In the next page, you will see a suggested mapping done for your data. Gigalogy personalization engine relies on specific default keys to operate. To integrate your item catalogue with our solution, it's essential to map you data source keys (such as item name, item description, tags, ingredients, category, etc.) with keys of Gigalogy personalization engine. Our personalizer then can understand your data.

You can update the suggested mapping as you find necessary. This can also be modified later. When satisfied, click continue to proceed to the next step

Rank Setting

In the Rank settings step, you can modify and set the settings for the rank training, which will impact the "trending items" recommendation. If you are not sure about this, we highly recommend to keep it as it is (Default settings). Click continue to proceed to the next step.

Setup completion

In the next page you will see the status of the project setup. If there is any issue, it should give you the reason and you will be asked to restart the process. Please contact us if you fail to resolve the issue at support@gigalogy.com.

If all above steps are completed successfully, your project setup is done.

Next go to the SETTINGS tab, and you will find the section "Train data". Hit the "Train" button for each (Item, image, rank) to train your data. This is recommended to do once when you first setup your project. After this, it will run automatically every 3 hours (Default setting is 3 hours, can be changed based on requirement).

Below you will find the training history with their status. You can update the rank settings and mapper.

If you want to re-upload your data, go to the DATASET tab again to re-upload. This will not replace your old data, only add new data.

Previous
Credentials
Next
Environment setup
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/project_setup_from_ui/#rank-setting
Skip to content
Gigalogy Tutorial
Project setup
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Project setup from UI
Upload data
Mapping your data
Rank Setting
Setup completion
Project setup

Once the project is created, now it is time to setup the project.

In the coming sections, we will guide you through various steps such as "data mapping" and "creating indexes" (found under the Environment Setup section), "integrating data" (explained in the Integration of Catalogue Information and User Behavior Data section), and "training your data" (in the Training Your Data section). These sections will cover you how to perform these actions by directly using our endpoints from the sandbox.

In this section, we will show you how to complete all these steps directly through our platform UI.

Project setup from UI
Upload data

After creating project, go inside the project. At the top, there will be four tabs DETAILS, SETTINGS, INSIGHTS, SETUP. Click and go to the SETUP. Here you will find a box to upload your data file. Currently we accept CSV files only. Select your CSV file and click upload.

Mapping your data

In the next page, you will see a suggested mapping done for your data. Gigalogy personalization engine relies on specific default keys to operate. To integrate your item catalogue with our solution, it's essential to map you data source keys (such as item name, item description, tags, ingredients, category, etc.) with keys of Gigalogy personalization engine. Our personalizer then can understand your data.

You can update the suggested mapping as you find necessary. This can also be modified later. When satisfied, click continue to proceed to the next step

Rank Setting

In the Rank settings step, you can modify and set the settings for the rank training, which will impact the "trending items" recommendation. If you are not sure about this, we highly recommend to keep it as it is (Default settings). Click continue to proceed to the next step.

Setup completion

In the next page you will see the status of the project setup. If there is any issue, it should give you the reason and you will be asked to restart the process. Please contact us if you fail to resolve the issue at support@gigalogy.com.

If all above steps are completed successfully, your project setup is done.

Next go to the SETTINGS tab, and you will find the section "Train data". Hit the "Train" button for each (Item, image, rank) to train your data. This is recommended to do once when you first setup your project. After this, it will run automatically every 3 hours (Default setting is 3 hours, can be changed based on requirement).

Below you will find the training history with their status. You can update the rank settings and mapper.

If you want to re-upload your data, go to the DATASET tab again to re-upload. This will not replace your old data, only add new data.

Previous
Credentials
Next
Environment setup
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/project_setup_from_ui/#setup-completion
Skip to content
Gigalogy Tutorial
Project setup
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Project setup from UI
Upload data
Mapping your data
Rank Setting
Setup completion
Project setup

Once the project is created, now it is time to setup the project.

In the coming sections, we will guide you through various steps such as "data mapping" and "creating indexes" (found under the Environment Setup section), "integrating data" (explained in the Integration of Catalogue Information and User Behavior Data section), and "training your data" (in the Training Your Data section). These sections will cover you how to perform these actions by directly using our endpoints from the sandbox.

In this section, we will show you how to complete all these steps directly through our platform UI.

Project setup from UI
Upload data

After creating project, go inside the project. At the top, there will be four tabs DETAILS, SETTINGS, INSIGHTS, SETUP. Click and go to the SETUP. Here you will find a box to upload your data file. Currently we accept CSV files only. Select your CSV file and click upload.

Mapping your data

In the next page, you will see a suggested mapping done for your data. Gigalogy personalization engine relies on specific default keys to operate. To integrate your item catalogue with our solution, it's essential to map you data source keys (such as item name, item description, tags, ingredients, category, etc.) with keys of Gigalogy personalization engine. Our personalizer then can understand your data.

You can update the suggested mapping as you find necessary. This can also be modified later. When satisfied, click continue to proceed to the next step

Rank Setting

In the Rank settings step, you can modify and set the settings for the rank training, which will impact the "trending items" recommendation. If you are not sure about this, we highly recommend to keep it as it is (Default settings). Click continue to proceed to the next step.

Setup completion

In the next page you will see the status of the project setup. If there is any issue, it should give you the reason and you will be asked to restart the process. Please contact us if you fail to resolve the issue at support@gigalogy.com.

If all above steps are completed successfully, your project setup is done.

Next go to the SETTINGS tab, and you will find the section "Train data". Hit the "Train" button for each (Item, image, rank) to train your data. This is recommended to do once when you first setup your project. After this, it will run automatically every 3 hours (Default setting is 3 hours, can be changed based on requirement).

Below you will find the training history with their status. You can update the rank settings and mapper.

If you want to re-upload your data, go to the DATASET tab again to re-upload. This will not replace your old data, only add new data.

Previous
Credentials
Next
Environment setup
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/02_environment_setup/
Skip to content
Gigalogy Tutorial
Environment setup
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Mapping creation
Item Catalogue Mapping
Example of this item mapper creation
Sample Code
User Behavior mapping
Index creation
Create Index
Delete Index
Reindex
Environment setup

To prepare GAIP for your site, there are 2 main steps

Mapping creation
Index creation

Info

These steps can also be done from our platform (GAIP). Refer here for detail.

For this, we will use API endpoints of GAIP listed in our Sandbox

You can also access our sandbox from the project setting page.

Mapping creation

The personalization engine relies on specific default keys to operate effectively. To integrate your item catalogue with our solution, it's essential to align your website's data source keys (such as item name, item description, tags, ingredients, category, etc.) with keys of personalization engine via mapping them. Our personalizer then can understand your data. This is the core part of the personalization system so the schema should be followed properly to successfully map your data.

Item Catalogue Mapping

To create mapping, use endpoints listed under Catalog Mapping in the sandbox.

GET /v1/mappers to get an existing Mapper.
PUT /v1/mappers to update an existing Mapper.
POST /v1/mappers to create a new mapper.

For set up your project mapping we will use POST /v1/mappers. You can find keys, value types, and description with an example request body in the sandbox You can simply replace the values in the example with your item catalogue keys and hit Execute to finish the mapping of your product catalogue Keys with GAIP keys.

After execution, confirm the server response is success.

Example of this item mapper creation

Here, we will use doozie shop as an example E-commerce site.

For an example item from the shop, this is how the data is structured.

json { "success": true, "result": [ { "item_id": "12345", "title": "Eco-Friendly Water Bottle", "description": "A durable, BPA-free water bottle designed for everyday use. Made from eco-friendly materials, it keeps your drink cold for up to 24 hours. Perfect for staying hydrated on the go.", "headline": "Stay Hydrated with Our Eco-Friendly Water Bottle", "availability": true, "affiliate_rate": 4, "price": 2360, "currency": "JPY", "shop_id": "hworks", "shop_name": "スマートビズ-ワイシャツ専門店-", "review_count": 2836, "review_average": 4.26, "genre_id": "206363", "brand": null, "shop_url": "https://hb.afl.rakuten.co.jp/hgc/g00rgm95.h3cpt446.g00rgm95.h3cpu696/?pc=https%3A%2F%2Fwww.rakuten.co.jp%2Fhworks%2F&m=http%3A%2F%2Fm.rakuten.co.jp%2Fhworks%2F", "item_url": "https://hb.afl.rakuten.co.jp/hgc/g00rgm95.h3cpt446.g00rgm95.h3cpu696/?pc=https%3A%2F%2Fitem.rakuten.co.jp%2Fhworks%2Fshirt-b0080%2F&m=http%3A%2F%2Fm.rakuten.co.jp%2Fhworks%2Fi%2F10000926%2F", "image_urls": [ "https://thumbnail.image.rakuten.co.jp/@0_mall/hworks/cabinet/001/014/shirt-b0080.jpg", "https://thumbnail.image.rakuten.co.jp/@0_mall/hworks/cabinet/001/014/b0080-lineup.jpg", "https://thumbnail.image.rakuten.co.jp/@0_mall/hworks/cabinet/banner/coupon202308-shdz_sq.jpg" ], "tag_ids": [ 1000903, 1008869, 1039853, 1013746 ], "tags": [], "shipping_overseas": "", "condition": 1, "genre_name": null, "parent_genre_categories": null, "shop_review_count": null, "shop_review_average": null, "tax_included": true, "point_multiplier": 1, "best_seller": false, "sale_start_time": "", "sale_end_time": "", "platform": "rakuten" } ] }

This shows the keys that doozie shop has for its products. Now we can create a mapping with GAIP keys, and use the POST /v1/mapper endpoint of GAIP. In this endpoint, we will pass the keys of above data source.

Here is an example of a mapper that we built for doozie shop, based on the keys above:

json { "key_map": { "item_id": "item_id", "title": "title", "second_title": "headline", "third_title": "shop_name", "fourth_title": "genre_name", "availability": "availability", "description": "description", "image_url": "image_urls", "image_url_type": "LIST_STR", "item_url": "item_url", "price": "price", "categories": [ { "name": "genre_id", "separator": "" } ], "custom":[], "flag": [ "condition" ] } }

Sample Code

You can find sample code for this implementation here

Once the mapper is created, you can use GET /v1/mappers endpoint to see the mapping. You can update any of mapped keys with PUT /v1/mapper endpoint and check the mapper you build from GET /v1/mapper endpoint.

User Behavior mapping

Similar to the item mapping key, there are some default keys for user behavior data.

Note

This step is required If you want to save historical user behavior data through CSV files. If you use our data collection endpoints to collect data from now on, this is not required.

You can find the Endpoints for user mapping under "Historical User Data Collection" section in the Sandbox

To implement this, please follow similar steps as above.

However, in this case please note that there are four sets of endpoints for Browsing history, purchase history, rating history, user detail. You have to create mapper for each if you want to import the data.

Index creation

In this step, you need to create indices. We need multiple indices to run recommender solution successfully. These indices will create the necessary schemas to hold your data.

There are 3 endpoints here

POST /v1/index/create --> Create indices to hold your data

DELETE /v1/index/delete --> Delete indices

POST /v1/reindex --> Creates index with new mappings and settings and create alias for new index

Create Index

Request endpoint

POST /v1/index/create Simply use your project key and API and click execute to create the indices for your project. Note that this will throw an error if the mapping in the previous step is not done correctly.

After the successful execution all the necessary index will be created and item index will be created in the background. You can check the status of item index creation with a task id from GET /v1/tasks/{task_id} API at the bottom of the page.

Please confirm the task was success.

Delete Index

You can delete an existing index with this endpoint.

Request endpoint

DELETE /v1/index/delete Available values are items, image_features, browse, purchase, ratings, search, stats, settings, user, tasks, logs.

If you Delete any index, please ensure to create the index again, unless you will get error when trying to input data/item catalogue or run training.

Reindex

In Elastic search, reindexing is the process of copying data from one index to another, either within the same cluster or to a different cluster. This can be useful in a variety of situations, such as:

Updating the mapping of an index: If you need to make changes to the mapping of an index, you can create a new index with the updated mapping and then reindex the data from the old index to the new one.

Moving data from one index to another: If you need to move data from one index to another, you can reindex the data from the source index to the destination index.

Updating the data with new data: If you have updated data that you want to add to an index, you can reindex the data with the updated data.

Changing the shard count of an index: If you need to change the number of shards that an index is using, you can reindex the data to a new index with the desired number of shards.

We can use Reindex API to copy data from index to another index.

Request Endpoint: POST /v1/reindex

Here is an example how to pass mappings and settings in request body:

json { "index_type": "items", "mappings": { "settings": { "analysis": { "char_filter": { "normalize": { "type": "icu_normalizer", "name": "nfkc", "mode": "compose" } }, "tokenizer": { "ja_kuromoji_tokenizer": { "mode": "search", "type": "kuromoji_tokenizer", "discard_compound_token": "true", "user_dictionary_rules": [] }, "ja_ngram_tokenizer": { "type": "ngram", "min_gram": 2, "max_gram": 3, "token_chars": [ "letter", "digit" ] } }, "filter": { "ja_index_synonym": { "type": "synonym", "lenient": "false", "synonyms": [] } }, "analyzer": { "ja_kuromoji_index_analyzer": { "type": "custom", "char_filter": [ "normalize" ], "tokenizer": "ja_kuromoji_tokenizer", "filter": [ "kuromoji_baseform", "kuromoji_part_of_speech", "ja_index_synonym", "cjk_width", "ja_stop", "kuromoji_stemmer", "lowercase" ] }, "ja_kuromoji_search_analyzer": { "type": "custom", "char_filter": [ "normalize" ], "tokenizer": "ja_kuromoji_tokenizer", "filter": [ "kuromoji_baseform", "kuromoji_part_of_speech", "cjk_width", "ja_stop", "kuromoji_stemmer", "lowercase" ] }, "ja_ngram_index_analyzer": { "type": "custom", "char_filter": [ "normalize" ], "tokenizer": "ja_ngram_tokenizer", "filter": [ "lowercase" ] }, "ja_ngram_search_analyzer": { "type": "custom", "char_filter": [ "normalize" ], "tokenizer": "ja_ngram_tokenizer", "filter": [ "lowercase" ] } } } }, "mappings": { "properties": { "item": { "properties": { "{title}": { "type": "text", "search_analyzer": "ja_kuromoji_search_analyzer", "analyzer": "ja_kuromoji_index_analyzer", "fields": { "ngram": { "type": "text", "search_analyzer": "ja_ngram_search_analyzer", "analyzer": "ja_ngram_index_analyzer" } } }, "{second_title}": { "type": "text", "search_analyzer": "ja_kuromoji_search_analyzer", "analyzer": "ja_kuromoji_index_analyzer", "fields": { "ngram": { "type": "text", "search_analyzer": "ja_ngram_search_analyzer", "analyzer": "ja_ngram_index_analyzer" } } }, "{third_title}": { "type": "text", "search_analyzer": "ja_kuromoji_search_analyzer", "analyzer": "ja_kuromoji_index_analyzer", "fields": { "ngram": { "type": "text", "search_analyzer": "ja_ngram_search_analyzer", "analyzer": "ja_ngram_index_analyzer" } } }, "{description}": { "type": "text", "search_analyzer": "ja_kuromoji_search_analyzer", "analyzer": "ja_kuromoji_index_analyzer", "fields": { "ngram": { "type": "text", "search_analyzer": "ja_ngram_search_analyzer", "analyzer": "ja_ngram_index_analyzer" } } }, "{price}": { "type": "float" }, "{availability}": { "type": "boolean" } } } } } } } Available values are items, image_features, browse, purchase, ratings, search, stats, settings, user, tasks, logs. When you define the mappings object, you should use the same keys as in the item mapper that you have built with POST /v1/mapper API.

You might not need analyzers or tokenizers for all indices. You can keep the settings field empty if it is not required. Here is an example,

json { "index_type": "search", "mappings": { "settings": {}, "mappings": { "properties": { "date": { "type": "date" } } } } }

Previous
Project setup
Next
Integration of Catalogue information and user behavior data
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/02_environment_setup/#mapping-creation
Skip to content
Gigalogy Tutorial
Environment setup
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Mapping creation
Item Catalogue Mapping
Example of this item mapper creation
Sample Code
User Behavior mapping
Index creation
Create Index
Delete Index
Reindex
Environment setup

To prepare GAIP for your site, there are 2 main steps

Mapping creation
Index creation

Info

These steps can also be done from our platform (GAIP). Refer here for detail.

For this, we will use API endpoints of GAIP listed in our Sandbox

You can also access our sandbox from the project setting page.

Mapping creation

The personalization engine relies on specific default keys to operate effectively. To integrate your item catalogue with our solution, it's essential to align your website's data source keys (such as item name, item description, tags, ingredients, category, etc.) with keys of personalization engine via mapping them. Our personalizer then can understand your data. This is the core part of the personalization system so the schema should be followed properly to successfully map your data.

Item Catalogue Mapping

To create mapping, use endpoints listed under Catalog Mapping in the sandbox.

GET /v1/mappers to get an existing Mapper.
PUT /v1/mappers to update an existing Mapper.
POST /v1/mappers to create a new mapper.

For set up your project mapping we will use POST /v1/mappers. You can find keys, value types, and description with an example request body in the sandbox You can simply replace the values in the example with your item catalogue keys and hit Execute to finish the mapping of your product catalogue Keys with GAIP keys.

After execution, confirm the server response is success.

Example of this item mapper creation

Here, we will use doozie shop as an example E-commerce site.

For an example item from the shop, this is how the data is structured.

json { "success": true, "result": [ { "item_id": "12345", "title": "Eco-Friendly Water Bottle", "description": "A durable, BPA-free water bottle designed for everyday use. Made from eco-friendly materials, it keeps your drink cold for up to 24 hours. Perfect for staying hydrated on the go.", "headline": "Stay Hydrated with Our Eco-Friendly Water Bottle", "availability": true, "affiliate_rate": 4, "price": 2360, "currency": "JPY", "shop_id": "hworks", "shop_name": "スマートビズ-ワイシャツ専門店-", "review_count": 2836, "review_average": 4.26, "genre_id": "206363", "brand": null, "shop_url": "https://hb.afl.rakuten.co.jp/hgc/g00rgm95.h3cpt446.g00rgm95.h3cpu696/?pc=https%3A%2F%2Fwww.rakuten.co.jp%2Fhworks%2F&m=http%3A%2F%2Fm.rakuten.co.jp%2Fhworks%2F", "item_url": "https://hb.afl.rakuten.co.jp/hgc/g00rgm95.h3cpt446.g00rgm95.h3cpu696/?pc=https%3A%2F%2Fitem.rakuten.co.jp%2Fhworks%2Fshirt-b0080%2F&m=http%3A%2F%2Fm.rakuten.co.jp%2Fhworks%2Fi%2F10000926%2F", "image_urls": [ "https://thumbnail.image.rakuten.co.jp/@0_mall/hworks/cabinet/001/014/shirt-b0080.jpg", "https://thumbnail.image.rakuten.co.jp/@0_mall/hworks/cabinet/001/014/b0080-lineup.jpg", "https://thumbnail.image.rakuten.co.jp/@0_mall/hworks/cabinet/banner/coupon202308-shdz_sq.jpg" ], "tag_ids": [ 1000903, 1008869, 1039853, 1013746 ], "tags": [], "shipping_overseas": "", "condition": 1, "genre_name": null, "parent_genre_categories": null, "shop_review_count": null, "shop_review_average": null, "tax_included": true, "point_multiplier": 1, "best_seller": false, "sale_start_time": "", "sale_end_time": "", "platform": "rakuten" } ] }

This shows the keys that doozie shop has for its products. Now we can create a mapping with GAIP keys, and use the POST /v1/mapper endpoint of GAIP. In this endpoint, we will pass the keys of above data source.

Here is an example of a mapper that we built for doozie shop, based on the keys above:

json { "key_map": { "item_id": "item_id", "title": "title", "second_title": "headline", "third_title": "shop_name", "fourth_title": "genre_name", "availability": "availability", "description": "description", "image_url": "image_urls", "image_url_type": "LIST_STR", "item_url": "item_url", "price": "price", "categories": [ { "name": "genre_id", "separator": "" } ], "custom":[], "flag": [ "condition" ] } }

Sample Code

You can find sample code for this implementation here

Once the mapper is created, you can use GET /v1/mappers endpoint to see the mapping. You can update any of mapped keys with PUT /v1/mapper endpoint and check the mapper you build from GET /v1/mapper endpoint.

User Behavior mapping

Similar to the item mapping key, there are some default keys for user behavior data.

Note

This step is required If you want to save historical user behavior data through CSV files. If you use our data collection endpoints to collect data from now on, this is not required.

You can find the Endpoints for user mapping under "Historical User Data Collection" section in the Sandbox

To implement this, please follow similar steps as above.

However, in this case please note that there are four sets of endpoints for Browsing history, purchase history, rating history, user detail. You have to create mapper for each if you want to import the data.

Index creation

In this step, you need to create indices. We need multiple indices to run recommender solution successfully. These indices will create the necessary schemas to hold your data.

There are 3 endpoints here

POST /v1/index/create --> Create indices to hold your data

DELETE /v1/index/delete --> Delete indices

POST /v1/reindex --> Creates index with new mappings and settings and create alias for new index

Create Index

Request endpoint

POST /v1/index/create Simply use your project key and API and click execute to create the indices for your project. Note that this will throw an error if the mapping in the previous step is not done correctly.

After the successful execution all the necessary index will be created and item index will be created in the background. You can check the status of item index creation with a task id from GET /v1/tasks/{task_id} API at the bottom of the page.

Please confirm the task was success.

Delete Index

You can delete an existing index with this endpoint.

Request endpoint

DELETE /v1/index/delete Available values are items, image_features, browse, purchase, ratings, search, stats, settings, user, tasks, logs.

If you Delete any index, please ensure to create the index again, unless you will get error when trying to input data/item catalogue or run training.

Reindex

In Elastic search, reindexing is the process of copying data from one index to another, either within the same cluster or to a different cluster. This can be useful in a variety of situations, such as:

Updating the mapping of an index: If you need to make changes to the mapping of an index, you can create a new index with the updated mapping and then reindex the data from the old index to the new one.

Moving data from one index to another: If you need to move data from one index to another, you can reindex the data from the source index to the destination index.

Updating the data with new data: If you have updated data that you want to add to an index, you can reindex the data with the updated data.

Changing the shard count of an index: If you need to change the number of shards that an index is using, you can reindex the data to a new index with the desired number of shards.

We can use Reindex API to copy data from index to another index.

Request Endpoint: POST /v1/reindex

Here is an example how to pass mappings and settings in request body:

json { "index_type": "items", "mappings": { "settings": { "analysis": { "char_filter": { "normalize": { "type": "icu_normalizer", "name": "nfkc", "mode": "compose" } }, "tokenizer": { "ja_kuromoji_tokenizer": { "mode": "search", "type": "kuromoji_tokenizer", "discard_compound_token": "true", "user_dictionary_rules": [] }, "ja_ngram_tokenizer": { "type": "ngram", "min_gram": 2, "max_gram": 3, "token_chars": [ "letter", "digit" ] } }, "filter": { "ja_index_synonym": { "type": "synonym", "lenient": "false", "synonyms": [] } }, "analyzer": { "ja_kuromoji_index_analyzer": { "type": "custom", "char_filter": [ "normalize" ], "tokenizer": "ja_kuromoji_tokenizer", "filter": [ "kuromoji_baseform", "kuromoji_part_of_speech", "ja_index_synonym", "cjk_width", "ja_stop", "kuromoji_stemmer", "lowercase" ] }, "ja_kuromoji_search_analyzer": { "type": "custom", "char_filter": [ "normalize" ], "tokenizer": "ja_kuromoji_tokenizer", "filter": [ "kuromoji_baseform", "kuromoji_part_of_speech", "cjk_width", "ja_stop", "kuromoji_stemmer", "lowercase" ] }, "ja_ngram_index_analyzer": { "type": "custom", "char_filter": [ "normalize" ], "tokenizer": "ja_ngram_tokenizer", "filter": [ "lowercase" ] }, "ja_ngram_search_analyzer": { "type": "custom", "char_filter": [ "normalize" ], "tokenizer": "ja_ngram_tokenizer", "filter": [ "lowercase" ] } } } }, "mappings": { "properties": { "item": { "properties": { "{title}": { "type": "text", "search_analyzer": "ja_kuromoji_search_analyzer", "analyzer": "ja_kuromoji_index_analyzer", "fields": { "ngram": { "type": "text", "search_analyzer": "ja_ngram_search_analyzer", "analyzer": "ja_ngram_index_analyzer" } } }, "{second_title}": { "type": "text", "search_analyzer": "ja_kuromoji_search_analyzer", "analyzer": "ja_kuromoji_index_analyzer", "fields": { "ngram": { "type": "text", "search_analyzer": "ja_ngram_search_analyzer", "analyzer": "ja_ngram_index_analyzer" } } }, "{third_title}": { "type": "text", "search_analyzer": "ja_kuromoji_search_analyzer", "analyzer": "ja_kuromoji_index_analyzer", "fields": { "ngram": { "type": "text", "search_analyzer": "ja_ngram_search_analyzer", "analyzer": "ja_ngram_index_analyzer" } } }, "{description}": { "type": "text", "search_analyzer": "ja_kuromoji_search_analyzer", "analyzer": "ja_kuromoji_index_analyzer", "fields": { "ngram": { "type": "text", "search_analyzer": "ja_ngram_search_analyzer", "analyzer": "ja_ngram_index_analyzer" } } }, "{price}": { "type": "float" }, "{availability}": { "type": "boolean" } } } } } } } Available values are items, image_features, browse, purchase, ratings, search, stats, settings, user, tasks, logs. When you define the mappings object, you should use the same keys as in the item mapper that you have built with POST /v1/mapper API.

You might not need analyzers or tokenizers for all indices. You can keep the settings field empty if it is not required. Here is an example,

json { "index_type": "search", "mappings": { "settings": {}, "mappings": { "properties": { "date": { "type": "date" } } } } }

Previous
Project setup
Next
Integration of Catalogue information and user behavior data
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/02_environment_setup/#item-catalogue-mapping
Skip to content
Gigalogy Tutorial
Environment setup
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Mapping creation
Item Catalogue Mapping
Example of this item mapper creation
Sample Code
User Behavior mapping
Index creation
Create Index
Delete Index
Reindex
Environment setup

To prepare GAIP for your site, there are 2 main steps

Mapping creation
Index creation

Info

These steps can also be done from our platform (GAIP). Refer here for detail.

For this, we will use API endpoints of GAIP listed in our Sandbox

You can also access our sandbox from the project setting page.

Mapping creation

The personalization engine relies on specific default keys to operate effectively. To integrate your item catalogue with our solution, it's essential to align your website's data source keys (such as item name, item description, tags, ingredients, category, etc.) with keys of personalization engine via mapping them. Our personalizer then can understand your data. This is the core part of the personalization system so the schema should be followed properly to successfully map your data.

Item Catalogue Mapping

To create mapping, use endpoints listed under Catalog Mapping in the sandbox.

GET /v1/mappers to get an existing Mapper.
PUT /v1/mappers to update an existing Mapper.
POST /v1/mappers to create a new mapper.

For set up your project mapping we will use POST /v1/mappers. You can find keys, value types, and description with an example request body in the sandbox You can simply replace the values in the example with your item catalogue keys and hit Execute to finish the mapping of your product catalogue Keys with GAIP keys.

After execution, confirm the server response is success.

Example of this item mapper creation

Here, we will use doozie shop as an example E-commerce site.

For an example item from the shop, this is how the data is structured.

json { "success": true, "result": [ { "item_id": "12345", "title": "Eco-Friendly Water Bottle", "description": "A durable, BPA-free water bottle designed for everyday use. Made from eco-friendly materials, it keeps your drink cold for up to 24 hours. Perfect for staying hydrated on the go.", "headline": "Stay Hydrated with Our Eco-Friendly Water Bottle", "availability": true, "affiliate_rate": 4, "price": 2360, "currency": "JPY", "shop_id": "hworks", "shop_name": "スマートビズ-ワイシャツ専門店-", "review_count": 2836, "review_average": 4.26, "genre_id": "206363", "brand": null, "shop_url": "https://hb.afl.rakuten.co.jp/hgc/g00rgm95.h3cpt446.g00rgm95.h3cpu696/?pc=https%3A%2F%2Fwww.rakuten.co.jp%2Fhworks%2F&m=http%3A%2F%2Fm.rakuten.co.jp%2Fhworks%2F", "item_url": "https://hb.afl.rakuten.co.jp/hgc/g00rgm95.h3cpt446.g00rgm95.h3cpu696/?pc=https%3A%2F%2Fitem.rakuten.co.jp%2Fhworks%2Fshirt-b0080%2F&m=http%3A%2F%2Fm.rakuten.co.jp%2Fhworks%2Fi%2F10000926%2F", "image_urls": [ "https://thumbnail.image.rakuten.co.jp/@0_mall/hworks/cabinet/001/014/shirt-b0080.jpg", "https://thumbnail.image.rakuten.co.jp/@0_mall/hworks/cabinet/001/014/b0080-lineup.jpg", "https://thumbnail.image.rakuten.co.jp/@0_mall/hworks/cabinet/banner/coupon202308-shdz_sq.jpg" ], "tag_ids": [ 1000903, 1008869, 1039853, 1013746 ], "tags": [], "shipping_overseas": "", "condition": 1, "genre_name": null, "parent_genre_categories": null, "shop_review_count": null, "shop_review_average": null, "tax_included": true, "point_multiplier": 1, "best_seller": false, "sale_start_time": "", "sale_end_time": "", "platform": "rakuten" } ] }

This shows the keys that doozie shop has for its products. Now we can create a mapping with GAIP keys, and use the POST /v1/mapper endpoint of GAIP. In this endpoint, we will pass the keys of above data source.

Here is an example of a mapper that we built for doozie shop, based on the keys above:

json { "key_map": { "item_id": "item_id", "title": "title", "second_title": "headline", "third_title": "shop_name", "fourth_title": "genre_name", "availability": "availability", "description": "description", "image_url": "image_urls", "image_url_type": "LIST_STR", "item_url": "item_url", "price": "price", "categories": [ { "name": "genre_id", "separator": "" } ], "custom":[], "flag": [ "condition" ] } }

Sample Code

You can find sample code for this implementation here

Once the mapper is created, you can use GET /v1/mappers endpoint to see the mapping. You can update any of mapped keys with PUT /v1/mapper endpoint and check the mapper you build from GET /v1/mapper endpoint.

User Behavior mapping

Similar to the item mapping key, there are some default keys for user behavior data.

Note

This step is required If you want to save historical user behavior data through CSV files. If you use our data collection endpoints to collect data from now on, this is not required.

You can find the Endpoints for user mapping under "Historical User Data Collection" section in the Sandbox

To implement this, please follow similar steps as above.

However, in this case please note that there are four sets of endpoints for Browsing history, purchase history, rating history, user detail. You have to create mapper for each if you want to import the data.

Index creation

In this step, you need to create indices. We need multiple indices to run recommender solution successfully. These indices will create the necessary schemas to hold your data.

There are 3 endpoints here

POST /v1/index/create --> Create indices to hold your data

DELETE /v1/index/delete --> Delete indices

POST /v1/reindex --> Creates index with new mappings and settings and create alias for new index

Create Index

Request endpoint

POST /v1/index/create Simply use your project key and API and click execute to create the indices for your project. Note that this will throw an error if the mapping in the previous step is not done correctly.

After the successful execution all the necessary index will be created and item index will be created in the background. You can check the status of item index creation with a task id from GET /v1/tasks/{task_id} API at the bottom of the page.

Please confirm the task was success.

Delete Index

You can delete an existing index with this endpoint.

Request endpoint

DELETE /v1/index/delete Available values are items, image_features, browse, purchase, ratings, search, stats, settings, user, tasks, logs.

If you Delete any index, please ensure to create the index again, unless you will get error when trying to input data/item catalogue or run training.

Reindex

In Elastic search, reindexing is the process of copying data from one index to another, either within the same cluster or to a different cluster. This can be useful in a variety of situations, such as:

Updating the mapping of an index: If you need to make changes to the mapping of an index, you can create a new index with the updated mapping and then reindex the data from the old index to the new one.

Moving data from one index to another: If you need to move data from one index to another, you can reindex the data from the source index to the destination index.

Updating the data with new data: If you have updated data that you want to add to an index, you can reindex the data with the updated data.

Changing the shard count of an index: If you need to change the number of shards that an index is using, you can reindex the data to a new index with the desired number of shards.

We can use Reindex API to copy data from index to another index.

Request Endpoint: POST /v1/reindex

Here is an example how to pass mappings and settings in request body:

json { "index_type": "items", "mappings": { "settings": { "analysis": { "char_filter": { "normalize": { "type": "icu_normalizer", "name": "nfkc", "mode": "compose" } }, "tokenizer": { "ja_kuromoji_tokenizer": { "mode": "search", "type": "kuromoji_tokenizer", "discard_compound_token": "true", "user_dictionary_rules": [] }, "ja_ngram_tokenizer": { "type": "ngram", "min_gram": 2, "max_gram": 3, "token_chars": [ "letter", "digit" ] } }, "filter": { "ja_index_synonym": { "type": "synonym", "lenient": "false", "synonyms": [] } }, "analyzer": { "ja_kuromoji_index_analyzer": { "type": "custom", "char_filter": [ "normalize" ], "tokenizer": "ja_kuromoji_tokenizer", "filter": [ "kuromoji_baseform", "kuromoji_part_of_speech", "ja_index_synonym", "cjk_width", "ja_stop", "kuromoji_stemmer", "lowercase" ] }, "ja_kuromoji_search_analyzer": { "type": "custom", "char_filter": [ "normalize" ], "tokenizer": "ja_kuromoji_tokenizer", "filter": [ "kuromoji_baseform", "kuromoji_part_of_speech", "cjk_width", "ja_stop", "kuromoji_stemmer", "lowercase" ] }, "ja_ngram_index_analyzer": { "type": "custom", "char_filter": [ "normalize" ], "tokenizer": "ja_ngram_tokenizer", "filter": [ "lowercase" ] }, "ja_ngram_search_analyzer": { "type": "custom", "char_filter": [ "normalize" ], "tokenizer": "ja_ngram_tokenizer", "filter": [ "lowercase" ] } } } }, "mappings": { "properties": { "item": { "properties": { "{title}": { "type": "text", "search_analyzer": "ja_kuromoji_search_analyzer", "analyzer": "ja_kuromoji_index_analyzer", "fields": { "ngram": { "type": "text", "search_analyzer": "ja_ngram_search_analyzer", "analyzer": "ja_ngram_index_analyzer" } } }, "{second_title}": { "type": "text", "search_analyzer": "ja_kuromoji_search_analyzer", "analyzer": "ja_kuromoji_index_analyzer", "fields": { "ngram": { "type": "text", "search_analyzer": "ja_ngram_search_analyzer", "analyzer": "ja_ngram_index_analyzer" } } }, "{third_title}": { "type": "text", "search_analyzer": "ja_kuromoji_search_analyzer", "analyzer": "ja_kuromoji_index_analyzer", "fields": { "ngram": { "type": "text", "search_analyzer": "ja_ngram_search_analyzer", "analyzer": "ja_ngram_index_analyzer" } } }, "{description}": { "type": "text", "search_analyzer": "ja_kuromoji_search_analyzer", "analyzer": "ja_kuromoji_index_analyzer", "fields": { "ngram": { "type": "text", "search_analyzer": "ja_ngram_search_analyzer", "analyzer": "ja_ngram_index_analyzer" } } }, "{price}": { "type": "float" }, "{availability}": { "type": "boolean" } } } } } } } Available values are items, image_features, browse, purchase, ratings, search, stats, settings, user, tasks, logs. When you define the mappings object, you should use the same keys as in the item mapper that you have built with POST /v1/mapper API.

You might not need analyzers or tokenizers for all indices. You can keep the settings field empty if it is not required. Here is an example,

json { "index_type": "search", "mappings": { "settings": {}, "mappings": { "properties": { "date": { "type": "date" } } } } }

Previous
Project setup
Next
Integration of Catalogue information and user behavior data
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/02_environment_setup/#example-of-this-item-mapper-creation
Skip to content
Gigalogy Tutorial
Environment setup
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Mapping creation
Item Catalogue Mapping
Example of this item mapper creation
Sample Code
User Behavior mapping
Index creation
Create Index
Delete Index
Reindex
Environment setup

To prepare GAIP for your site, there are 2 main steps

Mapping creation
Index creation

Info

These steps can also be done from our platform (GAIP). Refer here for detail.

For this, we will use API endpoints of GAIP listed in our Sandbox

You can also access our sandbox from the project setting page.

Mapping creation

The personalization engine relies on specific default keys to operate effectively. To integrate your item catalogue with our solution, it's essential to align your website's data source keys (such as item name, item description, tags, ingredients, category, etc.) with keys of personalization engine via mapping them. Our personalizer then can understand your data. This is the core part of the personalization system so the schema should be followed properly to successfully map your data.

Item Catalogue Mapping

To create mapping, use endpoints listed under Catalog Mapping in the sandbox.

GET /v1/mappers to get an existing Mapper.
PUT /v1/mappers to update an existing Mapper.
POST /v1/mappers to create a new mapper.

For set up your project mapping we will use POST /v1/mappers. You can find keys, value types, and description with an example request body in the sandbox You can simply replace the values in the example with your item catalogue keys and hit Execute to finish the mapping of your product catalogue Keys with GAIP keys.

After execution, confirm the server response is success.

Example of this item mapper creation

Here, we will use doozie shop as an example E-commerce site.

For an example item from the shop, this is how the data is structured.

json { "success": true, "result": [ { "item_id": "12345", "title": "Eco-Friendly Water Bottle", "description": "A durable, BPA-free water bottle designed for everyday use. Made from eco-friendly materials, it keeps your drink cold for up to 24 hours. Perfect for staying hydrated on the go.", "headline": "Stay Hydrated with Our Eco-Friendly Water Bottle", "availability": true, "affiliate_rate": 4, "price": 2360, "currency": "JPY", "shop_id": "hworks", "shop_name": "スマートビズ-ワイシャツ専門店-", "review_count": 2836, "review_average": 4.26, "genre_id": "206363", "brand": null, "shop_url": "https://hb.afl.rakuten.co.jp/hgc/g00rgm95.h3cpt446.g00rgm95.h3cpu696/?pc=https%3A%2F%2Fwww.rakuten.co.jp%2Fhworks%2F&m=http%3A%2F%2Fm.rakuten.co.jp%2Fhworks%2F", "item_url": "https://hb.afl.rakuten.co.jp/hgc/g00rgm95.h3cpt446.g00rgm95.h3cpu696/?pc=https%3A%2F%2Fitem.rakuten.co.jp%2Fhworks%2Fshirt-b0080%2F&m=http%3A%2F%2Fm.rakuten.co.jp%2Fhworks%2Fi%2F10000926%2F", "image_urls": [ "https://thumbnail.image.rakuten.co.jp/@0_mall/hworks/cabinet/001/014/shirt-b0080.jpg", "https://thumbnail.image.rakuten.co.jp/@0_mall/hworks/cabinet/001/014/b0080-lineup.jpg", "https://thumbnail.image.rakuten.co.jp/@0_mall/hworks/cabinet/banner/coupon202308-shdz_sq.jpg" ], "tag_ids": [ 1000903, 1008869, 1039853, 1013746 ], "tags": [], "shipping_overseas": "", "condition": 1, "genre_name": null, "parent_genre_categories": null, "shop_review_count": null, "shop_review_average": null, "tax_included": true, "point_multiplier": 1, "best_seller": false, "sale_start_time": "", "sale_end_time": "", "platform": "rakuten" } ] }

This shows the keys that doozie shop has for its products. Now we can create a mapping with GAIP keys, and use the POST /v1/mapper endpoint of GAIP. In this endpoint, we will pass the keys of above data source.

Here is an example of a mapper that we built for doozie shop, based on the keys above:

json { "key_map": { "item_id": "item_id", "title": "title", "second_title": "headline", "third_title": "shop_name", "fourth_title": "genre_name", "availability": "availability", "description": "description", "image_url": "image_urls", "image_url_type": "LIST_STR", "item_url": "item_url", "price": "price", "categories": [ { "name": "genre_id", "separator": "" } ], "custom":[], "flag": [ "condition" ] } }

Sample Code

You can find sample code for this implementation here

Once the mapper is created, you can use GET /v1/mappers endpoint to see the mapping. You can update any of mapped keys with PUT /v1/mapper endpoint and check the mapper you build from GET /v1/mapper endpoint.

User Behavior mapping

Similar to the item mapping key, there are some default keys for user behavior data.

Note

This step is required If you want to save historical user behavior data through CSV files. If you use our data collection endpoints to collect data from now on, this is not required.

You can find the Endpoints for user mapping under "Historical User Data Collection" section in the Sandbox

To implement this, please follow similar steps as above.

However, in this case please note that there are four sets of endpoints for Browsing history, purchase history, rating history, user detail. You have to create mapper for each if you want to import the data.

Index creation

In this step, you need to create indices. We need multiple indices to run recommender solution successfully. These indices will create the necessary schemas to hold your data.

There are 3 endpoints here

POST /v1/index/create --> Create indices to hold your data

DELETE /v1/index/delete --> Delete indices

POST /v1/reindex --> Creates index with new mappings and settings and create alias for new index

Create Index

Request endpoint

POST /v1/index/create Simply use your project key and API and click execute to create the indices for your project. Note that this will throw an error if the mapping in the previous step is not done correctly.

After the successful execution all the necessary index will be created and item index will be created in the background. You can check the status of item index creation with a task id from GET /v1/tasks/{task_id} API at the bottom of the page.

Please confirm the task was success.

Delete Index

You can delete an existing index with this endpoint.

Request endpoint

DELETE /v1/index/delete Available values are items, image_features, browse, purchase, ratings, search, stats, settings, user, tasks, logs.

If you Delete any index, please ensure to create the index again, unless you will get error when trying to input data/item catalogue or run training.

Reindex

In Elastic search, reindexing is the process of copying data from one index to another, either within the same cluster or to a different cluster. This can be useful in a variety of situations, such as:

Updating the mapping of an index: If you need to make changes to the mapping of an index, you can create a new index with the updated mapping and then reindex the data from the old index to the new one.

Moving data from one index to another: If you need to move data from one index to another, you can reindex the data from the source index to the destination index.

Updating the data with new data: If you have updated data that you want to add to an index, you can reindex the data with the updated data.

Changing the shard count of an index: If you need to change the number of shards that an index is using, you can reindex the data to a new index with the desired number of shards.

We can use Reindex API to copy data from index to another index.

Request Endpoint: POST /v1/reindex

Here is an example how to pass mappings and settings in request body:

json { "index_type": "items", "mappings": { "settings": { "analysis": { "char_filter": { "normalize": { "type": "icu_normalizer", "name": "nfkc", "mode": "compose" } }, "tokenizer": { "ja_kuromoji_tokenizer": { "mode": "search", "type": "kuromoji_tokenizer", "discard_compound_token": "true", "user_dictionary_rules": [] }, "ja_ngram_tokenizer": { "type": "ngram", "min_gram": 2, "max_gram": 3, "token_chars": [ "letter", "digit" ] } }, "filter": { "ja_index_synonym": { "type": "synonym", "lenient": "false", "synonyms": [] } }, "analyzer": { "ja_kuromoji_index_analyzer": { "type": "custom", "char_filter": [ "normalize" ], "tokenizer": "ja_kuromoji_tokenizer", "filter": [ "kuromoji_baseform", "kuromoji_part_of_speech", "ja_index_synonym", "cjk_width", "ja_stop", "kuromoji_stemmer", "lowercase" ] }, "ja_kuromoji_search_analyzer": { "type": "custom", "char_filter": [ "normalize" ], "tokenizer": "ja_kuromoji_tokenizer", "filter": [ "kuromoji_baseform", "kuromoji_part_of_speech", "cjk_width", "ja_stop", "kuromoji_stemmer", "lowercase" ] }, "ja_ngram_index_analyzer": { "type": "custom", "char_filter": [ "normalize" ], "tokenizer": "ja_ngram_tokenizer", "filter": [ "lowercase" ] }, "ja_ngram_search_analyzer": { "type": "custom", "char_filter": [ "normalize" ], "tokenizer": "ja_ngram_tokenizer", "filter": [ "lowercase" ] } } } }, "mappings": { "properties": { "item": { "properties": { "{title}": { "type": "text", "search_analyzer": "ja_kuromoji_search_analyzer", "analyzer": "ja_kuromoji_index_analyzer", "fields": { "ngram": { "type": "text", "search_analyzer": "ja_ngram_search_analyzer", "analyzer": "ja_ngram_index_analyzer" } } }, "{second_title}": { "type": "text", "search_analyzer": "ja_kuromoji_search_analyzer", "analyzer": "ja_kuromoji_index_analyzer", "fields": { "ngram": { "type": "text", "search_analyzer": "ja_ngram_search_analyzer", "analyzer": "ja_ngram_index_analyzer" } } }, "{third_title}": { "type": "text", "search_analyzer": "ja_kuromoji_search_analyzer", "analyzer": "ja_kuromoji_index_analyzer", "fields": { "ngram": { "type": "text", "search_analyzer": "ja_ngram_search_analyzer", "analyzer": "ja_ngram_index_analyzer" } } }, "{description}": { "type": "text", "search_analyzer": "ja_kuromoji_search_analyzer", "analyzer": "ja_kuromoji_index_analyzer", "fields": { "ngram": { "type": "text", "search_analyzer": "ja_ngram_search_analyzer", "analyzer": "ja_ngram_index_analyzer" } } }, "{price}": { "type": "float" }, "{availability}": { "type": "boolean" } } } } } } } Available values are items, image_features, browse, purchase, ratings, search, stats, settings, user, tasks, logs. When you define the mappings object, you should use the same keys as in the item mapper that you have built with POST /v1/mapper API.

You might not need analyzers or tokenizers for all indices. You can keep the settings field empty if it is not required. Here is an example,

json { "index_type": "search", "mappings": { "settings": {}, "mappings": { "properties": { "date": { "type": "date" } } } } }

Previous
Project setup
Next
Integration of Catalogue information and user behavior data
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/02_environment_setup/#sample-code
Skip to content
Gigalogy Tutorial
Environment setup
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Mapping creation
Item Catalogue Mapping
Example of this item mapper creation
Sample Code
User Behavior mapping
Index creation
Create Index
Delete Index
Reindex
Environment setup

To prepare GAIP for your site, there are 2 main steps

Mapping creation
Index creation

Info

These steps can also be done from our platform (GAIP). Refer here for detail.

For this, we will use API endpoints of GAIP listed in our Sandbox

You can also access our sandbox from the project setting page.

Mapping creation

The personalization engine relies on specific default keys to operate effectively. To integrate your item catalogue with our solution, it's essential to align your website's data source keys (such as item name, item description, tags, ingredients, category, etc.) with keys of personalization engine via mapping them. Our personalizer then can understand your data. This is the core part of the personalization system so the schema should be followed properly to successfully map your data.

Item Catalogue Mapping

To create mapping, use endpoints listed under Catalog Mapping in the sandbox.

GET /v1/mappers to get an existing Mapper.
PUT /v1/mappers to update an existing Mapper.
POST /v1/mappers to create a new mapper.

For set up your project mapping we will use POST /v1/mappers. You can find keys, value types, and description with an example request body in the sandbox You can simply replace the values in the example with your item catalogue keys and hit Execute to finish the mapping of your product catalogue Keys with GAIP keys.

After execution, confirm the server response is success.

Example of this item mapper creation

Here, we will use doozie shop as an example E-commerce site.

For an example item from the shop, this is how the data is structured.

json { "success": true, "result": [ { "item_id": "12345", "title": "Eco-Friendly Water Bottle", "description": "A durable, BPA-free water bottle designed for everyday use. Made from eco-friendly materials, it keeps your drink cold for up to 24 hours. Perfect for staying hydrated on the go.", "headline": "Stay Hydrated with Our Eco-Friendly Water Bottle", "availability": true, "affiliate_rate": 4, "price": 2360, "currency": "JPY", "shop_id": "hworks", "shop_name": "スマートビズ-ワイシャツ専門店-", "review_count": 2836, "review_average": 4.26, "genre_id": "206363", "brand": null, "shop_url": "https://hb.afl.rakuten.co.jp/hgc/g00rgm95.h3cpt446.g00rgm95.h3cpu696/?pc=https%3A%2F%2Fwww.rakuten.co.jp%2Fhworks%2F&m=http%3A%2F%2Fm.rakuten.co.jp%2Fhworks%2F", "item_url": "https://hb.afl.rakuten.co.jp/hgc/g00rgm95.h3cpt446.g00rgm95.h3cpu696/?pc=https%3A%2F%2Fitem.rakuten.co.jp%2Fhworks%2Fshirt-b0080%2F&m=http%3A%2F%2Fm.rakuten.co.jp%2Fhworks%2Fi%2F10000926%2F", "image_urls": [ "https://thumbnail.image.rakuten.co.jp/@0_mall/hworks/cabinet/001/014/shirt-b0080.jpg", "https://thumbnail.image.rakuten.co.jp/@0_mall/hworks/cabinet/001/014/b0080-lineup.jpg", "https://thumbnail.image.rakuten.co.jp/@0_mall/hworks/cabinet/banner/coupon202308-shdz_sq.jpg" ], "tag_ids": [ 1000903, 1008869, 1039853, 1013746 ], "tags": [], "shipping_overseas": "", "condition": 1, "genre_name": null, "parent_genre_categories": null, "shop_review_count": null, "shop_review_average": null, "tax_included": true, "point_multiplier": 1, "best_seller": false, "sale_start_time": "", "sale_end_time": "", "platform": "rakuten" } ] }

This shows the keys that doozie shop has for its products. Now we can create a mapping with GAIP keys, and use the POST /v1/mapper endpoint of GAIP. In this endpoint, we will pass the keys of above data source.

Here is an example of a mapper that we built for doozie shop, based on the keys above:

json { "key_map": { "item_id": "item_id", "title": "title", "second_title": "headline", "third_title": "shop_name", "fourth_title": "genre_name", "availability": "availability", "description": "description", "image_url": "image_urls", "image_url_type": "LIST_STR", "item_url": "item_url", "price": "price", "categories": [ { "name": "genre_id", "separator": "" } ], "custom":[], "flag": [ "condition" ] } }

Sample Code

You can find sample code for this implementation here

Once the mapper is created, you can use GET /v1/mappers endpoint to see the mapping. You can update any of mapped keys with PUT /v1/mapper endpoint and check the mapper you build from GET /v1/mapper endpoint.

User Behavior mapping

Similar to the item mapping key, there are some default keys for user behavior data.

Note

This step is required If you want to save historical user behavior data through CSV files. If you use our data collection endpoints to collect data from now on, this is not required.

You can find the Endpoints for user mapping under "Historical User Data Collection" section in the Sandbox

To implement this, please follow similar steps as above.

However, in this case please note that there are four sets of endpoints for Browsing history, purchase history, rating history, user detail. You have to create mapper for each if you want to import the data.

Index creation

In this step, you need to create indices. We need multiple indices to run recommender solution successfully. These indices will create the necessary schemas to hold your data.

There are 3 endpoints here

POST /v1/index/create --> Create indices to hold your data

DELETE /v1/index/delete --> Delete indices

POST /v1/reindex --> Creates index with new mappings and settings and create alias for new index

Create Index

Request endpoint

POST /v1/index/create Simply use your project key and API and click execute to create the indices for your project. Note that this will throw an error if the mapping in the previous step is not done correctly.

After the successful execution all the necessary index will be created and item index will be created in the background. You can check the status of item index creation with a task id from GET /v1/tasks/{task_id} API at the bottom of the page.

Please confirm the task was success.

Delete Index

You can delete an existing index with this endpoint.

Request endpoint

DELETE /v1/index/delete Available values are items, image_features, browse, purchase, ratings, search, stats, settings, user, tasks, logs.

If you Delete any index, please ensure to create the index again, unless you will get error when trying to input data/item catalogue or run training.

Reindex

In Elastic search, reindexing is the process of copying data from one index to another, either within the same cluster or to a different cluster. This can be useful in a variety of situations, such as:

Updating the mapping of an index: If you need to make changes to the mapping of an index, you can create a new index with the updated mapping and then reindex the data from the old index to the new one.

Moving data from one index to another: If you need to move data from one index to another, you can reindex the data from the source index to the destination index.

Updating the data with new data: If you have updated data that you want to add to an index, you can reindex the data with the updated data.

Changing the shard count of an index: If you need to change the number of shards that an index is using, you can reindex the data to a new index with the desired number of shards.

We can use Reindex API to copy data from index to another index.

Request Endpoint: POST /v1/reindex

Here is an example how to pass mappings and settings in request body:

json { "index_type": "items", "mappings": { "settings": { "analysis": { "char_filter": { "normalize": { "type": "icu_normalizer", "name": "nfkc", "mode": "compose" } }, "tokenizer": { "ja_kuromoji_tokenizer": { "mode": "search", "type": "kuromoji_tokenizer", "discard_compound_token": "true", "user_dictionary_rules": [] }, "ja_ngram_tokenizer": { "type": "ngram", "min_gram": 2, "max_gram": 3, "token_chars": [ "letter", "digit" ] } }, "filter": { "ja_index_synonym": { "type": "synonym", "lenient": "false", "synonyms": [] } }, "analyzer": { "ja_kuromoji_index_analyzer": { "type": "custom", "char_filter": [ "normalize" ], "tokenizer": "ja_kuromoji_tokenizer", "filter": [ "kuromoji_baseform", "kuromoji_part_of_speech", "ja_index_synonym", "cjk_width", "ja_stop", "kuromoji_stemmer", "lowercase" ] }, "ja_kuromoji_search_analyzer": { "type": "custom", "char_filter": [ "normalize" ], "tokenizer": "ja_kuromoji_tokenizer", "filter": [ "kuromoji_baseform", "kuromoji_part_of_speech", "cjk_width", "ja_stop", "kuromoji_stemmer", "lowercase" ] }, "ja_ngram_index_analyzer": { "type": "custom", "char_filter": [ "normalize" ], "tokenizer": "ja_ngram_tokenizer", "filter": [ "lowercase" ] }, "ja_ngram_search_analyzer": { "type": "custom", "char_filter": [ "normalize" ], "tokenizer": "ja_ngram_tokenizer", "filter": [ "lowercase" ] } } } }, "mappings": { "properties": { "item": { "properties": { "{title}": { "type": "text", "search_analyzer": "ja_kuromoji_search_analyzer", "analyzer": "ja_kuromoji_index_analyzer", "fields": { "ngram": { "type": "text", "search_analyzer": "ja_ngram_search_analyzer", "analyzer": "ja_ngram_index_analyzer" } } }, "{second_title}": { "type": "text", "search_analyzer": "ja_kuromoji_search_analyzer", "analyzer": "ja_kuromoji_index_analyzer", "fields": { "ngram": { "type": "text", "search_analyzer": "ja_ngram_search_analyzer", "analyzer": "ja_ngram_index_analyzer" } } }, "{third_title}": { "type": "text", "search_analyzer": "ja_kuromoji_search_analyzer", "analyzer": "ja_kuromoji_index_analyzer", "fields": { "ngram": { "type": "text", "search_analyzer": "ja_ngram_search_analyzer", "analyzer": "ja_ngram_index_analyzer" } } }, "{description}": { "type": "text", "search_analyzer": "ja_kuromoji_search_analyzer", "analyzer": "ja_kuromoji_index_analyzer", "fields": { "ngram": { "type": "text", "search_analyzer": "ja_ngram_search_analyzer", "analyzer": "ja_ngram_index_analyzer" } } }, "{price}": { "type": "float" }, "{availability}": { "type": "boolean" } } } } } } } Available values are items, image_features, browse, purchase, ratings, search, stats, settings, user, tasks, logs. When you define the mappings object, you should use the same keys as in the item mapper that you have built with POST /v1/mapper API.

You might not need analyzers or tokenizers for all indices. You can keep the settings field empty if it is not required. Here is an example,

json { "index_type": "search", "mappings": { "settings": {}, "mappings": { "properties": { "date": { "type": "date" } } } } }

Previous
Project setup
Next
Integration of Catalogue information and user behavior data
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/02_environment_setup/#user-behavior-mapping
Skip to content
Gigalogy Tutorial
Environment setup
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Mapping creation
Item Catalogue Mapping
Example of this item mapper creation
Sample Code
User Behavior mapping
Index creation
Create Index
Delete Index
Reindex
Environment setup

To prepare GAIP for your site, there are 2 main steps

Mapping creation
Index creation

Info

These steps can also be done from our platform (GAIP). Refer here for detail.

For this, we will use API endpoints of GAIP listed in our Sandbox

You can also access our sandbox from the project setting page.

Mapping creation

The personalization engine relies on specific default keys to operate effectively. To integrate your item catalogue with our solution, it's essential to align your website's data source keys (such as item name, item description, tags, ingredients, category, etc.) with keys of personalization engine via mapping them. Our personalizer then can understand your data. This is the core part of the personalization system so the schema should be followed properly to successfully map your data.

Item Catalogue Mapping

To create mapping, use endpoints listed under Catalog Mapping in the sandbox.

GET /v1/mappers to get an existing Mapper.
PUT /v1/mappers to update an existing Mapper.
POST /v1/mappers to create a new mapper.

For set up your project mapping we will use POST /v1/mappers. You can find keys, value types, and description with an example request body in the sandbox You can simply replace the values in the example with your item catalogue keys and hit Execute to finish the mapping of your product catalogue Keys with GAIP keys.

After execution, confirm the server response is success.

Example of this item mapper creation

Here, we will use doozie shop as an example E-commerce site.

For an example item from the shop, this is how the data is structured.

json { "success": true, "result": [ { "item_id": "12345", "title": "Eco-Friendly Water Bottle", "description": "A durable, BPA-free water bottle designed for everyday use. Made from eco-friendly materials, it keeps your drink cold for up to 24 hours. Perfect for staying hydrated on the go.", "headline": "Stay Hydrated with Our Eco-Friendly Water Bottle", "availability": true, "affiliate_rate": 4, "price": 2360, "currency": "JPY", "shop_id": "hworks", "shop_name": "スマートビズ-ワイシャツ専門店-", "review_count": 2836, "review_average": 4.26, "genre_id": "206363", "brand": null, "shop_url": "https://hb.afl.rakuten.co.jp/hgc/g00rgm95.h3cpt446.g00rgm95.h3cpu696/?pc=https%3A%2F%2Fwww.rakuten.co.jp%2Fhworks%2F&m=http%3A%2F%2Fm.rakuten.co.jp%2Fhworks%2F", "item_url": "https://hb.afl.rakuten.co.jp/hgc/g00rgm95.h3cpt446.g00rgm95.h3cpu696/?pc=https%3A%2F%2Fitem.rakuten.co.jp%2Fhworks%2Fshirt-b0080%2F&m=http%3A%2F%2Fm.rakuten.co.jp%2Fhworks%2Fi%2F10000926%2F", "image_urls": [ "https://thumbnail.image.rakuten.co.jp/@0_mall/hworks/cabinet/001/014/shirt-b0080.jpg", "https://thumbnail.image.rakuten.co.jp/@0_mall/hworks/cabinet/001/014/b0080-lineup.jpg", "https://thumbnail.image.rakuten.co.jp/@0_mall/hworks/cabinet/banner/coupon202308-shdz_sq.jpg" ], "tag_ids": [ 1000903, 1008869, 1039853, 1013746 ], "tags": [], "shipping_overseas": "", "condition": 1, "genre_name": null, "parent_genre_categories": null, "shop_review_count": null, "shop_review_average": null, "tax_included": true, "point_multiplier": 1, "best_seller": false, "sale_start_time": "", "sale_end_time": "", "platform": "rakuten" } ] }

This shows the keys that doozie shop has for its products. Now we can create a mapping with GAIP keys, and use the POST /v1/mapper endpoint of GAIP. In this endpoint, we will pass the keys of above data source.

Here is an example of a mapper that we built for doozie shop, based on the keys above:

json { "key_map": { "item_id": "item_id", "title": "title", "second_title": "headline", "third_title": "shop_name", "fourth_title": "genre_name", "availability": "availability", "description": "description", "image_url": "image_urls", "image_url_type": "LIST_STR", "item_url": "item_url", "price": "price", "categories": [ { "name": "genre_id", "separator": "" } ], "custom":[], "flag": [ "condition" ] } }

Sample Code

You can find sample code for this implementation here

Once the mapper is created, you can use GET /v1/mappers endpoint to see the mapping. You can update any of mapped keys with PUT /v1/mapper endpoint and check the mapper you build from GET /v1/mapper endpoint.

User Behavior mapping

Similar to the item mapping key, there are some default keys for user behavior data.

Note

This step is required If you want to save historical user behavior data through CSV files. If you use our data collection endpoints to collect data from now on, this is not required.

You can find the Endpoints for user mapping under "Historical User Data Collection" section in the Sandbox

To implement this, please follow similar steps as above.

However, in this case please note that there are four sets of endpoints for Browsing history, purchase history, rating history, user detail. You have to create mapper for each if you want to import the data.

Index creation

In this step, you need to create indices. We need multiple indices to run recommender solution successfully. These indices will create the necessary schemas to hold your data.

There are 3 endpoints here

POST /v1/index/create --> Create indices to hold your data

DELETE /v1/index/delete --> Delete indices

POST /v1/reindex --> Creates index with new mappings and settings and create alias for new index

Create Index

Request endpoint

POST /v1/index/create Simply use your project key and API and click execute to create the indices for your project. Note that this will throw an error if the mapping in the previous step is not done correctly.

After the successful execution all the necessary index will be created and item index will be created in the background. You can check the status of item index creation with a task id from GET /v1/tasks/{task_id} API at the bottom of the page.

Please confirm the task was success.

Delete Index

You can delete an existing index with this endpoint.

Request endpoint

DELETE /v1/index/delete Available values are items, image_features, browse, purchase, ratings, search, stats, settings, user, tasks, logs.

If you Delete any index, please ensure to create the index again, unless you will get error when trying to input data/item catalogue or run training.

Reindex

In Elastic search, reindexing is the process of copying data from one index to another, either within the same cluster or to a different cluster. This can be useful in a variety of situations, such as:

Updating the mapping of an index: If you need to make changes to the mapping of an index, you can create a new index with the updated mapping and then reindex the data from the old index to the new one.

Moving data from one index to another: If you need to move data from one index to another, you can reindex the data from the source index to the destination index.

Updating the data with new data: If you have updated data that you want to add to an index, you can reindex the data with the updated data.

Changing the shard count of an index: If you need to change the number of shards that an index is using, you can reindex the data to a new index with the desired number of shards.

We can use Reindex API to copy data from index to another index.

Request Endpoint: POST /v1/reindex

Here is an example how to pass mappings and settings in request body:

json { "index_type": "items", "mappings": { "settings": { "analysis": { "char_filter": { "normalize": { "type": "icu_normalizer", "name": "nfkc", "mode": "compose" } }, "tokenizer": { "ja_kuromoji_tokenizer": { "mode": "search", "type": "kuromoji_tokenizer", "discard_compound_token": "true", "user_dictionary_rules": [] }, "ja_ngram_tokenizer": { "type": "ngram", "min_gram": 2, "max_gram": 3, "token_chars": [ "letter", "digit" ] } }, "filter": { "ja_index_synonym": { "type": "synonym", "lenient": "false", "synonyms": [] } }, "analyzer": { "ja_kuromoji_index_analyzer": { "type": "custom", "char_filter": [ "normalize" ], "tokenizer": "ja_kuromoji_tokenizer", "filter": [ "kuromoji_baseform", "kuromoji_part_of_speech", "ja_index_synonym", "cjk_width", "ja_stop", "kuromoji_stemmer", "lowercase" ] }, "ja_kuromoji_search_analyzer": { "type": "custom", "char_filter": [ "normalize" ], "tokenizer": "ja_kuromoji_tokenizer", "filter": [ "kuromoji_baseform", "kuromoji_part_of_speech", "cjk_width", "ja_stop", "kuromoji_stemmer", "lowercase" ] }, "ja_ngram_index_analyzer": { "type": "custom", "char_filter": [ "normalize" ], "tokenizer": "ja_ngram_tokenizer", "filter": [ "lowercase" ] }, "ja_ngram_search_analyzer": { "type": "custom", "char_filter": [ "normalize" ], "tokenizer": "ja_ngram_tokenizer", "filter": [ "lowercase" ] } } } }, "mappings": { "properties": { "item": { "properties": { "{title}": { "type": "text", "search_analyzer": "ja_kuromoji_search_analyzer", "analyzer": "ja_kuromoji_index_analyzer", "fields": { "ngram": { "type": "text", "search_analyzer": "ja_ngram_search_analyzer", "analyzer": "ja_ngram_index_analyzer" } } }, "{second_title}": { "type": "text", "search_analyzer": "ja_kuromoji_search_analyzer", "analyzer": "ja_kuromoji_index_analyzer", "fields": { "ngram": { "type": "text", "search_analyzer": "ja_ngram_search_analyzer", "analyzer": "ja_ngram_index_analyzer" } } }, "{third_title}": { "type": "text", "search_analyzer": "ja_kuromoji_search_analyzer", "analyzer": "ja_kuromoji_index_analyzer", "fields": { "ngram": { "type": "text", "search_analyzer": "ja_ngram_search_analyzer", "analyzer": "ja_ngram_index_analyzer" } } }, "{description}": { "type": "text", "search_analyzer": "ja_kuromoji_search_analyzer", "analyzer": "ja_kuromoji_index_analyzer", "fields": { "ngram": { "type": "text", "search_analyzer": "ja_ngram_search_analyzer", "analyzer": "ja_ngram_index_analyzer" } } }, "{price}": { "type": "float" }, "{availability}": { "type": "boolean" } } } } } } } Available values are items, image_features, browse, purchase, ratings, search, stats, settings, user, tasks, logs. When you define the mappings object, you should use the same keys as in the item mapper that you have built with POST /v1/mapper API.

You might not need analyzers or tokenizers for all indices. You can keep the settings field empty if it is not required. Here is an example,

json { "index_type": "search", "mappings": { "settings": {}, "mappings": { "properties": { "date": { "type": "date" } } } } }

Previous
Project setup
Next
Integration of Catalogue information and user behavior data
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/02_environment_setup/#index-creation
Skip to content
Gigalogy Tutorial
Environment setup
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Mapping creation
Item Catalogue Mapping
Example of this item mapper creation
Sample Code
User Behavior mapping
Index creation
Create Index
Delete Index
Reindex
Environment setup

To prepare GAIP for your site, there are 2 main steps

Mapping creation
Index creation

Info

These steps can also be done from our platform (GAIP). Refer here for detail.

For this, we will use API endpoints of GAIP listed in our Sandbox

You can also access our sandbox from the project setting page.

Mapping creation

The personalization engine relies on specific default keys to operate effectively. To integrate your item catalogue with our solution, it's essential to align your website's data source keys (such as item name, item description, tags, ingredients, category, etc.) with keys of personalization engine via mapping them. Our personalizer then can understand your data. This is the core part of the personalization system so the schema should be followed properly to successfully map your data.

Item Catalogue Mapping

To create mapping, use endpoints listed under Catalog Mapping in the sandbox.

GET /v1/mappers to get an existing Mapper.
PUT /v1/mappers to update an existing Mapper.
POST /v1/mappers to create a new mapper.

For set up your project mapping we will use POST /v1/mappers. You can find keys, value types, and description with an example request body in the sandbox You can simply replace the values in the example with your item catalogue keys and hit Execute to finish the mapping of your product catalogue Keys with GAIP keys.

After execution, confirm the server response is success.

Example of this item mapper creation

Here, we will use doozie shop as an example E-commerce site.

For an example item from the shop, this is how the data is structured.

json { "success": true, "result": [ { "item_id": "12345", "title": "Eco-Friendly Water Bottle", "description": "A durable, BPA-free water bottle designed for everyday use. Made from eco-friendly materials, it keeps your drink cold for up to 24 hours. Perfect for staying hydrated on the go.", "headline": "Stay Hydrated with Our Eco-Friendly Water Bottle", "availability": true, "affiliate_rate": 4, "price": 2360, "currency": "JPY", "shop_id": "hworks", "shop_name": "スマートビズ-ワイシャツ専門店-", "review_count": 2836, "review_average": 4.26, "genre_id": "206363", "brand": null, "shop_url": "https://hb.afl.rakuten.co.jp/hgc/g00rgm95.h3cpt446.g00rgm95.h3cpu696/?pc=https%3A%2F%2Fwww.rakuten.co.jp%2Fhworks%2F&m=http%3A%2F%2Fm.rakuten.co.jp%2Fhworks%2F", "item_url": "https://hb.afl.rakuten.co.jp/hgc/g00rgm95.h3cpt446.g00rgm95.h3cpu696/?pc=https%3A%2F%2Fitem.rakuten.co.jp%2Fhworks%2Fshirt-b0080%2F&m=http%3A%2F%2Fm.rakuten.co.jp%2Fhworks%2Fi%2F10000926%2F", "image_urls": [ "https://thumbnail.image.rakuten.co.jp/@0_mall/hworks/cabinet/001/014/shirt-b0080.jpg", "https://thumbnail.image.rakuten.co.jp/@0_mall/hworks/cabinet/001/014/b0080-lineup.jpg", "https://thumbnail.image.rakuten.co.jp/@0_mall/hworks/cabinet/banner/coupon202308-shdz_sq.jpg" ], "tag_ids": [ 1000903, 1008869, 1039853, 1013746 ], "tags": [], "shipping_overseas": "", "condition": 1, "genre_name": null, "parent_genre_categories": null, "shop_review_count": null, "shop_review_average": null, "tax_included": true, "point_multiplier": 1, "best_seller": false, "sale_start_time": "", "sale_end_time": "", "platform": "rakuten" } ] }

This shows the keys that doozie shop has for its products. Now we can create a mapping with GAIP keys, and use the POST /v1/mapper endpoint of GAIP. In this endpoint, we will pass the keys of above data source.

Here is an example of a mapper that we built for doozie shop, based on the keys above:

json { "key_map": { "item_id": "item_id", "title": "title", "second_title": "headline", "third_title": "shop_name", "fourth_title": "genre_name", "availability": "availability", "description": "description", "image_url": "image_urls", "image_url_type": "LIST_STR", "item_url": "item_url", "price": "price", "categories": [ { "name": "genre_id", "separator": "" } ], "custom":[], "flag": [ "condition" ] } }

Sample Code

You can find sample code for this implementation here

Once the mapper is created, you can use GET /v1/mappers endpoint to see the mapping. You can update any of mapped keys with PUT /v1/mapper endpoint and check the mapper you build from GET /v1/mapper endpoint.

User Behavior mapping

Similar to the item mapping key, there are some default keys for user behavior data.

Note

This step is required If you want to save historical user behavior data through CSV files. If you use our data collection endpoints to collect data from now on, this is not required.

You can find the Endpoints for user mapping under "Historical User Data Collection" section in the Sandbox

To implement this, please follow similar steps as above.

However, in this case please note that there are four sets of endpoints for Browsing history, purchase history, rating history, user detail. You have to create mapper for each if you want to import the data.

Index creation

In this step, you need to create indices. We need multiple indices to run recommender solution successfully. These indices will create the necessary schemas to hold your data.

There are 3 endpoints here

POST /v1/index/create --> Create indices to hold your data

DELETE /v1/index/delete --> Delete indices

POST /v1/reindex --> Creates index with new mappings and settings and create alias for new index

Create Index

Request endpoint

POST /v1/index/create Simply use your project key and API and click execute to create the indices for your project. Note that this will throw an error if the mapping in the previous step is not done correctly.

After the successful execution all the necessary index will be created and item index will be created in the background. You can check the status of item index creation with a task id from GET /v1/tasks/{task_id} API at the bottom of the page.

Please confirm the task was success.

Delete Index

You can delete an existing index with this endpoint.

Request endpoint

DELETE /v1/index/delete Available values are items, image_features, browse, purchase, ratings, search, stats, settings, user, tasks, logs.

If you Delete any index, please ensure to create the index again, unless you will get error when trying to input data/item catalogue or run training.

Reindex

In Elastic search, reindexing is the process of copying data from one index to another, either within the same cluster or to a different cluster. This can be useful in a variety of situations, such as:

Updating the mapping of an index: If you need to make changes to the mapping of an index, you can create a new index with the updated mapping and then reindex the data from the old index to the new one.

Moving data from one index to another: If you need to move data from one index to another, you can reindex the data from the source index to the destination index.

Updating the data with new data: If you have updated data that you want to add to an index, you can reindex the data with the updated data.

Changing the shard count of an index: If you need to change the number of shards that an index is using, you can reindex the data to a new index with the desired number of shards.

We can use Reindex API to copy data from index to another index.

Request Endpoint: POST /v1/reindex

Here is an example how to pass mappings and settings in request body:

json { "index_type": "items", "mappings": { "settings": { "analysis": { "char_filter": { "normalize": { "type": "icu_normalizer", "name": "nfkc", "mode": "compose" } }, "tokenizer": { "ja_kuromoji_tokenizer": { "mode": "search", "type": "kuromoji_tokenizer", "discard_compound_token": "true", "user_dictionary_rules": [] }, "ja_ngram_tokenizer": { "type": "ngram", "min_gram": 2, "max_gram": 3, "token_chars": [ "letter", "digit" ] } }, "filter": { "ja_index_synonym": { "type": "synonym", "lenient": "false", "synonyms": [] } }, "analyzer": { "ja_kuromoji_index_analyzer": { "type": "custom", "char_filter": [ "normalize" ], "tokenizer": "ja_kuromoji_tokenizer", "filter": [ "kuromoji_baseform", "kuromoji_part_of_speech", "ja_index_synonym", "cjk_width", "ja_stop", "kuromoji_stemmer", "lowercase" ] }, "ja_kuromoji_search_analyzer": { "type": "custom", "char_filter": [ "normalize" ], "tokenizer": "ja_kuromoji_tokenizer", "filter": [ "kuromoji_baseform", "kuromoji_part_of_speech", "cjk_width", "ja_stop", "kuromoji_stemmer", "lowercase" ] }, "ja_ngram_index_analyzer": { "type": "custom", "char_filter": [ "normalize" ], "tokenizer": "ja_ngram_tokenizer", "filter": [ "lowercase" ] }, "ja_ngram_search_analyzer": { "type": "custom", "char_filter": [ "normalize" ], "tokenizer": "ja_ngram_tokenizer", "filter": [ "lowercase" ] } } } }, "mappings": { "properties": { "item": { "properties": { "{title}": { "type": "text", "search_analyzer": "ja_kuromoji_search_analyzer", "analyzer": "ja_kuromoji_index_analyzer", "fields": { "ngram": { "type": "text", "search_analyzer": "ja_ngram_search_analyzer", "analyzer": "ja_ngram_index_analyzer" } } }, "{second_title}": { "type": "text", "search_analyzer": "ja_kuromoji_search_analyzer", "analyzer": "ja_kuromoji_index_analyzer", "fields": { "ngram": { "type": "text", "search_analyzer": "ja_ngram_search_analyzer", "analyzer": "ja_ngram_index_analyzer" } } }, "{third_title}": { "type": "text", "search_analyzer": "ja_kuromoji_search_analyzer", "analyzer": "ja_kuromoji_index_analyzer", "fields": { "ngram": { "type": "text", "search_analyzer": "ja_ngram_search_analyzer", "analyzer": "ja_ngram_index_analyzer" } } }, "{description}": { "type": "text", "search_analyzer": "ja_kuromoji_search_analyzer", "analyzer": "ja_kuromoji_index_analyzer", "fields": { "ngram": { "type": "text", "search_analyzer": "ja_ngram_search_analyzer", "analyzer": "ja_ngram_index_analyzer" } } }, "{price}": { "type": "float" }, "{availability}": { "type": "boolean" } } } } } } } Available values are items, image_features, browse, purchase, ratings, search, stats, settings, user, tasks, logs. When you define the mappings object, you should use the same keys as in the item mapper that you have built with POST /v1/mapper API.

You might not need analyzers or tokenizers for all indices. You can keep the settings field empty if it is not required. Here is an example,

json { "index_type": "search", "mappings": { "settings": {}, "mappings": { "properties": { "date": { "type": "date" } } } } }

Previous
Project setup
Next
Integration of Catalogue information and user behavior data
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/02_environment_setup/#create-index
Skip to content
Gigalogy Tutorial
Environment setup
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Mapping creation
Item Catalogue Mapping
Example of this item mapper creation
Sample Code
User Behavior mapping
Index creation
Create Index
Delete Index
Reindex
Environment setup

To prepare GAIP for your site, there are 2 main steps

Mapping creation
Index creation

Info

These steps can also be done from our platform (GAIP). Refer here for detail.

For this, we will use API endpoints of GAIP listed in our Sandbox

You can also access our sandbox from the project setting page.

Mapping creation

The personalization engine relies on specific default keys to operate effectively. To integrate your item catalogue with our solution, it's essential to align your website's data source keys (such as item name, item description, tags, ingredients, category, etc.) with keys of personalization engine via mapping them. Our personalizer then can understand your data. This is the core part of the personalization system so the schema should be followed properly to successfully map your data.

Item Catalogue Mapping

To create mapping, use endpoints listed under Catalog Mapping in the sandbox.

GET /v1/mappers to get an existing Mapper.
PUT /v1/mappers to update an existing Mapper.
POST /v1/mappers to create a new mapper.

For set up your project mapping we will use POST /v1/mappers. You can find keys, value types, and description with an example request body in the sandbox You can simply replace the values in the example with your item catalogue keys and hit Execute to finish the mapping of your product catalogue Keys with GAIP keys.

After execution, confirm the server response is success.

Example of this item mapper creation

Here, we will use doozie shop as an example E-commerce site.

For an example item from the shop, this is how the data is structured.

json { "success": true, "result": [ { "item_id": "12345", "title": "Eco-Friendly Water Bottle", "description": "A durable, BPA-free water bottle designed for everyday use. Made from eco-friendly materials, it keeps your drink cold for up to 24 hours. Perfect for staying hydrated on the go.", "headline": "Stay Hydrated with Our Eco-Friendly Water Bottle", "availability": true, "affiliate_rate": 4, "price": 2360, "currency": "JPY", "shop_id": "hworks", "shop_name": "スマートビズ-ワイシャツ専門店-", "review_count": 2836, "review_average": 4.26, "genre_id": "206363", "brand": null, "shop_url": "https://hb.afl.rakuten.co.jp/hgc/g00rgm95.h3cpt446.g00rgm95.h3cpu696/?pc=https%3A%2F%2Fwww.rakuten.co.jp%2Fhworks%2F&m=http%3A%2F%2Fm.rakuten.co.jp%2Fhworks%2F", "item_url": "https://hb.afl.rakuten.co.jp/hgc/g00rgm95.h3cpt446.g00rgm95.h3cpu696/?pc=https%3A%2F%2Fitem.rakuten.co.jp%2Fhworks%2Fshirt-b0080%2F&m=http%3A%2F%2Fm.rakuten.co.jp%2Fhworks%2Fi%2F10000926%2F", "image_urls": [ "https://thumbnail.image.rakuten.co.jp/@0_mall/hworks/cabinet/001/014/shirt-b0080.jpg", "https://thumbnail.image.rakuten.co.jp/@0_mall/hworks/cabinet/001/014/b0080-lineup.jpg", "https://thumbnail.image.rakuten.co.jp/@0_mall/hworks/cabinet/banner/coupon202308-shdz_sq.jpg" ], "tag_ids": [ 1000903, 1008869, 1039853, 1013746 ], "tags": [], "shipping_overseas": "", "condition": 1, "genre_name": null, "parent_genre_categories": null, "shop_review_count": null, "shop_review_average": null, "tax_included": true, "point_multiplier": 1, "best_seller": false, "sale_start_time": "", "sale_end_time": "", "platform": "rakuten" } ] }

This shows the keys that doozie shop has for its products. Now we can create a mapping with GAIP keys, and use the POST /v1/mapper endpoint of GAIP. In this endpoint, we will pass the keys of above data source.

Here is an example of a mapper that we built for doozie shop, based on the keys above:

json { "key_map": { "item_id": "item_id", "title": "title", "second_title": "headline", "third_title": "shop_name", "fourth_title": "genre_name", "availability": "availability", "description": "description", "image_url": "image_urls", "image_url_type": "LIST_STR", "item_url": "item_url", "price": "price", "categories": [ { "name": "genre_id", "separator": "" } ], "custom":[], "flag": [ "condition" ] } }

Sample Code

You can find sample code for this implementation here

Once the mapper is created, you can use GET /v1/mappers endpoint to see the mapping. You can update any of mapped keys with PUT /v1/mapper endpoint and check the mapper you build from GET /v1/mapper endpoint.

User Behavior mapping

Similar to the item mapping key, there are some default keys for user behavior data.

Note

This step is required If you want to save historical user behavior data through CSV files. If you use our data collection endpoints to collect data from now on, this is not required.

You can find the Endpoints for user mapping under "Historical User Data Collection" section in the Sandbox

To implement this, please follow similar steps as above.

However, in this case please note that there are four sets of endpoints for Browsing history, purchase history, rating history, user detail. You have to create mapper for each if you want to import the data.

Index creation

In this step, you need to create indices. We need multiple indices to run recommender solution successfully. These indices will create the necessary schemas to hold your data.

There are 3 endpoints here

POST /v1/index/create --> Create indices to hold your data

DELETE /v1/index/delete --> Delete indices

POST /v1/reindex --> Creates index with new mappings and settings and create alias for new index

Create Index

Request endpoint

POST /v1/index/create Simply use your project key and API and click execute to create the indices for your project. Note that this will throw an error if the mapping in the previous step is not done correctly.

After the successful execution all the necessary index will be created and item index will be created in the background. You can check the status of item index creation with a task id from GET /v1/tasks/{task_id} API at the bottom of the page.

Please confirm the task was success.

Delete Index

You can delete an existing index with this endpoint.

Request endpoint

DELETE /v1/index/delete Available values are items, image_features, browse, purchase, ratings, search, stats, settings, user, tasks, logs.

If you Delete any index, please ensure to create the index again, unless you will get error when trying to input data/item catalogue or run training.

Reindex

In Elastic search, reindexing is the process of copying data from one index to another, either within the same cluster or to a different cluster. This can be useful in a variety of situations, such as:

Updating the mapping of an index: If you need to make changes to the mapping of an index, you can create a new index with the updated mapping and then reindex the data from the old index to the new one.

Moving data from one index to another: If you need to move data from one index to another, you can reindex the data from the source index to the destination index.

Updating the data with new data: If you have updated data that you want to add to an index, you can reindex the data with the updated data.

Changing the shard count of an index: If you need to change the number of shards that an index is using, you can reindex the data to a new index with the desired number of shards.

We can use Reindex API to copy data from index to another index.

Request Endpoint: POST /v1/reindex

Here is an example how to pass mappings and settings in request body:

json { "index_type": "items", "mappings": { "settings": { "analysis": { "char_filter": { "normalize": { "type": "icu_normalizer", "name": "nfkc", "mode": "compose" } }, "tokenizer": { "ja_kuromoji_tokenizer": { "mode": "search", "type": "kuromoji_tokenizer", "discard_compound_token": "true", "user_dictionary_rules": [] }, "ja_ngram_tokenizer": { "type": "ngram", "min_gram": 2, "max_gram": 3, "token_chars": [ "letter", "digit" ] } }, "filter": { "ja_index_synonym": { "type": "synonym", "lenient": "false", "synonyms": [] } }, "analyzer": { "ja_kuromoji_index_analyzer": { "type": "custom", "char_filter": [ "normalize" ], "tokenizer": "ja_kuromoji_tokenizer", "filter": [ "kuromoji_baseform", "kuromoji_part_of_speech", "ja_index_synonym", "cjk_width", "ja_stop", "kuromoji_stemmer", "lowercase" ] }, "ja_kuromoji_search_analyzer": { "type": "custom", "char_filter": [ "normalize" ], "tokenizer": "ja_kuromoji_tokenizer", "filter": [ "kuromoji_baseform", "kuromoji_part_of_speech", "cjk_width", "ja_stop", "kuromoji_stemmer", "lowercase" ] }, "ja_ngram_index_analyzer": { "type": "custom", "char_filter": [ "normalize" ], "tokenizer": "ja_ngram_tokenizer", "filter": [ "lowercase" ] }, "ja_ngram_search_analyzer": { "type": "custom", "char_filter": [ "normalize" ], "tokenizer": "ja_ngram_tokenizer", "filter": [ "lowercase" ] } } } }, "mappings": { "properties": { "item": { "properties": { "{title}": { "type": "text", "search_analyzer": "ja_kuromoji_search_analyzer", "analyzer": "ja_kuromoji_index_analyzer", "fields": { "ngram": { "type": "text", "search_analyzer": "ja_ngram_search_analyzer", "analyzer": "ja_ngram_index_analyzer" } } }, "{second_title}": { "type": "text", "search_analyzer": "ja_kuromoji_search_analyzer", "analyzer": "ja_kuromoji_index_analyzer", "fields": { "ngram": { "type": "text", "search_analyzer": "ja_ngram_search_analyzer", "analyzer": "ja_ngram_index_analyzer" } } }, "{third_title}": { "type": "text", "search_analyzer": "ja_kuromoji_search_analyzer", "analyzer": "ja_kuromoji_index_analyzer", "fields": { "ngram": { "type": "text", "search_analyzer": "ja_ngram_search_analyzer", "analyzer": "ja_ngram_index_analyzer" } } }, "{description}": { "type": "text", "search_analyzer": "ja_kuromoji_search_analyzer", "analyzer": "ja_kuromoji_index_analyzer", "fields": { "ngram": { "type": "text", "search_analyzer": "ja_ngram_search_analyzer", "analyzer": "ja_ngram_index_analyzer" } } }, "{price}": { "type": "float" }, "{availability}": { "type": "boolean" } } } } } } } Available values are items, image_features, browse, purchase, ratings, search, stats, settings, user, tasks, logs. When you define the mappings object, you should use the same keys as in the item mapper that you have built with POST /v1/mapper API.

You might not need analyzers or tokenizers for all indices. You can keep the settings field empty if it is not required. Here is an example,

json { "index_type": "search", "mappings": { "settings": {}, "mappings": { "properties": { "date": { "type": "date" } } } } }

Previous
Project setup
Next
Integration of Catalogue information and user behavior data
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/02_environment_setup/#delete-index
Skip to content
Gigalogy Tutorial
Environment setup
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Mapping creation
Item Catalogue Mapping
Example of this item mapper creation
Sample Code
User Behavior mapping
Index creation
Create Index
Delete Index
Reindex
Environment setup

To prepare GAIP for your site, there are 2 main steps

Mapping creation
Index creation

Info

These steps can also be done from our platform (GAIP). Refer here for detail.

For this, we will use API endpoints of GAIP listed in our Sandbox

You can also access our sandbox from the project setting page.

Mapping creation

The personalization engine relies on specific default keys to operate effectively. To integrate your item catalogue with our solution, it's essential to align your website's data source keys (such as item name, item description, tags, ingredients, category, etc.) with keys of personalization engine via mapping them. Our personalizer then can understand your data. This is the core part of the personalization system so the schema should be followed properly to successfully map your data.

Item Catalogue Mapping

To create mapping, use endpoints listed under Catalog Mapping in the sandbox.

GET /v1/mappers to get an existing Mapper.
PUT /v1/mappers to update an existing Mapper.
POST /v1/mappers to create a new mapper.

For set up your project mapping we will use POST /v1/mappers. You can find keys, value types, and description with an example request body in the sandbox You can simply replace the values in the example with your item catalogue keys and hit Execute to finish the mapping of your product catalogue Keys with GAIP keys.

After execution, confirm the server response is success.

Example of this item mapper creation

Here, we will use doozie shop as an example E-commerce site.

For an example item from the shop, this is how the data is structured.

json { "success": true, "result": [ { "item_id": "12345", "title": "Eco-Friendly Water Bottle", "description": "A durable, BPA-free water bottle designed for everyday use. Made from eco-friendly materials, it keeps your drink cold for up to 24 hours. Perfect for staying hydrated on the go.", "headline": "Stay Hydrated with Our Eco-Friendly Water Bottle", "availability": true, "affiliate_rate": 4, "price": 2360, "currency": "JPY", "shop_id": "hworks", "shop_name": "スマートビズ-ワイシャツ専門店-", "review_count": 2836, "review_average": 4.26, "genre_id": "206363", "brand": null, "shop_url": "https://hb.afl.rakuten.co.jp/hgc/g00rgm95.h3cpt446.g00rgm95.h3cpu696/?pc=https%3A%2F%2Fwww.rakuten.co.jp%2Fhworks%2F&m=http%3A%2F%2Fm.rakuten.co.jp%2Fhworks%2F", "item_url": "https://hb.afl.rakuten.co.jp/hgc/g00rgm95.h3cpt446.g00rgm95.h3cpu696/?pc=https%3A%2F%2Fitem.rakuten.co.jp%2Fhworks%2Fshirt-b0080%2F&m=http%3A%2F%2Fm.rakuten.co.jp%2Fhworks%2Fi%2F10000926%2F", "image_urls": [ "https://thumbnail.image.rakuten.co.jp/@0_mall/hworks/cabinet/001/014/shirt-b0080.jpg", "https://thumbnail.image.rakuten.co.jp/@0_mall/hworks/cabinet/001/014/b0080-lineup.jpg", "https://thumbnail.image.rakuten.co.jp/@0_mall/hworks/cabinet/banner/coupon202308-shdz_sq.jpg" ], "tag_ids": [ 1000903, 1008869, 1039853, 1013746 ], "tags": [], "shipping_overseas": "", "condition": 1, "genre_name": null, "parent_genre_categories": null, "shop_review_count": null, "shop_review_average": null, "tax_included": true, "point_multiplier": 1, "best_seller": false, "sale_start_time": "", "sale_end_time": "", "platform": "rakuten" } ] }

This shows the keys that doozie shop has for its products. Now we can create a mapping with GAIP keys, and use the POST /v1/mapper endpoint of GAIP. In this endpoint, we will pass the keys of above data source.

Here is an example of a mapper that we built for doozie shop, based on the keys above:

json { "key_map": { "item_id": "item_id", "title": "title", "second_title": "headline", "third_title": "shop_name", "fourth_title": "genre_name", "availability": "availability", "description": "description", "image_url": "image_urls", "image_url_type": "LIST_STR", "item_url": "item_url", "price": "price", "categories": [ { "name": "genre_id", "separator": "" } ], "custom":[], "flag": [ "condition" ] } }

Sample Code

You can find sample code for this implementation here

Once the mapper is created, you can use GET /v1/mappers endpoint to see the mapping. You can update any of mapped keys with PUT /v1/mapper endpoint and check the mapper you build from GET /v1/mapper endpoint.

User Behavior mapping

Similar to the item mapping key, there are some default keys for user behavior data.

Note

This step is required If you want to save historical user behavior data through CSV files. If you use our data collection endpoints to collect data from now on, this is not required.

You can find the Endpoints for user mapping under "Historical User Data Collection" section in the Sandbox

To implement this, please follow similar steps as above.

However, in this case please note that there are four sets of endpoints for Browsing history, purchase history, rating history, user detail. You have to create mapper for each if you want to import the data.

Index creation

In this step, you need to create indices. We need multiple indices to run recommender solution successfully. These indices will create the necessary schemas to hold your data.

There are 3 endpoints here

POST /v1/index/create --> Create indices to hold your data

DELETE /v1/index/delete --> Delete indices

POST /v1/reindex --> Creates index with new mappings and settings and create alias for new index

Create Index

Request endpoint

POST /v1/index/create Simply use your project key and API and click execute to create the indices for your project. Note that this will throw an error if the mapping in the previous step is not done correctly.

After the successful execution all the necessary index will be created and item index will be created in the background. You can check the status of item index creation with a task id from GET /v1/tasks/{task_id} API at the bottom of the page.

Please confirm the task was success.

Delete Index

You can delete an existing index with this endpoint.

Request endpoint

DELETE /v1/index/delete Available values are items, image_features, browse, purchase, ratings, search, stats, settings, user, tasks, logs.

If you Delete any index, please ensure to create the index again, unless you will get error when trying to input data/item catalogue or run training.

Reindex

In Elastic search, reindexing is the process of copying data from one index to another, either within the same cluster or to a different cluster. This can be useful in a variety of situations, such as:

Updating the mapping of an index: If you need to make changes to the mapping of an index, you can create a new index with the updated mapping and then reindex the data from the old index to the new one.

Moving data from one index to another: If you need to move data from one index to another, you can reindex the data from the source index to the destination index.

Updating the data with new data: If you have updated data that you want to add to an index, you can reindex the data with the updated data.

Changing the shard count of an index: If you need to change the number of shards that an index is using, you can reindex the data to a new index with the desired number of shards.

We can use Reindex API to copy data from index to another index.

Request Endpoint: POST /v1/reindex

Here is an example how to pass mappings and settings in request body:

json { "index_type": "items", "mappings": { "settings": { "analysis": { "char_filter": { "normalize": { "type": "icu_normalizer", "name": "nfkc", "mode": "compose" } }, "tokenizer": { "ja_kuromoji_tokenizer": { "mode": "search", "type": "kuromoji_tokenizer", "discard_compound_token": "true", "user_dictionary_rules": [] }, "ja_ngram_tokenizer": { "type": "ngram", "min_gram": 2, "max_gram": 3, "token_chars": [ "letter", "digit" ] } }, "filter": { "ja_index_synonym": { "type": "synonym", "lenient": "false", "synonyms": [] } }, "analyzer": { "ja_kuromoji_index_analyzer": { "type": "custom", "char_filter": [ "normalize" ], "tokenizer": "ja_kuromoji_tokenizer", "filter": [ "kuromoji_baseform", "kuromoji_part_of_speech", "ja_index_synonym", "cjk_width", "ja_stop", "kuromoji_stemmer", "lowercase" ] }, "ja_kuromoji_search_analyzer": { "type": "custom", "char_filter": [ "normalize" ], "tokenizer": "ja_kuromoji_tokenizer", "filter": [ "kuromoji_baseform", "kuromoji_part_of_speech", "cjk_width", "ja_stop", "kuromoji_stemmer", "lowercase" ] }, "ja_ngram_index_analyzer": { "type": "custom", "char_filter": [ "normalize" ], "tokenizer": "ja_ngram_tokenizer", "filter": [ "lowercase" ] }, "ja_ngram_search_analyzer": { "type": "custom", "char_filter": [ "normalize" ], "tokenizer": "ja_ngram_tokenizer", "filter": [ "lowercase" ] } } } }, "mappings": { "properties": { "item": { "properties": { "{title}": { "type": "text", "search_analyzer": "ja_kuromoji_search_analyzer", "analyzer": "ja_kuromoji_index_analyzer", "fields": { "ngram": { "type": "text", "search_analyzer": "ja_ngram_search_analyzer", "analyzer": "ja_ngram_index_analyzer" } } }, "{second_title}": { "type": "text", "search_analyzer": "ja_kuromoji_search_analyzer", "analyzer": "ja_kuromoji_index_analyzer", "fields": { "ngram": { "type": "text", "search_analyzer": "ja_ngram_search_analyzer", "analyzer": "ja_ngram_index_analyzer" } } }, "{third_title}": { "type": "text", "search_analyzer": "ja_kuromoji_search_analyzer", "analyzer": "ja_kuromoji_index_analyzer", "fields": { "ngram": { "type": "text", "search_analyzer": "ja_ngram_search_analyzer", "analyzer": "ja_ngram_index_analyzer" } } }, "{description}": { "type": "text", "search_analyzer": "ja_kuromoji_search_analyzer", "analyzer": "ja_kuromoji_index_analyzer", "fields": { "ngram": { "type": "text", "search_analyzer": "ja_ngram_search_analyzer", "analyzer": "ja_ngram_index_analyzer" } } }, "{price}": { "type": "float" }, "{availability}": { "type": "boolean" } } } } } } } Available values are items, image_features, browse, purchase, ratings, search, stats, settings, user, tasks, logs. When you define the mappings object, you should use the same keys as in the item mapper that you have built with POST /v1/mapper API.

You might not need analyzers or tokenizers for all indices. You can keep the settings field empty if it is not required. Here is an example,

json { "index_type": "search", "mappings": { "settings": {}, "mappings": { "properties": { "date": { "type": "date" } } } } }

Previous
Project setup
Next
Integration of Catalogue information and user behavior data
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/02_environment_setup/#reindex
Skip to content
Gigalogy Tutorial
Environment setup
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Mapping creation
Item Catalogue Mapping
Example of this item mapper creation
Sample Code
User Behavior mapping
Index creation
Create Index
Delete Index
Reindex
Environment setup

To prepare GAIP for your site, there are 2 main steps

Mapping creation
Index creation

Info

These steps can also be done from our platform (GAIP). Refer here for detail.

For this, we will use API endpoints of GAIP listed in our Sandbox

You can also access our sandbox from the project setting page.

Mapping creation

The personalization engine relies on specific default keys to operate effectively. To integrate your item catalogue with our solution, it's essential to align your website's data source keys (such as item name, item description, tags, ingredients, category, etc.) with keys of personalization engine via mapping them. Our personalizer then can understand your data. This is the core part of the personalization system so the schema should be followed properly to successfully map your data.

Item Catalogue Mapping

To create mapping, use endpoints listed under Catalog Mapping in the sandbox.

GET /v1/mappers to get an existing Mapper.
PUT /v1/mappers to update an existing Mapper.
POST /v1/mappers to create a new mapper.

For set up your project mapping we will use POST /v1/mappers. You can find keys, value types, and description with an example request body in the sandbox You can simply replace the values in the example with your item catalogue keys and hit Execute to finish the mapping of your product catalogue Keys with GAIP keys.

After execution, confirm the server response is success.

Example of this item mapper creation

Here, we will use doozie shop as an example E-commerce site.

For an example item from the shop, this is how the data is structured.

json { "success": true, "result": [ { "item_id": "12345", "title": "Eco-Friendly Water Bottle", "description": "A durable, BPA-free water bottle designed for everyday use. Made from eco-friendly materials, it keeps your drink cold for up to 24 hours. Perfect for staying hydrated on the go.", "headline": "Stay Hydrated with Our Eco-Friendly Water Bottle", "availability": true, "affiliate_rate": 4, "price": 2360, "currency": "JPY", "shop_id": "hworks", "shop_name": "スマートビズ-ワイシャツ専門店-", "review_count": 2836, "review_average": 4.26, "genre_id": "206363", "brand": null, "shop_url": "https://hb.afl.rakuten.co.jp/hgc/g00rgm95.h3cpt446.g00rgm95.h3cpu696/?pc=https%3A%2F%2Fwww.rakuten.co.jp%2Fhworks%2F&m=http%3A%2F%2Fm.rakuten.co.jp%2Fhworks%2F", "item_url": "https://hb.afl.rakuten.co.jp/hgc/g00rgm95.h3cpt446.g00rgm95.h3cpu696/?pc=https%3A%2F%2Fitem.rakuten.co.jp%2Fhworks%2Fshirt-b0080%2F&m=http%3A%2F%2Fm.rakuten.co.jp%2Fhworks%2Fi%2F10000926%2F", "image_urls": [ "https://thumbnail.image.rakuten.co.jp/@0_mall/hworks/cabinet/001/014/shirt-b0080.jpg", "https://thumbnail.image.rakuten.co.jp/@0_mall/hworks/cabinet/001/014/b0080-lineup.jpg", "https://thumbnail.image.rakuten.co.jp/@0_mall/hworks/cabinet/banner/coupon202308-shdz_sq.jpg" ], "tag_ids": [ 1000903, 1008869, 1039853, 1013746 ], "tags": [], "shipping_overseas": "", "condition": 1, "genre_name": null, "parent_genre_categories": null, "shop_review_count": null, "shop_review_average": null, "tax_included": true, "point_multiplier": 1, "best_seller": false, "sale_start_time": "", "sale_end_time": "", "platform": "rakuten" } ] }

This shows the keys that doozie shop has for its products. Now we can create a mapping with GAIP keys, and use the POST /v1/mapper endpoint of GAIP. In this endpoint, we will pass the keys of above data source.

Here is an example of a mapper that we built for doozie shop, based on the keys above:

json { "key_map": { "item_id": "item_id", "title": "title", "second_title": "headline", "third_title": "shop_name", "fourth_title": "genre_name", "availability": "availability", "description": "description", "image_url": "image_urls", "image_url_type": "LIST_STR", "item_url": "item_url", "price": "price", "categories": [ { "name": "genre_id", "separator": "" } ], "custom":[], "flag": [ "condition" ] } }

Sample Code

You can find sample code for this implementation here

Once the mapper is created, you can use GET /v1/mappers endpoint to see the mapping. You can update any of mapped keys with PUT /v1/mapper endpoint and check the mapper you build from GET /v1/mapper endpoint.

User Behavior mapping

Similar to the item mapping key, there are some default keys for user behavior data.

Note

This step is required If you want to save historical user behavior data through CSV files. If you use our data collection endpoints to collect data from now on, this is not required.

You can find the Endpoints for user mapping under "Historical User Data Collection" section in the Sandbox

To implement this, please follow similar steps as above.

However, in this case please note that there are four sets of endpoints for Browsing history, purchase history, rating history, user detail. You have to create mapper for each if you want to import the data.

Index creation

In this step, you need to create indices. We need multiple indices to run recommender solution successfully. These indices will create the necessary schemas to hold your data.

There are 3 endpoints here

POST /v1/index/create --> Create indices to hold your data

DELETE /v1/index/delete --> Delete indices

POST /v1/reindex --> Creates index with new mappings and settings and create alias for new index

Create Index

Request endpoint

POST /v1/index/create Simply use your project key and API and click execute to create the indices for your project. Note that this will throw an error if the mapping in the previous step is not done correctly.

After the successful execution all the necessary index will be created and item index will be created in the background. You can check the status of item index creation with a task id from GET /v1/tasks/{task_id} API at the bottom of the page.

Please confirm the task was success.

Delete Index

You can delete an existing index with this endpoint.

Request endpoint

DELETE /v1/index/delete Available values are items, image_features, browse, purchase, ratings, search, stats, settings, user, tasks, logs.

If you Delete any index, please ensure to create the index again, unless you will get error when trying to input data/item catalogue or run training.

Reindex

In Elastic search, reindexing is the process of copying data from one index to another, either within the same cluster or to a different cluster. This can be useful in a variety of situations, such as:

Updating the mapping of an index: If you need to make changes to the mapping of an index, you can create a new index with the updated mapping and then reindex the data from the old index to the new one.

Moving data from one index to another: If you need to move data from one index to another, you can reindex the data from the source index to the destination index.

Updating the data with new data: If you have updated data that you want to add to an index, you can reindex the data with the updated data.

Changing the shard count of an index: If you need to change the number of shards that an index is using, you can reindex the data to a new index with the desired number of shards.

We can use Reindex API to copy data from index to another index.

Request Endpoint: POST /v1/reindex

Here is an example how to pass mappings and settings in request body:

json { "index_type": "items", "mappings": { "settings": { "analysis": { "char_filter": { "normalize": { "type": "icu_normalizer", "name": "nfkc", "mode": "compose" } }, "tokenizer": { "ja_kuromoji_tokenizer": { "mode": "search", "type": "kuromoji_tokenizer", "discard_compound_token": "true", "user_dictionary_rules": [] }, "ja_ngram_tokenizer": { "type": "ngram", "min_gram": 2, "max_gram": 3, "token_chars": [ "letter", "digit" ] } }, "filter": { "ja_index_synonym": { "type": "synonym", "lenient": "false", "synonyms": [] } }, "analyzer": { "ja_kuromoji_index_analyzer": { "type": "custom", "char_filter": [ "normalize" ], "tokenizer": "ja_kuromoji_tokenizer", "filter": [ "kuromoji_baseform", "kuromoji_part_of_speech", "ja_index_synonym", "cjk_width", "ja_stop", "kuromoji_stemmer", "lowercase" ] }, "ja_kuromoji_search_analyzer": { "type": "custom", "char_filter": [ "normalize" ], "tokenizer": "ja_kuromoji_tokenizer", "filter": [ "kuromoji_baseform", "kuromoji_part_of_speech", "cjk_width", "ja_stop", "kuromoji_stemmer", "lowercase" ] }, "ja_ngram_index_analyzer": { "type": "custom", "char_filter": [ "normalize" ], "tokenizer": "ja_ngram_tokenizer", "filter": [ "lowercase" ] }, "ja_ngram_search_analyzer": { "type": "custom", "char_filter": [ "normalize" ], "tokenizer": "ja_ngram_tokenizer", "filter": [ "lowercase" ] } } } }, "mappings": { "properties": { "item": { "properties": { "{title}": { "type": "text", "search_analyzer": "ja_kuromoji_search_analyzer", "analyzer": "ja_kuromoji_index_analyzer", "fields": { "ngram": { "type": "text", "search_analyzer": "ja_ngram_search_analyzer", "analyzer": "ja_ngram_index_analyzer" } } }, "{second_title}": { "type": "text", "search_analyzer": "ja_kuromoji_search_analyzer", "analyzer": "ja_kuromoji_index_analyzer", "fields": { "ngram": { "type": "text", "search_analyzer": "ja_ngram_search_analyzer", "analyzer": "ja_ngram_index_analyzer" } } }, "{third_title}": { "type": "text", "search_analyzer": "ja_kuromoji_search_analyzer", "analyzer": "ja_kuromoji_index_analyzer", "fields": { "ngram": { "type": "text", "search_analyzer": "ja_ngram_search_analyzer", "analyzer": "ja_ngram_index_analyzer" } } }, "{description}": { "type": "text", "search_analyzer": "ja_kuromoji_search_analyzer", "analyzer": "ja_kuromoji_index_analyzer", "fields": { "ngram": { "type": "text", "search_analyzer": "ja_ngram_search_analyzer", "analyzer": "ja_ngram_index_analyzer" } } }, "{price}": { "type": "float" }, "{availability}": { "type": "boolean" } } } } } } } Available values are items, image_features, browse, purchase, ratings, search, stats, settings, user, tasks, logs. When you define the mappings object, you should use the same keys as in the item mapper that you have built with POST /v1/mapper API.

You might not need analyzers or tokenizers for all indices. You can keep the settings field empty if it is not required. Here is an example,

json { "index_type": "search", "mappings": { "settings": {}, "mappings": { "properties": { "date": { "type": "date" } } } } }

Previous
Project setup
Next
Integration of Catalogue information and user behavior data
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/03_Data_integration_%26_user_behavior_collection/
Skip to content
Gigalogy Tutorial
Integration of Catalogue information and user behavior data
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Catalogue information integration.
Uploading data using a file
Fetch item from external API
Search Items in GAIP after import
User behavior data collection integration
Comparison of each approach.
Google Tag manager
Generating user ID
Collecting and sending user browsing data
Collecting and sending user purchase data
Collecting and sending user rating data
Collecting and sending user data
Server to server integration
User information
Product browse
Product purchase
Product rating
Client to server integration
Import user behavior data
Integration of Catalogue information and user behavior data

This tutorial will cover how to integrate your catalogue information into GAIP and how to set up user behavior tracking and integrate with GAIP.

Catalogue information integration.

Prerequisite: Mapping creation and the index creation is done.

Info

This step can also be done from our platform (GAIP). Refer here for detail.

There are two ways to import your catalogue information into GAIP.

Upload you catalogue information as a CSV or JSON file using endpoint POST /v1/item/save.
Fetch data from your API (or any external API) using endpoint POST /v1/item/save/remote.
Uploading data using a file

For uploading the data using a CSV file or JSON file, please use the POST /v1/item/save endpoint. Simply upload the file and confirm the server response is success. Confirm the task was successful using the GET/v1/tasks/{task_id} endpoint.

Info

This will throw an error and task will fail, if the keys during the Mapping creation step does not match with the keys in the file, OR if the indices were not created succesfully.

Fetch item from external API

To fetch data from external API, use the POST /v1/item/save/remote endpoint. The key and value types and an example request body for the endpoint can be found here in our sandbox.

After hitting either of the endpoint above to import your data into GAIP, you will get a task ID in the response. Use this task ID and hit the /v1/tasks/{task_id} endpoint to confirm the operation was successful. In case it fails, you can also find the details there. For API documentation, please refer <>

Search Items in GAIP after import

You can search items by passing list of item ids fromPOST /v1/items/search endpoint. This endpoint will return searched items with item details.

It is recommended to use this endpoint to confirm that the item catalogue is successfully imported into your project.

User behavior data collection integration

GAIP can collect different user behavior related information to optimize the recommendation for the user. Types of data collected are listed below with their endpoints.

Data type	Endpoint
Product browsing: When user browse products.	/v1/items/browse or /v1/items/browse/client
Product Purchase: When user purchase a product with its quantity.	/v1/items/purchase or /v1/items/purchase/client
Rating: When a user rates a product.	/v1/items/rating or /v1/items/rating/client
User: User information such as age, gender and other customized attributes depending on your website.	/v1/users or /v1/users/client

You will find these endpoints listed in our Sandbox under section "User Data Collection". Please check the required parameters, value types and example request bodies for all the endpoints there.

There are 3 ways to integrate user behavior data collection with GAIP

Google Tag Manager
Server to server integration
Client to server integration

You can also bulk upload user behavior data from the past. For that, please refer to Import user behavior data section.

Comparison of each approach.
Approach	GTM	Server to Server	Client to Server
Description	Use Google Tag Manager (GTM) to collect data (User behavior) from your website and send it to GAIP via endpoint.	The data is captured in the backend server of your application and then sent to GAIP via endpoint.	The data is directly sent from your front end (Client side) to GAIP via endpoint.
Pros	Easy to implement, Minimum coding required, Flexible configuration	More secure, Data integrity, Controlled environment	Real-time data, Less server dependency
Cons	Limited customization, need to have basic knowledge about GTM, Dependency on third-party service, Might not work for certain browsers and plugins like AdBlockers	More complex to set up, potential latency, maintenance required	Less secure, Potential for inconsistent data, dependency on client-side behavior

Below we will show the implementation of each approach.

BE ADVISED: The following is a general guideline, and it may vary across different websites, contingent upon the specific implementation of your website.

Google Tag manager

Prerequisite: Your website must have GTM setup. If you do not have GTM setup, you can easily do the setup by following the guidelines here.

If you are not familiar with basic GTM concepts, such as Tags, Triggers and Variables, please familiarize yourself first with these concepts before proceeding with this approach. You can find more resources related to this here.

Generating user ID

In our sandbox, Notice that in the user data collection endpoints, every endpoint has a parameter called user_id, and member_id. These are vital to identify each user so that you can personalize their experience. user_id is generated by GAIP for each user of your site. The endpoint to generate and user_id is GET /v1/users/generate/id. You can generate the user_id using GTM using below code. This code can be used with every Tag, which checks if there is a user_id and creates one if there is none.

js // Function to get or generate 'gaip_user_id' using a function expression var getGaipUser = function() { return new Promise(function(resolve, reject) { if (gaipUser !== null) { resolve(gaipUser); } else { var idHeaders = new Headers(); idHeaders.append("project-key", "{{ YOUR_PROJECT_KEY_HERE }}"); idHeaders.append("api-key", "{{ YOUR_API_KEY_HERE }}"); fetch("None/v1/users/generate/id", { headers: idHeaders }) .then(function(response) { return response.json(); }) .then(function(data) { localStorage.setItem('gaip_user_id', data.detail.response); resolve(data.detail.response); }) .catch(reject); } }); };

member_id can be set by you depending on how your site identifies unique users such as user ID, phone number, email address etc.

Collecting and sending user browsing data

Set up a variable to capture the product name/title/ID, when the user goes a product details page or clicks on a product to enlarge it or open a pop-up etc.

Set up a trigger so that the tag would fire when the user goes browses an item (Go to product detail page or quick view options etc.).

Create a custom HTML Tag with the above trigger and variable and put the below code in the tag.

Collecting and sending user purchase data

Set up a variable to capture the all the purchase detail, when the user makes a purchase. This could be from the purchase confirmation page etc.

Set up a trigger so that the tag would fire when the user make the purchase.

Create a custom HTML Tag with the above trigger to send the information to endpoint POST /v1/purchase or POST /v1/purchase/client

Collecting and sending user rating data

Setup variables to capture the product name/title/ID and the rating, when the user rates an item positively or negatively. We can also consider an item is positively rated when user adds the item to wishlist.

Set up a trigger for the tag to fire when the user rates a product.

Create a custom HTML Tag with the above trigger and variables to send the information to the endpoint POST /v1/rating or POST /v1/rating/client

Collecting and sending user data

Setup variable to capture the user information.

For this, the trigger could be setup up when the user logs or update their information.

Create a custom HTML Tag with the above trigger and variables to send the information to the endpoint POST /v1/user or POST /v1/user/client

Server to server integration

For server to server integration, you will need to generate Project key and API as mentioned in the credentials section.

User information

The below request path, takes user information, such as name, age, gender, address and saves them in the gaip database.

POST /v1/user

Here is an example request body

json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "user_info": { "address": "string", "gender": "integer --> 1 for male or 2 for female or 3 for others", "age": 25, "user_type": [ { "key_name1": "value1_value2", "separator": "_" }, { "key_name2": "value3" } ] } } You can find the sample code for implementation here

Product browse

You can use the below endpoint to capture user browsing information and save them in GAIP database js POST /v1/items/browse

It takes user_id and item_id as required parameters.

Here is an example value of the request body JSON { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_id": "1000764491" }

You can find sample code here

Product purchase

You can use the below endpoint to capture user's product purchase information and save them in GAIP database

POST /v1/purchase It takes user_id, item_list which includes item_id, price, quantity for a specific item as required parameters.

Here is an example request body json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_list": [ { "item_id": "1000757666", "price": 5000, "quantity": 1 }, { "item_id": "1000764491", "price": 400, "quantity": 7 } ] }

You a find sample code for this implementation here

Product rating

You can use the below endpoint to capture user's product rating information and save them in GAIP database

POST /v1/rating

It takes user_id, item_id, and rating for the specific item as required parameters.

Here is a sample request body

json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_id": "1000764491", "rating": "1" }

You can find the sample code for this implementation here

If you want to save your data with bulk upload you can use above-mentioned endpoint.

Client to server integration

For client to server integration, you will need to generate client key as described in the Credentials sections. Once the client key is ready, you can directly send the request from your client side to GAIP, using the client key provided.

Note that while generating client key, you can add whitelisted domains, which whitelists the request origin. This is recommended to enhance security.

The rest of the implementation method is same as server to server integration.

Import user behavior data

Similar to data integration, all 4 kinds of user information (browse, purchase, rating, user) can be bulk uploaded. This could be useful if you already have this information from the past and want to import it into GAIP.

To import user behavior and user information in bulk, first you need to create mapper to match the keys with GAIP.

To create the mapper, the endpoints with the example request bodies can be found here in the gigalogy recommender page. You can also find the sample codes for mapper creation here in the API documentation page

Next we will use the below 4 endpoints to upload each category of data in bulk

Request path for product browsing history: POST /v1/items/browse/save
Request path for purchase history: POST /v1/items/purchase/save
Request path for rating history: POST /v1/items/rating/save
Request path to upload user information in bulk: POST /v1/users/save

You can find these endpoints with the example request bodies here. The sample code can be found here in the API documentation page

Previous
Environment setup
Next
Training your data
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/03_Data_integration_%26_user_behavior_collection/#integration-of-catalogue-information-and-user-behavior-data
Skip to content
Gigalogy Tutorial
Integration of Catalogue information and user behavior data
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Catalogue information integration.
Uploading data using a file
Fetch item from external API
Search Items in GAIP after import
User behavior data collection integration
Comparison of each approach.
Google Tag manager
Generating user ID
Collecting and sending user browsing data
Collecting and sending user purchase data
Collecting and sending user rating data
Collecting and sending user data
Server to server integration
User information
Product browse
Product purchase
Product rating
Client to server integration
Import user behavior data
Integration of Catalogue information and user behavior data

This tutorial will cover how to integrate your catalogue information into GAIP and how to set up user behavior tracking and integrate with GAIP.

Catalogue information integration.

Prerequisite: Mapping creation and the index creation is done.

Info

This step can also be done from our platform (GAIP). Refer here for detail.

There are two ways to import your catalogue information into GAIP.

Upload you catalogue information as a CSV or JSON file using endpoint POST /v1/item/save.
Fetch data from your API (or any external API) using endpoint POST /v1/item/save/remote.
Uploading data using a file

For uploading the data using a CSV file or JSON file, please use the POST /v1/item/save endpoint. Simply upload the file and confirm the server response is success. Confirm the task was successful using the GET/v1/tasks/{task_id} endpoint.

Info

This will throw an error and task will fail, if the keys during the Mapping creation step does not match with the keys in the file, OR if the indices were not created succesfully.

Fetch item from external API

To fetch data from external API, use the POST /v1/item/save/remote endpoint. The key and value types and an example request body for the endpoint can be found here in our sandbox.

After hitting either of the endpoint above to import your data into GAIP, you will get a task ID in the response. Use this task ID and hit the /v1/tasks/{task_id} endpoint to confirm the operation was successful. In case it fails, you can also find the details there. For API documentation, please refer <>

Search Items in GAIP after import

You can search items by passing list of item ids fromPOST /v1/items/search endpoint. This endpoint will return searched items with item details.

It is recommended to use this endpoint to confirm that the item catalogue is successfully imported into your project.

User behavior data collection integration

GAIP can collect different user behavior related information to optimize the recommendation for the user. Types of data collected are listed below with their endpoints.

Data type	Endpoint
Product browsing: When user browse products.	/v1/items/browse or /v1/items/browse/client
Product Purchase: When user purchase a product with its quantity.	/v1/items/purchase or /v1/items/purchase/client
Rating: When a user rates a product.	/v1/items/rating or /v1/items/rating/client
User: User information such as age, gender and other customized attributes depending on your website.	/v1/users or /v1/users/client

You will find these endpoints listed in our Sandbox under section "User Data Collection". Please check the required parameters, value types and example request bodies for all the endpoints there.

There are 3 ways to integrate user behavior data collection with GAIP

Google Tag Manager
Server to server integration
Client to server integration

You can also bulk upload user behavior data from the past. For that, please refer to Import user behavior data section.

Comparison of each approach.
Approach	GTM	Server to Server	Client to Server
Description	Use Google Tag Manager (GTM) to collect data (User behavior) from your website and send it to GAIP via endpoint.	The data is captured in the backend server of your application and then sent to GAIP via endpoint.	The data is directly sent from your front end (Client side) to GAIP via endpoint.
Pros	Easy to implement, Minimum coding required, Flexible configuration	More secure, Data integrity, Controlled environment	Real-time data, Less server dependency
Cons	Limited customization, need to have basic knowledge about GTM, Dependency on third-party service, Might not work for certain browsers and plugins like AdBlockers	More complex to set up, potential latency, maintenance required	Less secure, Potential for inconsistent data, dependency on client-side behavior

Below we will show the implementation of each approach.

BE ADVISED: The following is a general guideline, and it may vary across different websites, contingent upon the specific implementation of your website.

Google Tag manager

Prerequisite: Your website must have GTM setup. If you do not have GTM setup, you can easily do the setup by following the guidelines here.

If you are not familiar with basic GTM concepts, such as Tags, Triggers and Variables, please familiarize yourself first with these concepts before proceeding with this approach. You can find more resources related to this here.

Generating user ID

In our sandbox, Notice that in the user data collection endpoints, every endpoint has a parameter called user_id, and member_id. These are vital to identify each user so that you can personalize their experience. user_id is generated by GAIP for each user of your site. The endpoint to generate and user_id is GET /v1/users/generate/id. You can generate the user_id using GTM using below code. This code can be used with every Tag, which checks if there is a user_id and creates one if there is none.

js // Function to get or generate 'gaip_user_id' using a function expression var getGaipUser = function() { return new Promise(function(resolve, reject) { if (gaipUser !== null) { resolve(gaipUser); } else { var idHeaders = new Headers(); idHeaders.append("project-key", "{{ YOUR_PROJECT_KEY_HERE }}"); idHeaders.append("api-key", "{{ YOUR_API_KEY_HERE }}"); fetch("None/v1/users/generate/id", { headers: idHeaders }) .then(function(response) { return response.json(); }) .then(function(data) { localStorage.setItem('gaip_user_id', data.detail.response); resolve(data.detail.response); }) .catch(reject); } }); };

member_id can be set by you depending on how your site identifies unique users such as user ID, phone number, email address etc.

Collecting and sending user browsing data

Set up a variable to capture the product name/title/ID, when the user goes a product details page or clicks on a product to enlarge it or open a pop-up etc.

Set up a trigger so that the tag would fire when the user goes browses an item (Go to product detail page or quick view options etc.).

Create a custom HTML Tag with the above trigger and variable and put the below code in the tag.

Collecting and sending user purchase data

Set up a variable to capture the all the purchase detail, when the user makes a purchase. This could be from the purchase confirmation page etc.

Set up a trigger so that the tag would fire when the user make the purchase.

Create a custom HTML Tag with the above trigger to send the information to endpoint POST /v1/purchase or POST /v1/purchase/client

Collecting and sending user rating data

Setup variables to capture the product name/title/ID and the rating, when the user rates an item positively or negatively. We can also consider an item is positively rated when user adds the item to wishlist.

Set up a trigger for the tag to fire when the user rates a product.

Create a custom HTML Tag with the above trigger and variables to send the information to the endpoint POST /v1/rating or POST /v1/rating/client

Collecting and sending user data

Setup variable to capture the user information.

For this, the trigger could be setup up when the user logs or update their information.

Create a custom HTML Tag with the above trigger and variables to send the information to the endpoint POST /v1/user or POST /v1/user/client

Server to server integration

For server to server integration, you will need to generate Project key and API as mentioned in the credentials section.

User information

The below request path, takes user information, such as name, age, gender, address and saves them in the gaip database.

POST /v1/user

Here is an example request body

json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "user_info": { "address": "string", "gender": "integer --> 1 for male or 2 for female or 3 for others", "age": 25, "user_type": [ { "key_name1": "value1_value2", "separator": "_" }, { "key_name2": "value3" } ] } } You can find the sample code for implementation here

Product browse

You can use the below endpoint to capture user browsing information and save them in GAIP database js POST /v1/items/browse

It takes user_id and item_id as required parameters.

Here is an example value of the request body JSON { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_id": "1000764491" }

You can find sample code here

Product purchase

You can use the below endpoint to capture user's product purchase information and save them in GAIP database

POST /v1/purchase It takes user_id, item_list which includes item_id, price, quantity for a specific item as required parameters.

Here is an example request body json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_list": [ { "item_id": "1000757666", "price": 5000, "quantity": 1 }, { "item_id": "1000764491", "price": 400, "quantity": 7 } ] }

You a find sample code for this implementation here

Product rating

You can use the below endpoint to capture user's product rating information and save them in GAIP database

POST /v1/rating

It takes user_id, item_id, and rating for the specific item as required parameters.

Here is a sample request body

json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_id": "1000764491", "rating": "1" }

You can find the sample code for this implementation here

If you want to save your data with bulk upload you can use above-mentioned endpoint.

Client to server integration

For client to server integration, you will need to generate client key as described in the Credentials sections. Once the client key is ready, you can directly send the request from your client side to GAIP, using the client key provided.

Note that while generating client key, you can add whitelisted domains, which whitelists the request origin. This is recommended to enhance security.

The rest of the implementation method is same as server to server integration.

Import user behavior data

Similar to data integration, all 4 kinds of user information (browse, purchase, rating, user) can be bulk uploaded. This could be useful if you already have this information from the past and want to import it into GAIP.

To import user behavior and user information in bulk, first you need to create mapper to match the keys with GAIP.

To create the mapper, the endpoints with the example request bodies can be found here in the gigalogy recommender page. You can also find the sample codes for mapper creation here in the API documentation page

Next we will use the below 4 endpoints to upload each category of data in bulk

Request path for product browsing history: POST /v1/items/browse/save
Request path for purchase history: POST /v1/items/purchase/save
Request path for rating history: POST /v1/items/rating/save
Request path to upload user information in bulk: POST /v1/users/save

You can find these endpoints with the example request bodies here. The sample code can be found here in the API documentation page

Previous
Environment setup
Next
Training your data
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/03_Data_integration_%26_user_behavior_collection/#catalogue-information-integration
Skip to content
Gigalogy Tutorial
Integration of Catalogue information and user behavior data
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Catalogue information integration.
Uploading data using a file
Fetch item from external API
Search Items in GAIP after import
User behavior data collection integration
Comparison of each approach.
Google Tag manager
Generating user ID
Collecting and sending user browsing data
Collecting and sending user purchase data
Collecting and sending user rating data
Collecting and sending user data
Server to server integration
User information
Product browse
Product purchase
Product rating
Client to server integration
Import user behavior data
Integration of Catalogue information and user behavior data

This tutorial will cover how to integrate your catalogue information into GAIP and how to set up user behavior tracking and integrate with GAIP.

Catalogue information integration.

Prerequisite: Mapping creation and the index creation is done.

Info

This step can also be done from our platform (GAIP). Refer here for detail.

There are two ways to import your catalogue information into GAIP.

Upload you catalogue information as a CSV or JSON file using endpoint POST /v1/item/save.
Fetch data from your API (or any external API) using endpoint POST /v1/item/save/remote.
Uploading data using a file

For uploading the data using a CSV file or JSON file, please use the POST /v1/item/save endpoint. Simply upload the file and confirm the server response is success. Confirm the task was successful using the GET/v1/tasks/{task_id} endpoint.

Info

This will throw an error and task will fail, if the keys during the Mapping creation step does not match with the keys in the file, OR if the indices were not created succesfully.

Fetch item from external API

To fetch data from external API, use the POST /v1/item/save/remote endpoint. The key and value types and an example request body for the endpoint can be found here in our sandbox.

After hitting either of the endpoint above to import your data into GAIP, you will get a task ID in the response. Use this task ID and hit the /v1/tasks/{task_id} endpoint to confirm the operation was successful. In case it fails, you can also find the details there. For API documentation, please refer <>

Search Items in GAIP after import

You can search items by passing list of item ids fromPOST /v1/items/search endpoint. This endpoint will return searched items with item details.

It is recommended to use this endpoint to confirm that the item catalogue is successfully imported into your project.

User behavior data collection integration

GAIP can collect different user behavior related information to optimize the recommendation for the user. Types of data collected are listed below with their endpoints.

Data type	Endpoint
Product browsing: When user browse products.	/v1/items/browse or /v1/items/browse/client
Product Purchase: When user purchase a product with its quantity.	/v1/items/purchase or /v1/items/purchase/client
Rating: When a user rates a product.	/v1/items/rating or /v1/items/rating/client
User: User information such as age, gender and other customized attributes depending on your website.	/v1/users or /v1/users/client

You will find these endpoints listed in our Sandbox under section "User Data Collection". Please check the required parameters, value types and example request bodies for all the endpoints there.

There are 3 ways to integrate user behavior data collection with GAIP

Google Tag Manager
Server to server integration
Client to server integration

You can also bulk upload user behavior data from the past. For that, please refer to Import user behavior data section.

Comparison of each approach.
Approach	GTM	Server to Server	Client to Server
Description	Use Google Tag Manager (GTM) to collect data (User behavior) from your website and send it to GAIP via endpoint.	The data is captured in the backend server of your application and then sent to GAIP via endpoint.	The data is directly sent from your front end (Client side) to GAIP via endpoint.
Pros	Easy to implement, Minimum coding required, Flexible configuration	More secure, Data integrity, Controlled environment	Real-time data, Less server dependency
Cons	Limited customization, need to have basic knowledge about GTM, Dependency on third-party service, Might not work for certain browsers and plugins like AdBlockers	More complex to set up, potential latency, maintenance required	Less secure, Potential for inconsistent data, dependency on client-side behavior

Below we will show the implementation of each approach.

BE ADVISED: The following is a general guideline, and it may vary across different websites, contingent upon the specific implementation of your website.

Google Tag manager

Prerequisite: Your website must have GTM setup. If you do not have GTM setup, you can easily do the setup by following the guidelines here.

If you are not familiar with basic GTM concepts, such as Tags, Triggers and Variables, please familiarize yourself first with these concepts before proceeding with this approach. You can find more resources related to this here.

Generating user ID

In our sandbox, Notice that in the user data collection endpoints, every endpoint has a parameter called user_id, and member_id. These are vital to identify each user so that you can personalize their experience. user_id is generated by GAIP for each user of your site. The endpoint to generate and user_id is GET /v1/users/generate/id. You can generate the user_id using GTM using below code. This code can be used with every Tag, which checks if there is a user_id and creates one if there is none.

js // Function to get or generate 'gaip_user_id' using a function expression var getGaipUser = function() { return new Promise(function(resolve, reject) { if (gaipUser !== null) { resolve(gaipUser); } else { var idHeaders = new Headers(); idHeaders.append("project-key", "{{ YOUR_PROJECT_KEY_HERE }}"); idHeaders.append("api-key", "{{ YOUR_API_KEY_HERE }}"); fetch("None/v1/users/generate/id", { headers: idHeaders }) .then(function(response) { return response.json(); }) .then(function(data) { localStorage.setItem('gaip_user_id', data.detail.response); resolve(data.detail.response); }) .catch(reject); } }); };

member_id can be set by you depending on how your site identifies unique users such as user ID, phone number, email address etc.

Collecting and sending user browsing data

Set up a variable to capture the product name/title/ID, when the user goes a product details page or clicks on a product to enlarge it or open a pop-up etc.

Set up a trigger so that the tag would fire when the user goes browses an item (Go to product detail page or quick view options etc.).

Create a custom HTML Tag with the above trigger and variable and put the below code in the tag.

Collecting and sending user purchase data

Set up a variable to capture the all the purchase detail, when the user makes a purchase. This could be from the purchase confirmation page etc.

Set up a trigger so that the tag would fire when the user make the purchase.

Create a custom HTML Tag with the above trigger to send the information to endpoint POST /v1/purchase or POST /v1/purchase/client

Collecting and sending user rating data

Setup variables to capture the product name/title/ID and the rating, when the user rates an item positively or negatively. We can also consider an item is positively rated when user adds the item to wishlist.

Set up a trigger for the tag to fire when the user rates a product.

Create a custom HTML Tag with the above trigger and variables to send the information to the endpoint POST /v1/rating or POST /v1/rating/client

Collecting and sending user data

Setup variable to capture the user information.

For this, the trigger could be setup up when the user logs or update their information.

Create a custom HTML Tag with the above trigger and variables to send the information to the endpoint POST /v1/user or POST /v1/user/client

Server to server integration

For server to server integration, you will need to generate Project key and API as mentioned in the credentials section.

User information

The below request path, takes user information, such as name, age, gender, address and saves them in the gaip database.

POST /v1/user

Here is an example request body

json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "user_info": { "address": "string", "gender": "integer --> 1 for male or 2 for female or 3 for others", "age": 25, "user_type": [ { "key_name1": "value1_value2", "separator": "_" }, { "key_name2": "value3" } ] } } You can find the sample code for implementation here

Product browse

You can use the below endpoint to capture user browsing information and save them in GAIP database js POST /v1/items/browse

It takes user_id and item_id as required parameters.

Here is an example value of the request body JSON { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_id": "1000764491" }

You can find sample code here

Product purchase

You can use the below endpoint to capture user's product purchase information and save them in GAIP database

POST /v1/purchase It takes user_id, item_list which includes item_id, price, quantity for a specific item as required parameters.

Here is an example request body json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_list": [ { "item_id": "1000757666", "price": 5000, "quantity": 1 }, { "item_id": "1000764491", "price": 400, "quantity": 7 } ] }

You a find sample code for this implementation here

Product rating

You can use the below endpoint to capture user's product rating information and save them in GAIP database

POST /v1/rating

It takes user_id, item_id, and rating for the specific item as required parameters.

Here is a sample request body

json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_id": "1000764491", "rating": "1" }

You can find the sample code for this implementation here

If you want to save your data with bulk upload you can use above-mentioned endpoint.

Client to server integration

For client to server integration, you will need to generate client key as described in the Credentials sections. Once the client key is ready, you can directly send the request from your client side to GAIP, using the client key provided.

Note that while generating client key, you can add whitelisted domains, which whitelists the request origin. This is recommended to enhance security.

The rest of the implementation method is same as server to server integration.

Import user behavior data

Similar to data integration, all 4 kinds of user information (browse, purchase, rating, user) can be bulk uploaded. This could be useful if you already have this information from the past and want to import it into GAIP.

To import user behavior and user information in bulk, first you need to create mapper to match the keys with GAIP.

To create the mapper, the endpoints with the example request bodies can be found here in the gigalogy recommender page. You can also find the sample codes for mapper creation here in the API documentation page

Next we will use the below 4 endpoints to upload each category of data in bulk

Request path for product browsing history: POST /v1/items/browse/save
Request path for purchase history: POST /v1/items/purchase/save
Request path for rating history: POST /v1/items/rating/save
Request path to upload user information in bulk: POST /v1/users/save

You can find these endpoints with the example request bodies here. The sample code can be found here in the API documentation page

Previous
Environment setup
Next
Training your data
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/03_Data_integration_%26_user_behavior_collection/#uploading-data-using-a-file
Skip to content
Gigalogy Tutorial
Integration of Catalogue information and user behavior data
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Catalogue information integration.
Uploading data using a file
Fetch item from external API
Search Items in GAIP after import
User behavior data collection integration
Comparison of each approach.
Google Tag manager
Generating user ID
Collecting and sending user browsing data
Collecting and sending user purchase data
Collecting and sending user rating data
Collecting and sending user data
Server to server integration
User information
Product browse
Product purchase
Product rating
Client to server integration
Import user behavior data
Integration of Catalogue information and user behavior data

This tutorial will cover how to integrate your catalogue information into GAIP and how to set up user behavior tracking and integrate with GAIP.

Catalogue information integration.

Prerequisite: Mapping creation and the index creation is done.

Info

This step can also be done from our platform (GAIP). Refer here for detail.

There are two ways to import your catalogue information into GAIP.

Upload you catalogue information as a CSV or JSON file using endpoint POST /v1/item/save.
Fetch data from your API (or any external API) using endpoint POST /v1/item/save/remote.
Uploading data using a file

For uploading the data using a CSV file or JSON file, please use the POST /v1/item/save endpoint. Simply upload the file and confirm the server response is success. Confirm the task was successful using the GET/v1/tasks/{task_id} endpoint.

Info

This will throw an error and task will fail, if the keys during the Mapping creation step does not match with the keys in the file, OR if the indices were not created succesfully.

Fetch item from external API

To fetch data from external API, use the POST /v1/item/save/remote endpoint. The key and value types and an example request body for the endpoint can be found here in our sandbox.

After hitting either of the endpoint above to import your data into GAIP, you will get a task ID in the response. Use this task ID and hit the /v1/tasks/{task_id} endpoint to confirm the operation was successful. In case it fails, you can also find the details there. For API documentation, please refer <>

Search Items in GAIP after import

You can search items by passing list of item ids fromPOST /v1/items/search endpoint. This endpoint will return searched items with item details.

It is recommended to use this endpoint to confirm that the item catalogue is successfully imported into your project.

User behavior data collection integration

GAIP can collect different user behavior related information to optimize the recommendation for the user. Types of data collected are listed below with their endpoints.

Data type	Endpoint
Product browsing: When user browse products.	/v1/items/browse or /v1/items/browse/client
Product Purchase: When user purchase a product with its quantity.	/v1/items/purchase or /v1/items/purchase/client
Rating: When a user rates a product.	/v1/items/rating or /v1/items/rating/client
User: User information such as age, gender and other customized attributes depending on your website.	/v1/users or /v1/users/client

You will find these endpoints listed in our Sandbox under section "User Data Collection". Please check the required parameters, value types and example request bodies for all the endpoints there.

There are 3 ways to integrate user behavior data collection with GAIP

Google Tag Manager
Server to server integration
Client to server integration

You can also bulk upload user behavior data from the past. For that, please refer to Import user behavior data section.

Comparison of each approach.
Approach	GTM	Server to Server	Client to Server
Description	Use Google Tag Manager (GTM) to collect data (User behavior) from your website and send it to GAIP via endpoint.	The data is captured in the backend server of your application and then sent to GAIP via endpoint.	The data is directly sent from your front end (Client side) to GAIP via endpoint.
Pros	Easy to implement, Minimum coding required, Flexible configuration	More secure, Data integrity, Controlled environment	Real-time data, Less server dependency
Cons	Limited customization, need to have basic knowledge about GTM, Dependency on third-party service, Might not work for certain browsers and plugins like AdBlockers	More complex to set up, potential latency, maintenance required	Less secure, Potential for inconsistent data, dependency on client-side behavior

Below we will show the implementation of each approach.

BE ADVISED: The following is a general guideline, and it may vary across different websites, contingent upon the specific implementation of your website.

Google Tag manager

Prerequisite: Your website must have GTM setup. If you do not have GTM setup, you can easily do the setup by following the guidelines here.

If you are not familiar with basic GTM concepts, such as Tags, Triggers and Variables, please familiarize yourself first with these concepts before proceeding with this approach. You can find more resources related to this here.

Generating user ID

In our sandbox, Notice that in the user data collection endpoints, every endpoint has a parameter called user_id, and member_id. These are vital to identify each user so that you can personalize their experience. user_id is generated by GAIP for each user of your site. The endpoint to generate and user_id is GET /v1/users/generate/id. You can generate the user_id using GTM using below code. This code can be used with every Tag, which checks if there is a user_id and creates one if there is none.

js // Function to get or generate 'gaip_user_id' using a function expression var getGaipUser = function() { return new Promise(function(resolve, reject) { if (gaipUser !== null) { resolve(gaipUser); } else { var idHeaders = new Headers(); idHeaders.append("project-key", "{{ YOUR_PROJECT_KEY_HERE }}"); idHeaders.append("api-key", "{{ YOUR_API_KEY_HERE }}"); fetch("None/v1/users/generate/id", { headers: idHeaders }) .then(function(response) { return response.json(); }) .then(function(data) { localStorage.setItem('gaip_user_id', data.detail.response); resolve(data.detail.response); }) .catch(reject); } }); };

member_id can be set by you depending on how your site identifies unique users such as user ID, phone number, email address etc.

Collecting and sending user browsing data

Set up a variable to capture the product name/title/ID, when the user goes a product details page or clicks on a product to enlarge it or open a pop-up etc.

Set up a trigger so that the tag would fire when the user goes browses an item (Go to product detail page or quick view options etc.).

Create a custom HTML Tag with the above trigger and variable and put the below code in the tag.

Collecting and sending user purchase data

Set up a variable to capture the all the purchase detail, when the user makes a purchase. This could be from the purchase confirmation page etc.

Set up a trigger so that the tag would fire when the user make the purchase.

Create a custom HTML Tag with the above trigger to send the information to endpoint POST /v1/purchase or POST /v1/purchase/client

Collecting and sending user rating data

Setup variables to capture the product name/title/ID and the rating, when the user rates an item positively or negatively. We can also consider an item is positively rated when user adds the item to wishlist.

Set up a trigger for the tag to fire when the user rates a product.

Create a custom HTML Tag with the above trigger and variables to send the information to the endpoint POST /v1/rating or POST /v1/rating/client

Collecting and sending user data

Setup variable to capture the user information.

For this, the trigger could be setup up when the user logs or update their information.

Create a custom HTML Tag with the above trigger and variables to send the information to the endpoint POST /v1/user or POST /v1/user/client

Server to server integration

For server to server integration, you will need to generate Project key and API as mentioned in the credentials section.

User information

The below request path, takes user information, such as name, age, gender, address and saves them in the gaip database.

POST /v1/user

Here is an example request body

json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "user_info": { "address": "string", "gender": "integer --> 1 for male or 2 for female or 3 for others", "age": 25, "user_type": [ { "key_name1": "value1_value2", "separator": "_" }, { "key_name2": "value3" } ] } } You can find the sample code for implementation here

Product browse

You can use the below endpoint to capture user browsing information and save them in GAIP database js POST /v1/items/browse

It takes user_id and item_id as required parameters.

Here is an example value of the request body JSON { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_id": "1000764491" }

You can find sample code here

Product purchase

You can use the below endpoint to capture user's product purchase information and save them in GAIP database

POST /v1/purchase It takes user_id, item_list which includes item_id, price, quantity for a specific item as required parameters.

Here is an example request body json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_list": [ { "item_id": "1000757666", "price": 5000, "quantity": 1 }, { "item_id": "1000764491", "price": 400, "quantity": 7 } ] }

You a find sample code for this implementation here

Product rating

You can use the below endpoint to capture user's product rating information and save them in GAIP database

POST /v1/rating

It takes user_id, item_id, and rating for the specific item as required parameters.

Here is a sample request body

json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_id": "1000764491", "rating": "1" }

You can find the sample code for this implementation here

If you want to save your data with bulk upload you can use above-mentioned endpoint.

Client to server integration

For client to server integration, you will need to generate client key as described in the Credentials sections. Once the client key is ready, you can directly send the request from your client side to GAIP, using the client key provided.

Note that while generating client key, you can add whitelisted domains, which whitelists the request origin. This is recommended to enhance security.

The rest of the implementation method is same as server to server integration.

Import user behavior data

Similar to data integration, all 4 kinds of user information (browse, purchase, rating, user) can be bulk uploaded. This could be useful if you already have this information from the past and want to import it into GAIP.

To import user behavior and user information in bulk, first you need to create mapper to match the keys with GAIP.

To create the mapper, the endpoints with the example request bodies can be found here in the gigalogy recommender page. You can also find the sample codes for mapper creation here in the API documentation page

Next we will use the below 4 endpoints to upload each category of data in bulk

Request path for product browsing history: POST /v1/items/browse/save
Request path for purchase history: POST /v1/items/purchase/save
Request path for rating history: POST /v1/items/rating/save
Request path to upload user information in bulk: POST /v1/users/save

You can find these endpoints with the example request bodies here. The sample code can be found here in the API documentation page

Previous
Environment setup
Next
Training your data
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/03_Data_integration_%26_user_behavior_collection/#fetch-item-from-external-api
Skip to content
Gigalogy Tutorial
Integration of Catalogue information and user behavior data
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Catalogue information integration.
Uploading data using a file
Fetch item from external API
Search Items in GAIP after import
User behavior data collection integration
Comparison of each approach.
Google Tag manager
Generating user ID
Collecting and sending user browsing data
Collecting and sending user purchase data
Collecting and sending user rating data
Collecting and sending user data
Server to server integration
User information
Product browse
Product purchase
Product rating
Client to server integration
Import user behavior data
Integration of Catalogue information and user behavior data

This tutorial will cover how to integrate your catalogue information into GAIP and how to set up user behavior tracking and integrate with GAIP.

Catalogue information integration.

Prerequisite: Mapping creation and the index creation is done.

Info

This step can also be done from our platform (GAIP). Refer here for detail.

There are two ways to import your catalogue information into GAIP.

Upload you catalogue information as a CSV or JSON file using endpoint POST /v1/item/save.
Fetch data from your API (or any external API) using endpoint POST /v1/item/save/remote.
Uploading data using a file

For uploading the data using a CSV file or JSON file, please use the POST /v1/item/save endpoint. Simply upload the file and confirm the server response is success. Confirm the task was successful using the GET/v1/tasks/{task_id} endpoint.

Info

This will throw an error and task will fail, if the keys during the Mapping creation step does not match with the keys in the file, OR if the indices were not created succesfully.

Fetch item from external API

To fetch data from external API, use the POST /v1/item/save/remote endpoint. The key and value types and an example request body for the endpoint can be found here in our sandbox.

After hitting either of the endpoint above to import your data into GAIP, you will get a task ID in the response. Use this task ID and hit the /v1/tasks/{task_id} endpoint to confirm the operation was successful. In case it fails, you can also find the details there. For API documentation, please refer <>

Search Items in GAIP after import

You can search items by passing list of item ids fromPOST /v1/items/search endpoint. This endpoint will return searched items with item details.

It is recommended to use this endpoint to confirm that the item catalogue is successfully imported into your project.

User behavior data collection integration

GAIP can collect different user behavior related information to optimize the recommendation for the user. Types of data collected are listed below with their endpoints.

Data type	Endpoint
Product browsing: When user browse products.	/v1/items/browse or /v1/items/browse/client
Product Purchase: When user purchase a product with its quantity.	/v1/items/purchase or /v1/items/purchase/client
Rating: When a user rates a product.	/v1/items/rating or /v1/items/rating/client
User: User information such as age, gender and other customized attributes depending on your website.	/v1/users or /v1/users/client

You will find these endpoints listed in our Sandbox under section "User Data Collection". Please check the required parameters, value types and example request bodies for all the endpoints there.

There are 3 ways to integrate user behavior data collection with GAIP

Google Tag Manager
Server to server integration
Client to server integration

You can also bulk upload user behavior data from the past. For that, please refer to Import user behavior data section.

Comparison of each approach.
Approach	GTM	Server to Server	Client to Server
Description	Use Google Tag Manager (GTM) to collect data (User behavior) from your website and send it to GAIP via endpoint.	The data is captured in the backend server of your application and then sent to GAIP via endpoint.	The data is directly sent from your front end (Client side) to GAIP via endpoint.
Pros	Easy to implement, Minimum coding required, Flexible configuration	More secure, Data integrity, Controlled environment	Real-time data, Less server dependency
Cons	Limited customization, need to have basic knowledge about GTM, Dependency on third-party service, Might not work for certain browsers and plugins like AdBlockers	More complex to set up, potential latency, maintenance required	Less secure, Potential for inconsistent data, dependency on client-side behavior

Below we will show the implementation of each approach.

BE ADVISED: The following is a general guideline, and it may vary across different websites, contingent upon the specific implementation of your website.

Google Tag manager

Prerequisite: Your website must have GTM setup. If you do not have GTM setup, you can easily do the setup by following the guidelines here.

If you are not familiar with basic GTM concepts, such as Tags, Triggers and Variables, please familiarize yourself first with these concepts before proceeding with this approach. You can find more resources related to this here.

Generating user ID

In our sandbox, Notice that in the user data collection endpoints, every endpoint has a parameter called user_id, and member_id. These are vital to identify each user so that you can personalize their experience. user_id is generated by GAIP for each user of your site. The endpoint to generate and user_id is GET /v1/users/generate/id. You can generate the user_id using GTM using below code. This code can be used with every Tag, which checks if there is a user_id and creates one if there is none.

js // Function to get or generate 'gaip_user_id' using a function expression var getGaipUser = function() { return new Promise(function(resolve, reject) { if (gaipUser !== null) { resolve(gaipUser); } else { var idHeaders = new Headers(); idHeaders.append("project-key", "{{ YOUR_PROJECT_KEY_HERE }}"); idHeaders.append("api-key", "{{ YOUR_API_KEY_HERE }}"); fetch("None/v1/users/generate/id", { headers: idHeaders }) .then(function(response) { return response.json(); }) .then(function(data) { localStorage.setItem('gaip_user_id', data.detail.response); resolve(data.detail.response); }) .catch(reject); } }); };

member_id can be set by you depending on how your site identifies unique users such as user ID, phone number, email address etc.

Collecting and sending user browsing data

Set up a variable to capture the product name/title/ID, when the user goes a product details page or clicks on a product to enlarge it or open a pop-up etc.

Set up a trigger so that the tag would fire when the user goes browses an item (Go to product detail page or quick view options etc.).

Create a custom HTML Tag with the above trigger and variable and put the below code in the tag.

Collecting and sending user purchase data

Set up a variable to capture the all the purchase detail, when the user makes a purchase. This could be from the purchase confirmation page etc.

Set up a trigger so that the tag would fire when the user make the purchase.

Create a custom HTML Tag with the above trigger to send the information to endpoint POST /v1/purchase or POST /v1/purchase/client

Collecting and sending user rating data

Setup variables to capture the product name/title/ID and the rating, when the user rates an item positively or negatively. We can also consider an item is positively rated when user adds the item to wishlist.

Set up a trigger for the tag to fire when the user rates a product.

Create a custom HTML Tag with the above trigger and variables to send the information to the endpoint POST /v1/rating or POST /v1/rating/client

Collecting and sending user data

Setup variable to capture the user information.

For this, the trigger could be setup up when the user logs or update their information.

Create a custom HTML Tag with the above trigger and variables to send the information to the endpoint POST /v1/user or POST /v1/user/client

Server to server integration

For server to server integration, you will need to generate Project key and API as mentioned in the credentials section.

User information

The below request path, takes user information, such as name, age, gender, address and saves them in the gaip database.

POST /v1/user

Here is an example request body

json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "user_info": { "address": "string", "gender": "integer --> 1 for male or 2 for female or 3 for others", "age": 25, "user_type": [ { "key_name1": "value1_value2", "separator": "_" }, { "key_name2": "value3" } ] } } You can find the sample code for implementation here

Product browse

You can use the below endpoint to capture user browsing information and save them in GAIP database js POST /v1/items/browse

It takes user_id and item_id as required parameters.

Here is an example value of the request body JSON { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_id": "1000764491" }

You can find sample code here

Product purchase

You can use the below endpoint to capture user's product purchase information and save them in GAIP database

POST /v1/purchase It takes user_id, item_list which includes item_id, price, quantity for a specific item as required parameters.

Here is an example request body json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_list": [ { "item_id": "1000757666", "price": 5000, "quantity": 1 }, { "item_id": "1000764491", "price": 400, "quantity": 7 } ] }

You a find sample code for this implementation here

Product rating

You can use the below endpoint to capture user's product rating information and save them in GAIP database

POST /v1/rating

It takes user_id, item_id, and rating for the specific item as required parameters.

Here is a sample request body

json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_id": "1000764491", "rating": "1" }

You can find the sample code for this implementation here

If you want to save your data with bulk upload you can use above-mentioned endpoint.

Client to server integration

For client to server integration, you will need to generate client key as described in the Credentials sections. Once the client key is ready, you can directly send the request from your client side to GAIP, using the client key provided.

Note that while generating client key, you can add whitelisted domains, which whitelists the request origin. This is recommended to enhance security.

The rest of the implementation method is same as server to server integration.

Import user behavior data

Similar to data integration, all 4 kinds of user information (browse, purchase, rating, user) can be bulk uploaded. This could be useful if you already have this information from the past and want to import it into GAIP.

To import user behavior and user information in bulk, first you need to create mapper to match the keys with GAIP.

To create the mapper, the endpoints with the example request bodies can be found here in the gigalogy recommender page. You can also find the sample codes for mapper creation here in the API documentation page

Next we will use the below 4 endpoints to upload each category of data in bulk

Request path for product browsing history: POST /v1/items/browse/save
Request path for purchase history: POST /v1/items/purchase/save
Request path for rating history: POST /v1/items/rating/save
Request path to upload user information in bulk: POST /v1/users/save

You can find these endpoints with the example request bodies here. The sample code can be found here in the API documentation page

Previous
Environment setup
Next
Training your data
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/03_Data_integration_%26_user_behavior_collection/#search-items-in-gaip-after-import
Skip to content
Gigalogy Tutorial
Integration of Catalogue information and user behavior data
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Catalogue information integration.
Uploading data using a file
Fetch item from external API
Search Items in GAIP after import
User behavior data collection integration
Comparison of each approach.
Google Tag manager
Generating user ID
Collecting and sending user browsing data
Collecting and sending user purchase data
Collecting and sending user rating data
Collecting and sending user data
Server to server integration
User information
Product browse
Product purchase
Product rating
Client to server integration
Import user behavior data
Integration of Catalogue information and user behavior data

This tutorial will cover how to integrate your catalogue information into GAIP and how to set up user behavior tracking and integrate with GAIP.

Catalogue information integration.

Prerequisite: Mapping creation and the index creation is done.

Info

This step can also be done from our platform (GAIP). Refer here for detail.

There are two ways to import your catalogue information into GAIP.

Upload you catalogue information as a CSV or JSON file using endpoint POST /v1/item/save.
Fetch data from your API (or any external API) using endpoint POST /v1/item/save/remote.
Uploading data using a file

For uploading the data using a CSV file or JSON file, please use the POST /v1/item/save endpoint. Simply upload the file and confirm the server response is success. Confirm the task was successful using the GET/v1/tasks/{task_id} endpoint.

Info

This will throw an error and task will fail, if the keys during the Mapping creation step does not match with the keys in the file, OR if the indices were not created succesfully.

Fetch item from external API

To fetch data from external API, use the POST /v1/item/save/remote endpoint. The key and value types and an example request body for the endpoint can be found here in our sandbox.

After hitting either of the endpoint above to import your data into GAIP, you will get a task ID in the response. Use this task ID and hit the /v1/tasks/{task_id} endpoint to confirm the operation was successful. In case it fails, you can also find the details there. For API documentation, please refer <>

Search Items in GAIP after import

You can search items by passing list of item ids fromPOST /v1/items/search endpoint. This endpoint will return searched items with item details.

It is recommended to use this endpoint to confirm that the item catalogue is successfully imported into your project.

User behavior data collection integration

GAIP can collect different user behavior related information to optimize the recommendation for the user. Types of data collected are listed below with their endpoints.

Data type	Endpoint
Product browsing: When user browse products.	/v1/items/browse or /v1/items/browse/client
Product Purchase: When user purchase a product with its quantity.	/v1/items/purchase or /v1/items/purchase/client
Rating: When a user rates a product.	/v1/items/rating or /v1/items/rating/client
User: User information such as age, gender and other customized attributes depending on your website.	/v1/users or /v1/users/client

You will find these endpoints listed in our Sandbox under section "User Data Collection". Please check the required parameters, value types and example request bodies for all the endpoints there.

There are 3 ways to integrate user behavior data collection with GAIP

Google Tag Manager
Server to server integration
Client to server integration

You can also bulk upload user behavior data from the past. For that, please refer to Import user behavior data section.

Comparison of each approach.
Approach	GTM	Server to Server	Client to Server
Description	Use Google Tag Manager (GTM) to collect data (User behavior) from your website and send it to GAIP via endpoint.	The data is captured in the backend server of your application and then sent to GAIP via endpoint.	The data is directly sent from your front end (Client side) to GAIP via endpoint.
Pros	Easy to implement, Minimum coding required, Flexible configuration	More secure, Data integrity, Controlled environment	Real-time data, Less server dependency
Cons	Limited customization, need to have basic knowledge about GTM, Dependency on third-party service, Might not work for certain browsers and plugins like AdBlockers	More complex to set up, potential latency, maintenance required	Less secure, Potential for inconsistent data, dependency on client-side behavior

Below we will show the implementation of each approach.

BE ADVISED: The following is a general guideline, and it may vary across different websites, contingent upon the specific implementation of your website.

Google Tag manager

Prerequisite: Your website must have GTM setup. If you do not have GTM setup, you can easily do the setup by following the guidelines here.

If you are not familiar with basic GTM concepts, such as Tags, Triggers and Variables, please familiarize yourself first with these concepts before proceeding with this approach. You can find more resources related to this here.

Generating user ID

In our sandbox, Notice that in the user data collection endpoints, every endpoint has a parameter called user_id, and member_id. These are vital to identify each user so that you can personalize their experience. user_id is generated by GAIP for each user of your site. The endpoint to generate and user_id is GET /v1/users/generate/id. You can generate the user_id using GTM using below code. This code can be used with every Tag, which checks if there is a user_id and creates one if there is none.

js // Function to get or generate 'gaip_user_id' using a function expression var getGaipUser = function() { return new Promise(function(resolve, reject) { if (gaipUser !== null) { resolve(gaipUser); } else { var idHeaders = new Headers(); idHeaders.append("project-key", "{{ YOUR_PROJECT_KEY_HERE }}"); idHeaders.append("api-key", "{{ YOUR_API_KEY_HERE }}"); fetch("None/v1/users/generate/id", { headers: idHeaders }) .then(function(response) { return response.json(); }) .then(function(data) { localStorage.setItem('gaip_user_id', data.detail.response); resolve(data.detail.response); }) .catch(reject); } }); };

member_id can be set by you depending on how your site identifies unique users such as user ID, phone number, email address etc.

Collecting and sending user browsing data

Set up a variable to capture the product name/title/ID, when the user goes a product details page or clicks on a product to enlarge it or open a pop-up etc.

Set up a trigger so that the tag would fire when the user goes browses an item (Go to product detail page or quick view options etc.).

Create a custom HTML Tag with the above trigger and variable and put the below code in the tag.

Collecting and sending user purchase data

Set up a variable to capture the all the purchase detail, when the user makes a purchase. This could be from the purchase confirmation page etc.

Set up a trigger so that the tag would fire when the user make the purchase.

Create a custom HTML Tag with the above trigger to send the information to endpoint POST /v1/purchase or POST /v1/purchase/client

Collecting and sending user rating data

Setup variables to capture the product name/title/ID and the rating, when the user rates an item positively or negatively. We can also consider an item is positively rated when user adds the item to wishlist.

Set up a trigger for the tag to fire when the user rates a product.

Create a custom HTML Tag with the above trigger and variables to send the information to the endpoint POST /v1/rating or POST /v1/rating/client

Collecting and sending user data

Setup variable to capture the user information.

For this, the trigger could be setup up when the user logs or update their information.

Create a custom HTML Tag with the above trigger and variables to send the information to the endpoint POST /v1/user or POST /v1/user/client

Server to server integration

For server to server integration, you will need to generate Project key and API as mentioned in the credentials section.

User information

The below request path, takes user information, such as name, age, gender, address and saves them in the gaip database.

POST /v1/user

Here is an example request body

json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "user_info": { "address": "string", "gender": "integer --> 1 for male or 2 for female or 3 for others", "age": 25, "user_type": [ { "key_name1": "value1_value2", "separator": "_" }, { "key_name2": "value3" } ] } } You can find the sample code for implementation here

Product browse

You can use the below endpoint to capture user browsing information and save them in GAIP database js POST /v1/items/browse

It takes user_id and item_id as required parameters.

Here is an example value of the request body JSON { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_id": "1000764491" }

You can find sample code here

Product purchase

You can use the below endpoint to capture user's product purchase information and save them in GAIP database

POST /v1/purchase It takes user_id, item_list which includes item_id, price, quantity for a specific item as required parameters.

Here is an example request body json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_list": [ { "item_id": "1000757666", "price": 5000, "quantity": 1 }, { "item_id": "1000764491", "price": 400, "quantity": 7 } ] }

You a find sample code for this implementation here

Product rating

You can use the below endpoint to capture user's product rating information and save them in GAIP database

POST /v1/rating

It takes user_id, item_id, and rating for the specific item as required parameters.

Here is a sample request body

json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_id": "1000764491", "rating": "1" }

You can find the sample code for this implementation here

If you want to save your data with bulk upload you can use above-mentioned endpoint.

Client to server integration

For client to server integration, you will need to generate client key as described in the Credentials sections. Once the client key is ready, you can directly send the request from your client side to GAIP, using the client key provided.

Note that while generating client key, you can add whitelisted domains, which whitelists the request origin. This is recommended to enhance security.

The rest of the implementation method is same as server to server integration.

Import user behavior data

Similar to data integration, all 4 kinds of user information (browse, purchase, rating, user) can be bulk uploaded. This could be useful if you already have this information from the past and want to import it into GAIP.

To import user behavior and user information in bulk, first you need to create mapper to match the keys with GAIP.

To create the mapper, the endpoints with the example request bodies can be found here in the gigalogy recommender page. You can also find the sample codes for mapper creation here in the API documentation page

Next we will use the below 4 endpoints to upload each category of data in bulk

Request path for product browsing history: POST /v1/items/browse/save
Request path for purchase history: POST /v1/items/purchase/save
Request path for rating history: POST /v1/items/rating/save
Request path to upload user information in bulk: POST /v1/users/save

You can find these endpoints with the example request bodies here. The sample code can be found here in the API documentation page

Previous
Environment setup
Next
Training your data
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/03_Data_integration_%26_user_behavior_collection/#user-behavior-data-collection-integration
Skip to content
Gigalogy Tutorial
Integration of Catalogue information and user behavior data
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Catalogue information integration.
Uploading data using a file
Fetch item from external API
Search Items in GAIP after import
User behavior data collection integration
Comparison of each approach.
Google Tag manager
Generating user ID
Collecting and sending user browsing data
Collecting and sending user purchase data
Collecting and sending user rating data
Collecting and sending user data
Server to server integration
User information
Product browse
Product purchase
Product rating
Client to server integration
Import user behavior data
Integration of Catalogue information and user behavior data

This tutorial will cover how to integrate your catalogue information into GAIP and how to set up user behavior tracking and integrate with GAIP.

Catalogue information integration.

Prerequisite: Mapping creation and the index creation is done.

Info

This step can also be done from our platform (GAIP). Refer here for detail.

There are two ways to import your catalogue information into GAIP.

Upload you catalogue information as a CSV or JSON file using endpoint POST /v1/item/save.
Fetch data from your API (or any external API) using endpoint POST /v1/item/save/remote.
Uploading data using a file

For uploading the data using a CSV file or JSON file, please use the POST /v1/item/save endpoint. Simply upload the file and confirm the server response is success. Confirm the task was successful using the GET/v1/tasks/{task_id} endpoint.

Info

This will throw an error and task will fail, if the keys during the Mapping creation step does not match with the keys in the file, OR if the indices were not created succesfully.

Fetch item from external API

To fetch data from external API, use the POST /v1/item/save/remote endpoint. The key and value types and an example request body for the endpoint can be found here in our sandbox.

After hitting either of the endpoint above to import your data into GAIP, you will get a task ID in the response. Use this task ID and hit the /v1/tasks/{task_id} endpoint to confirm the operation was successful. In case it fails, you can also find the details there. For API documentation, please refer <>

Search Items in GAIP after import

You can search items by passing list of item ids fromPOST /v1/items/search endpoint. This endpoint will return searched items with item details.

It is recommended to use this endpoint to confirm that the item catalogue is successfully imported into your project.

User behavior data collection integration

GAIP can collect different user behavior related information to optimize the recommendation for the user. Types of data collected are listed below with their endpoints.

Data type	Endpoint
Product browsing: When user browse products.	/v1/items/browse or /v1/items/browse/client
Product Purchase: When user purchase a product with its quantity.	/v1/items/purchase or /v1/items/purchase/client
Rating: When a user rates a product.	/v1/items/rating or /v1/items/rating/client
User: User information such as age, gender and other customized attributes depending on your website.	/v1/users or /v1/users/client

You will find these endpoints listed in our Sandbox under section "User Data Collection". Please check the required parameters, value types and example request bodies for all the endpoints there.

There are 3 ways to integrate user behavior data collection with GAIP

Google Tag Manager
Server to server integration
Client to server integration

You can also bulk upload user behavior data from the past. For that, please refer to Import user behavior data section.

Comparison of each approach.
Approach	GTM	Server to Server	Client to Server
Description	Use Google Tag Manager (GTM) to collect data (User behavior) from your website and send it to GAIP via endpoint.	The data is captured in the backend server of your application and then sent to GAIP via endpoint.	The data is directly sent from your front end (Client side) to GAIP via endpoint.
Pros	Easy to implement, Minimum coding required, Flexible configuration	More secure, Data integrity, Controlled environment	Real-time data, Less server dependency
Cons	Limited customization, need to have basic knowledge about GTM, Dependency on third-party service, Might not work for certain browsers and plugins like AdBlockers	More complex to set up, potential latency, maintenance required	Less secure, Potential for inconsistent data, dependency on client-side behavior

Below we will show the implementation of each approach.

BE ADVISED: The following is a general guideline, and it may vary across different websites, contingent upon the specific implementation of your website.

Google Tag manager

Prerequisite: Your website must have GTM setup. If you do not have GTM setup, you can easily do the setup by following the guidelines here.

If you are not familiar with basic GTM concepts, such as Tags, Triggers and Variables, please familiarize yourself first with these concepts before proceeding with this approach. You can find more resources related to this here.

Generating user ID

In our sandbox, Notice that in the user data collection endpoints, every endpoint has a parameter called user_id, and member_id. These are vital to identify each user so that you can personalize their experience. user_id is generated by GAIP for each user of your site. The endpoint to generate and user_id is GET /v1/users/generate/id. You can generate the user_id using GTM using below code. This code can be used with every Tag, which checks if there is a user_id and creates one if there is none.

js // Function to get or generate 'gaip_user_id' using a function expression var getGaipUser = function() { return new Promise(function(resolve, reject) { if (gaipUser !== null) { resolve(gaipUser); } else { var idHeaders = new Headers(); idHeaders.append("project-key", "{{ YOUR_PROJECT_KEY_HERE }}"); idHeaders.append("api-key", "{{ YOUR_API_KEY_HERE }}"); fetch("None/v1/users/generate/id", { headers: idHeaders }) .then(function(response) { return response.json(); }) .then(function(data) { localStorage.setItem('gaip_user_id', data.detail.response); resolve(data.detail.response); }) .catch(reject); } }); };

member_id can be set by you depending on how your site identifies unique users such as user ID, phone number, email address etc.

Collecting and sending user browsing data

Set up a variable to capture the product name/title/ID, when the user goes a product details page or clicks on a product to enlarge it or open a pop-up etc.

Set up a trigger so that the tag would fire when the user goes browses an item (Go to product detail page or quick view options etc.).

Create a custom HTML Tag with the above trigger and variable and put the below code in the tag.

Collecting and sending user purchase data

Set up a variable to capture the all the purchase detail, when the user makes a purchase. This could be from the purchase confirmation page etc.

Set up a trigger so that the tag would fire when the user make the purchase.

Create a custom HTML Tag with the above trigger to send the information to endpoint POST /v1/purchase or POST /v1/purchase/client

Collecting and sending user rating data

Setup variables to capture the product name/title/ID and the rating, when the user rates an item positively or negatively. We can also consider an item is positively rated when user adds the item to wishlist.

Set up a trigger for the tag to fire when the user rates a product.

Create a custom HTML Tag with the above trigger and variables to send the information to the endpoint POST /v1/rating or POST /v1/rating/client

Collecting and sending user data

Setup variable to capture the user information.

For this, the trigger could be setup up when the user logs or update their information.

Create a custom HTML Tag with the above trigger and variables to send the information to the endpoint POST /v1/user or POST /v1/user/client

Server to server integration

For server to server integration, you will need to generate Project key and API as mentioned in the credentials section.

User information

The below request path, takes user information, such as name, age, gender, address and saves them in the gaip database.

POST /v1/user

Here is an example request body

json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "user_info": { "address": "string", "gender": "integer --> 1 for male or 2 for female or 3 for others", "age": 25, "user_type": [ { "key_name1": "value1_value2", "separator": "_" }, { "key_name2": "value3" } ] } } You can find the sample code for implementation here

Product browse

You can use the below endpoint to capture user browsing information and save them in GAIP database js POST /v1/items/browse

It takes user_id and item_id as required parameters.

Here is an example value of the request body JSON { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_id": "1000764491" }

You can find sample code here

Product purchase

You can use the below endpoint to capture user's product purchase information and save them in GAIP database

POST /v1/purchase It takes user_id, item_list which includes item_id, price, quantity for a specific item as required parameters.

Here is an example request body json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_list": [ { "item_id": "1000757666", "price": 5000, "quantity": 1 }, { "item_id": "1000764491", "price": 400, "quantity": 7 } ] }

You a find sample code for this implementation here

Product rating

You can use the below endpoint to capture user's product rating information and save them in GAIP database

POST /v1/rating

It takes user_id, item_id, and rating for the specific item as required parameters.

Here is a sample request body

json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_id": "1000764491", "rating": "1" }

You can find the sample code for this implementation here

If you want to save your data with bulk upload you can use above-mentioned endpoint.

Client to server integration

For client to server integration, you will need to generate client key as described in the Credentials sections. Once the client key is ready, you can directly send the request from your client side to GAIP, using the client key provided.

Note that while generating client key, you can add whitelisted domains, which whitelists the request origin. This is recommended to enhance security.

The rest of the implementation method is same as server to server integration.

Import user behavior data

Similar to data integration, all 4 kinds of user information (browse, purchase, rating, user) can be bulk uploaded. This could be useful if you already have this information from the past and want to import it into GAIP.

To import user behavior and user information in bulk, first you need to create mapper to match the keys with GAIP.

To create the mapper, the endpoints with the example request bodies can be found here in the gigalogy recommender page. You can also find the sample codes for mapper creation here in the API documentation page

Next we will use the below 4 endpoints to upload each category of data in bulk

Request path for product browsing history: POST /v1/items/browse/save
Request path for purchase history: POST /v1/items/purchase/save
Request path for rating history: POST /v1/items/rating/save
Request path to upload user information in bulk: POST /v1/users/save

You can find these endpoints with the example request bodies here. The sample code can be found here in the API documentation page

Previous
Environment setup
Next
Training your data
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/03_Data_integration_%26_user_behavior_collection/#comparison-of-each-approach
Skip to content
Gigalogy Tutorial
Integration of Catalogue information and user behavior data
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Catalogue information integration.
Uploading data using a file
Fetch item from external API
Search Items in GAIP after import
User behavior data collection integration
Comparison of each approach.
Google Tag manager
Generating user ID
Collecting and sending user browsing data
Collecting and sending user purchase data
Collecting and sending user rating data
Collecting and sending user data
Server to server integration
User information
Product browse
Product purchase
Product rating
Client to server integration
Import user behavior data
Integration of Catalogue information and user behavior data

This tutorial will cover how to integrate your catalogue information into GAIP and how to set up user behavior tracking and integrate with GAIP.

Catalogue information integration.

Prerequisite: Mapping creation and the index creation is done.

Info

This step can also be done from our platform (GAIP). Refer here for detail.

There are two ways to import your catalogue information into GAIP.

Upload you catalogue information as a CSV or JSON file using endpoint POST /v1/item/save.
Fetch data from your API (or any external API) using endpoint POST /v1/item/save/remote.
Uploading data using a file

For uploading the data using a CSV file or JSON file, please use the POST /v1/item/save endpoint. Simply upload the file and confirm the server response is success. Confirm the task was successful using the GET/v1/tasks/{task_id} endpoint.

Info

This will throw an error and task will fail, if the keys during the Mapping creation step does not match with the keys in the file, OR if the indices were not created succesfully.

Fetch item from external API

To fetch data from external API, use the POST /v1/item/save/remote endpoint. The key and value types and an example request body for the endpoint can be found here in our sandbox.

After hitting either of the endpoint above to import your data into GAIP, you will get a task ID in the response. Use this task ID and hit the /v1/tasks/{task_id} endpoint to confirm the operation was successful. In case it fails, you can also find the details there. For API documentation, please refer <>

Search Items in GAIP after import

You can search items by passing list of item ids fromPOST /v1/items/search endpoint. This endpoint will return searched items with item details.

It is recommended to use this endpoint to confirm that the item catalogue is successfully imported into your project.

User behavior data collection integration

GAIP can collect different user behavior related information to optimize the recommendation for the user. Types of data collected are listed below with their endpoints.

Data type	Endpoint
Product browsing: When user browse products.	/v1/items/browse or /v1/items/browse/client
Product Purchase: When user purchase a product with its quantity.	/v1/items/purchase or /v1/items/purchase/client
Rating: When a user rates a product.	/v1/items/rating or /v1/items/rating/client
User: User information such as age, gender and other customized attributes depending on your website.	/v1/users or /v1/users/client

You will find these endpoints listed in our Sandbox under section "User Data Collection". Please check the required parameters, value types and example request bodies for all the endpoints there.

There are 3 ways to integrate user behavior data collection with GAIP

Google Tag Manager
Server to server integration
Client to server integration

You can also bulk upload user behavior data from the past. For that, please refer to Import user behavior data section.

Comparison of each approach.
Approach	GTM	Server to Server	Client to Server
Description	Use Google Tag Manager (GTM) to collect data (User behavior) from your website and send it to GAIP via endpoint.	The data is captured in the backend server of your application and then sent to GAIP via endpoint.	The data is directly sent from your front end (Client side) to GAIP via endpoint.
Pros	Easy to implement, Minimum coding required, Flexible configuration	More secure, Data integrity, Controlled environment	Real-time data, Less server dependency
Cons	Limited customization, need to have basic knowledge about GTM, Dependency on third-party service, Might not work for certain browsers and plugins like AdBlockers	More complex to set up, potential latency, maintenance required	Less secure, Potential for inconsistent data, dependency on client-side behavior

Below we will show the implementation of each approach.

BE ADVISED: The following is a general guideline, and it may vary across different websites, contingent upon the specific implementation of your website.

Google Tag manager

Prerequisite: Your website must have GTM setup. If you do not have GTM setup, you can easily do the setup by following the guidelines here.

If you are not familiar with basic GTM concepts, such as Tags, Triggers and Variables, please familiarize yourself first with these concepts before proceeding with this approach. You can find more resources related to this here.

Generating user ID

In our sandbox, Notice that in the user data collection endpoints, every endpoint has a parameter called user_id, and member_id. These are vital to identify each user so that you can personalize their experience. user_id is generated by GAIP for each user of your site. The endpoint to generate and user_id is GET /v1/users/generate/id. You can generate the user_id using GTM using below code. This code can be used with every Tag, which checks if there is a user_id and creates one if there is none.

js // Function to get or generate 'gaip_user_id' using a function expression var getGaipUser = function() { return new Promise(function(resolve, reject) { if (gaipUser !== null) { resolve(gaipUser); } else { var idHeaders = new Headers(); idHeaders.append("project-key", "{{ YOUR_PROJECT_KEY_HERE }}"); idHeaders.append("api-key", "{{ YOUR_API_KEY_HERE }}"); fetch("None/v1/users/generate/id", { headers: idHeaders }) .then(function(response) { return response.json(); }) .then(function(data) { localStorage.setItem('gaip_user_id', data.detail.response); resolve(data.detail.response); }) .catch(reject); } }); };

member_id can be set by you depending on how your site identifies unique users such as user ID, phone number, email address etc.

Collecting and sending user browsing data

Set up a variable to capture the product name/title/ID, when the user goes a product details page or clicks on a product to enlarge it or open a pop-up etc.

Set up a trigger so that the tag would fire when the user goes browses an item (Go to product detail page or quick view options etc.).

Create a custom HTML Tag with the above trigger and variable and put the below code in the tag.

Collecting and sending user purchase data

Set up a variable to capture the all the purchase detail, when the user makes a purchase. This could be from the purchase confirmation page etc.

Set up a trigger so that the tag would fire when the user make the purchase.

Create a custom HTML Tag with the above trigger to send the information to endpoint POST /v1/purchase or POST /v1/purchase/client

Collecting and sending user rating data

Setup variables to capture the product name/title/ID and the rating, when the user rates an item positively or negatively. We can also consider an item is positively rated when user adds the item to wishlist.

Set up a trigger for the tag to fire when the user rates a product.

Create a custom HTML Tag with the above trigger and variables to send the information to the endpoint POST /v1/rating or POST /v1/rating/client

Collecting and sending user data

Setup variable to capture the user information.

For this, the trigger could be setup up when the user logs or update their information.

Create a custom HTML Tag with the above trigger and variables to send the information to the endpoint POST /v1/user or POST /v1/user/client

Server to server integration

For server to server integration, you will need to generate Project key and API as mentioned in the credentials section.

User information

The below request path, takes user information, such as name, age, gender, address and saves them in the gaip database.

POST /v1/user

Here is an example request body

json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "user_info": { "address": "string", "gender": "integer --> 1 for male or 2 for female or 3 for others", "age": 25, "user_type": [ { "key_name1": "value1_value2", "separator": "_" }, { "key_name2": "value3" } ] } } You can find the sample code for implementation here

Product browse

You can use the below endpoint to capture user browsing information and save them in GAIP database js POST /v1/items/browse

It takes user_id and item_id as required parameters.

Here is an example value of the request body JSON { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_id": "1000764491" }

You can find sample code here

Product purchase

You can use the below endpoint to capture user's product purchase information and save them in GAIP database

POST /v1/purchase It takes user_id, item_list which includes item_id, price, quantity for a specific item as required parameters.

Here is an example request body json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_list": [ { "item_id": "1000757666", "price": 5000, "quantity": 1 }, { "item_id": "1000764491", "price": 400, "quantity": 7 } ] }

You a find sample code for this implementation here

Product rating

You can use the below endpoint to capture user's product rating information and save them in GAIP database

POST /v1/rating

It takes user_id, item_id, and rating for the specific item as required parameters.

Here is a sample request body

json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_id": "1000764491", "rating": "1" }

You can find the sample code for this implementation here

If you want to save your data with bulk upload you can use above-mentioned endpoint.

Client to server integration

For client to server integration, you will need to generate client key as described in the Credentials sections. Once the client key is ready, you can directly send the request from your client side to GAIP, using the client key provided.

Note that while generating client key, you can add whitelisted domains, which whitelists the request origin. This is recommended to enhance security.

The rest of the implementation method is same as server to server integration.

Import user behavior data

Similar to data integration, all 4 kinds of user information (browse, purchase, rating, user) can be bulk uploaded. This could be useful if you already have this information from the past and want to import it into GAIP.

To import user behavior and user information in bulk, first you need to create mapper to match the keys with GAIP.

To create the mapper, the endpoints with the example request bodies can be found here in the gigalogy recommender page. You can also find the sample codes for mapper creation here in the API documentation page

Next we will use the below 4 endpoints to upload each category of data in bulk

Request path for product browsing history: POST /v1/items/browse/save
Request path for purchase history: POST /v1/items/purchase/save
Request path for rating history: POST /v1/items/rating/save
Request path to upload user information in bulk: POST /v1/users/save

You can find these endpoints with the example request bodies here. The sample code can be found here in the API documentation page

Previous
Environment setup
Next
Training your data
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/03_Data_integration_%26_user_behavior_collection/#google-tag-manager
Skip to content
Gigalogy Tutorial
Integration of Catalogue information and user behavior data
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Catalogue information integration.
Uploading data using a file
Fetch item from external API
Search Items in GAIP after import
User behavior data collection integration
Comparison of each approach.
Google Tag manager
Generating user ID
Collecting and sending user browsing data
Collecting and sending user purchase data
Collecting and sending user rating data
Collecting and sending user data
Server to server integration
User information
Product browse
Product purchase
Product rating
Client to server integration
Import user behavior data
Integration of Catalogue information and user behavior data

This tutorial will cover how to integrate your catalogue information into GAIP and how to set up user behavior tracking and integrate with GAIP.

Catalogue information integration.

Prerequisite: Mapping creation and the index creation is done.

Info

This step can also be done from our platform (GAIP). Refer here for detail.

There are two ways to import your catalogue information into GAIP.

Upload you catalogue information as a CSV or JSON file using endpoint POST /v1/item/save.
Fetch data from your API (or any external API) using endpoint POST /v1/item/save/remote.
Uploading data using a file

For uploading the data using a CSV file or JSON file, please use the POST /v1/item/save endpoint. Simply upload the file and confirm the server response is success. Confirm the task was successful using the GET/v1/tasks/{task_id} endpoint.

Info

This will throw an error and task will fail, if the keys during the Mapping creation step does not match with the keys in the file, OR if the indices were not created succesfully.

Fetch item from external API

To fetch data from external API, use the POST /v1/item/save/remote endpoint. The key and value types and an example request body for the endpoint can be found here in our sandbox.

After hitting either of the endpoint above to import your data into GAIP, you will get a task ID in the response. Use this task ID and hit the /v1/tasks/{task_id} endpoint to confirm the operation was successful. In case it fails, you can also find the details there. For API documentation, please refer <>

Search Items in GAIP after import

You can search items by passing list of item ids fromPOST /v1/items/search endpoint. This endpoint will return searched items with item details.

It is recommended to use this endpoint to confirm that the item catalogue is successfully imported into your project.

User behavior data collection integration

GAIP can collect different user behavior related information to optimize the recommendation for the user. Types of data collected are listed below with their endpoints.

Data type	Endpoint
Product browsing: When user browse products.	/v1/items/browse or /v1/items/browse/client
Product Purchase: When user purchase a product with its quantity.	/v1/items/purchase or /v1/items/purchase/client
Rating: When a user rates a product.	/v1/items/rating or /v1/items/rating/client
User: User information such as age, gender and other customized attributes depending on your website.	/v1/users or /v1/users/client

You will find these endpoints listed in our Sandbox under section "User Data Collection". Please check the required parameters, value types and example request bodies for all the endpoints there.

There are 3 ways to integrate user behavior data collection with GAIP

Google Tag Manager
Server to server integration
Client to server integration

You can also bulk upload user behavior data from the past. For that, please refer to Import user behavior data section.

Comparison of each approach.
Approach	GTM	Server to Server	Client to Server
Description	Use Google Tag Manager (GTM) to collect data (User behavior) from your website and send it to GAIP via endpoint.	The data is captured in the backend server of your application and then sent to GAIP via endpoint.	The data is directly sent from your front end (Client side) to GAIP via endpoint.
Pros	Easy to implement, Minimum coding required, Flexible configuration	More secure, Data integrity, Controlled environment	Real-time data, Less server dependency
Cons	Limited customization, need to have basic knowledge about GTM, Dependency on third-party service, Might not work for certain browsers and plugins like AdBlockers	More complex to set up, potential latency, maintenance required	Less secure, Potential for inconsistent data, dependency on client-side behavior

Below we will show the implementation of each approach.

BE ADVISED: The following is a general guideline, and it may vary across different websites, contingent upon the specific implementation of your website.

Google Tag manager

Prerequisite: Your website must have GTM setup. If you do not have GTM setup, you can easily do the setup by following the guidelines here.

If you are not familiar with basic GTM concepts, such as Tags, Triggers and Variables, please familiarize yourself first with these concepts before proceeding with this approach. You can find more resources related to this here.

Generating user ID

In our sandbox, Notice that in the user data collection endpoints, every endpoint has a parameter called user_id, and member_id. These are vital to identify each user so that you can personalize their experience. user_id is generated by GAIP for each user of your site. The endpoint to generate and user_id is GET /v1/users/generate/id. You can generate the user_id using GTM using below code. This code can be used with every Tag, which checks if there is a user_id and creates one if there is none.

js // Function to get or generate 'gaip_user_id' using a function expression var getGaipUser = function() { return new Promise(function(resolve, reject) { if (gaipUser !== null) { resolve(gaipUser); } else { var idHeaders = new Headers(); idHeaders.append("project-key", "{{ YOUR_PROJECT_KEY_HERE }}"); idHeaders.append("api-key", "{{ YOUR_API_KEY_HERE }}"); fetch("None/v1/users/generate/id", { headers: idHeaders }) .then(function(response) { return response.json(); }) .then(function(data) { localStorage.setItem('gaip_user_id', data.detail.response); resolve(data.detail.response); }) .catch(reject); } }); };

member_id can be set by you depending on how your site identifies unique users such as user ID, phone number, email address etc.

Collecting and sending user browsing data

Set up a variable to capture the product name/title/ID, when the user goes a product details page or clicks on a product to enlarge it or open a pop-up etc.

Set up a trigger so that the tag would fire when the user goes browses an item (Go to product detail page or quick view options etc.).

Create a custom HTML Tag with the above trigger and variable and put the below code in the tag.

Collecting and sending user purchase data

Set up a variable to capture the all the purchase detail, when the user makes a purchase. This could be from the purchase confirmation page etc.

Set up a trigger so that the tag would fire when the user make the purchase.

Create a custom HTML Tag with the above trigger to send the information to endpoint POST /v1/purchase or POST /v1/purchase/client

Collecting and sending user rating data

Setup variables to capture the product name/title/ID and the rating, when the user rates an item positively or negatively. We can also consider an item is positively rated when user adds the item to wishlist.

Set up a trigger for the tag to fire when the user rates a product.

Create a custom HTML Tag with the above trigger and variables to send the information to the endpoint POST /v1/rating or POST /v1/rating/client

Collecting and sending user data

Setup variable to capture the user information.

For this, the trigger could be setup up when the user logs or update their information.

Create a custom HTML Tag with the above trigger and variables to send the information to the endpoint POST /v1/user or POST /v1/user/client

Server to server integration

For server to server integration, you will need to generate Project key and API as mentioned in the credentials section.

User information

The below request path, takes user information, such as name, age, gender, address and saves them in the gaip database.

POST /v1/user

Here is an example request body

json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "user_info": { "address": "string", "gender": "integer --> 1 for male or 2 for female or 3 for others", "age": 25, "user_type": [ { "key_name1": "value1_value2", "separator": "_" }, { "key_name2": "value3" } ] } } You can find the sample code for implementation here

Product browse

You can use the below endpoint to capture user browsing information and save them in GAIP database js POST /v1/items/browse

It takes user_id and item_id as required parameters.

Here is an example value of the request body JSON { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_id": "1000764491" }

You can find sample code here

Product purchase

You can use the below endpoint to capture user's product purchase information and save them in GAIP database

POST /v1/purchase It takes user_id, item_list which includes item_id, price, quantity for a specific item as required parameters.

Here is an example request body json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_list": [ { "item_id": "1000757666", "price": 5000, "quantity": 1 }, { "item_id": "1000764491", "price": 400, "quantity": 7 } ] }

You a find sample code for this implementation here

Product rating

You can use the below endpoint to capture user's product rating information and save them in GAIP database

POST /v1/rating

It takes user_id, item_id, and rating for the specific item as required parameters.

Here is a sample request body

json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_id": "1000764491", "rating": "1" }

You can find the sample code for this implementation here

If you want to save your data with bulk upload you can use above-mentioned endpoint.

Client to server integration

For client to server integration, you will need to generate client key as described in the Credentials sections. Once the client key is ready, you can directly send the request from your client side to GAIP, using the client key provided.

Note that while generating client key, you can add whitelisted domains, which whitelists the request origin. This is recommended to enhance security.

The rest of the implementation method is same as server to server integration.

Import user behavior data

Similar to data integration, all 4 kinds of user information (browse, purchase, rating, user) can be bulk uploaded. This could be useful if you already have this information from the past and want to import it into GAIP.

To import user behavior and user information in bulk, first you need to create mapper to match the keys with GAIP.

To create the mapper, the endpoints with the example request bodies can be found here in the gigalogy recommender page. You can also find the sample codes for mapper creation here in the API documentation page

Next we will use the below 4 endpoints to upload each category of data in bulk

Request path for product browsing history: POST /v1/items/browse/save
Request path for purchase history: POST /v1/items/purchase/save
Request path for rating history: POST /v1/items/rating/save
Request path to upload user information in bulk: POST /v1/users/save

You can find these endpoints with the example request bodies here. The sample code can be found here in the API documentation page

Previous
Environment setup
Next
Training your data
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/03_Data_integration_%26_user_behavior_collection/#generating-user-id
Skip to content
Gigalogy Tutorial
Integration of Catalogue information and user behavior data
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Catalogue information integration.
Uploading data using a file
Fetch item from external API
Search Items in GAIP after import
User behavior data collection integration
Comparison of each approach.
Google Tag manager
Generating user ID
Collecting and sending user browsing data
Collecting and sending user purchase data
Collecting and sending user rating data
Collecting and sending user data
Server to server integration
User information
Product browse
Product purchase
Product rating
Client to server integration
Import user behavior data
Integration of Catalogue information and user behavior data

This tutorial will cover how to integrate your catalogue information into GAIP and how to set up user behavior tracking and integrate with GAIP.

Catalogue information integration.

Prerequisite: Mapping creation and the index creation is done.

Info

This step can also be done from our platform (GAIP). Refer here for detail.

There are two ways to import your catalogue information into GAIP.

Upload you catalogue information as a CSV or JSON file using endpoint POST /v1/item/save.
Fetch data from your API (or any external API) using endpoint POST /v1/item/save/remote.
Uploading data using a file

For uploading the data using a CSV file or JSON file, please use the POST /v1/item/save endpoint. Simply upload the file and confirm the server response is success. Confirm the task was successful using the GET/v1/tasks/{task_id} endpoint.

Info

This will throw an error and task will fail, if the keys during the Mapping creation step does not match with the keys in the file, OR if the indices were not created succesfully.

Fetch item from external API

To fetch data from external API, use the POST /v1/item/save/remote endpoint. The key and value types and an example request body for the endpoint can be found here in our sandbox.

After hitting either of the endpoint above to import your data into GAIP, you will get a task ID in the response. Use this task ID and hit the /v1/tasks/{task_id} endpoint to confirm the operation was successful. In case it fails, you can also find the details there. For API documentation, please refer <>

Search Items in GAIP after import

You can search items by passing list of item ids fromPOST /v1/items/search endpoint. This endpoint will return searched items with item details.

It is recommended to use this endpoint to confirm that the item catalogue is successfully imported into your project.

User behavior data collection integration

GAIP can collect different user behavior related information to optimize the recommendation for the user. Types of data collected are listed below with their endpoints.

Data type	Endpoint
Product browsing: When user browse products.	/v1/items/browse or /v1/items/browse/client
Product Purchase: When user purchase a product with its quantity.	/v1/items/purchase or /v1/items/purchase/client
Rating: When a user rates a product.	/v1/items/rating or /v1/items/rating/client
User: User information such as age, gender and other customized attributes depending on your website.	/v1/users or /v1/users/client

You will find these endpoints listed in our Sandbox under section "User Data Collection". Please check the required parameters, value types and example request bodies for all the endpoints there.

There are 3 ways to integrate user behavior data collection with GAIP

Google Tag Manager
Server to server integration
Client to server integration

You can also bulk upload user behavior data from the past. For that, please refer to Import user behavior data section.

Comparison of each approach.
Approach	GTM	Server to Server	Client to Server
Description	Use Google Tag Manager (GTM) to collect data (User behavior) from your website and send it to GAIP via endpoint.	The data is captured in the backend server of your application and then sent to GAIP via endpoint.	The data is directly sent from your front end (Client side) to GAIP via endpoint.
Pros	Easy to implement, Minimum coding required, Flexible configuration	More secure, Data integrity, Controlled environment	Real-time data, Less server dependency
Cons	Limited customization, need to have basic knowledge about GTM, Dependency on third-party service, Might not work for certain browsers and plugins like AdBlockers	More complex to set up, potential latency, maintenance required	Less secure, Potential for inconsistent data, dependency on client-side behavior

Below we will show the implementation of each approach.

BE ADVISED: The following is a general guideline, and it may vary across different websites, contingent upon the specific implementation of your website.

Google Tag manager

Prerequisite: Your website must have GTM setup. If you do not have GTM setup, you can easily do the setup by following the guidelines here.

If you are not familiar with basic GTM concepts, such as Tags, Triggers and Variables, please familiarize yourself first with these concepts before proceeding with this approach. You can find more resources related to this here.

Generating user ID

In our sandbox, Notice that in the user data collection endpoints, every endpoint has a parameter called user_id, and member_id. These are vital to identify each user so that you can personalize their experience. user_id is generated by GAIP for each user of your site. The endpoint to generate and user_id is GET /v1/users/generate/id. You can generate the user_id using GTM using below code. This code can be used with every Tag, which checks if there is a user_id and creates one if there is none.

js // Function to get or generate 'gaip_user_id' using a function expression var getGaipUser = function() { return new Promise(function(resolve, reject) { if (gaipUser !== null) { resolve(gaipUser); } else { var idHeaders = new Headers(); idHeaders.append("project-key", "{{ YOUR_PROJECT_KEY_HERE }}"); idHeaders.append("api-key", "{{ YOUR_API_KEY_HERE }}"); fetch("None/v1/users/generate/id", { headers: idHeaders }) .then(function(response) { return response.json(); }) .then(function(data) { localStorage.setItem('gaip_user_id', data.detail.response); resolve(data.detail.response); }) .catch(reject); } }); };

member_id can be set by you depending on how your site identifies unique users such as user ID, phone number, email address etc.

Collecting and sending user browsing data

Set up a variable to capture the product name/title/ID, when the user goes a product details page or clicks on a product to enlarge it or open a pop-up etc.

Set up a trigger so that the tag would fire when the user goes browses an item (Go to product detail page or quick view options etc.).

Create a custom HTML Tag with the above trigger and variable and put the below code in the tag.

Collecting and sending user purchase data

Set up a variable to capture the all the purchase detail, when the user makes a purchase. This could be from the purchase confirmation page etc.

Set up a trigger so that the tag would fire when the user make the purchase.

Create a custom HTML Tag with the above trigger to send the information to endpoint POST /v1/purchase or POST /v1/purchase/client

Collecting and sending user rating data

Setup variables to capture the product name/title/ID and the rating, when the user rates an item positively or negatively. We can also consider an item is positively rated when user adds the item to wishlist.

Set up a trigger for the tag to fire when the user rates a product.

Create a custom HTML Tag with the above trigger and variables to send the information to the endpoint POST /v1/rating or POST /v1/rating/client

Collecting and sending user data

Setup variable to capture the user information.

For this, the trigger could be setup up when the user logs or update their information.

Create a custom HTML Tag with the above trigger and variables to send the information to the endpoint POST /v1/user or POST /v1/user/client

Server to server integration

For server to server integration, you will need to generate Project key and API as mentioned in the credentials section.

User information

The below request path, takes user information, such as name, age, gender, address and saves them in the gaip database.

POST /v1/user

Here is an example request body

json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "user_info": { "address": "string", "gender": "integer --> 1 for male or 2 for female or 3 for others", "age": 25, "user_type": [ { "key_name1": "value1_value2", "separator": "_" }, { "key_name2": "value3" } ] } } You can find the sample code for implementation here

Product browse

You can use the below endpoint to capture user browsing information and save them in GAIP database js POST /v1/items/browse

It takes user_id and item_id as required parameters.

Here is an example value of the request body JSON { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_id": "1000764491" }

You can find sample code here

Product purchase

You can use the below endpoint to capture user's product purchase information and save them in GAIP database

POST /v1/purchase It takes user_id, item_list which includes item_id, price, quantity for a specific item as required parameters.

Here is an example request body json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_list": [ { "item_id": "1000757666", "price": 5000, "quantity": 1 }, { "item_id": "1000764491", "price": 400, "quantity": 7 } ] }

You a find sample code for this implementation here

Product rating

You can use the below endpoint to capture user's product rating information and save them in GAIP database

POST /v1/rating

It takes user_id, item_id, and rating for the specific item as required parameters.

Here is a sample request body

json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_id": "1000764491", "rating": "1" }

You can find the sample code for this implementation here

If you want to save your data with bulk upload you can use above-mentioned endpoint.

Client to server integration

For client to server integration, you will need to generate client key as described in the Credentials sections. Once the client key is ready, you can directly send the request from your client side to GAIP, using the client key provided.

Note that while generating client key, you can add whitelisted domains, which whitelists the request origin. This is recommended to enhance security.

The rest of the implementation method is same as server to server integration.

Import user behavior data

Similar to data integration, all 4 kinds of user information (browse, purchase, rating, user) can be bulk uploaded. This could be useful if you already have this information from the past and want to import it into GAIP.

To import user behavior and user information in bulk, first you need to create mapper to match the keys with GAIP.

To create the mapper, the endpoints with the example request bodies can be found here in the gigalogy recommender page. You can also find the sample codes for mapper creation here in the API documentation page

Next we will use the below 4 endpoints to upload each category of data in bulk

Request path for product browsing history: POST /v1/items/browse/save
Request path for purchase history: POST /v1/items/purchase/save
Request path for rating history: POST /v1/items/rating/save
Request path to upload user information in bulk: POST /v1/users/save

You can find these endpoints with the example request bodies here. The sample code can be found here in the API documentation page

Previous
Environment setup
Next
Training your data
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/03_Data_integration_%26_user_behavior_collection/#collecting-and-sending-user-browsing-data
Skip to content
Gigalogy Tutorial
Integration of Catalogue information and user behavior data
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Catalogue information integration.
Uploading data using a file
Fetch item from external API
Search Items in GAIP after import
User behavior data collection integration
Comparison of each approach.
Google Tag manager
Generating user ID
Collecting and sending user browsing data
Collecting and sending user purchase data
Collecting and sending user rating data
Collecting and sending user data
Server to server integration
User information
Product browse
Product purchase
Product rating
Client to server integration
Import user behavior data
Integration of Catalogue information and user behavior data

This tutorial will cover how to integrate your catalogue information into GAIP and how to set up user behavior tracking and integrate with GAIP.

Catalogue information integration.

Prerequisite: Mapping creation and the index creation is done.

Info

This step can also be done from our platform (GAIP). Refer here for detail.

There are two ways to import your catalogue information into GAIP.

Upload you catalogue information as a CSV or JSON file using endpoint POST /v1/item/save.
Fetch data from your API (or any external API) using endpoint POST /v1/item/save/remote.
Uploading data using a file

For uploading the data using a CSV file or JSON file, please use the POST /v1/item/save endpoint. Simply upload the file and confirm the server response is success. Confirm the task was successful using the GET/v1/tasks/{task_id} endpoint.

Info

This will throw an error and task will fail, if the keys during the Mapping creation step does not match with the keys in the file, OR if the indices were not created succesfully.

Fetch item from external API

To fetch data from external API, use the POST /v1/item/save/remote endpoint. The key and value types and an example request body for the endpoint can be found here in our sandbox.

After hitting either of the endpoint above to import your data into GAIP, you will get a task ID in the response. Use this task ID and hit the /v1/tasks/{task_id} endpoint to confirm the operation was successful. In case it fails, you can also find the details there. For API documentation, please refer <>

Search Items in GAIP after import

You can search items by passing list of item ids fromPOST /v1/items/search endpoint. This endpoint will return searched items with item details.

It is recommended to use this endpoint to confirm that the item catalogue is successfully imported into your project.

User behavior data collection integration

GAIP can collect different user behavior related information to optimize the recommendation for the user. Types of data collected are listed below with their endpoints.

Data type	Endpoint
Product browsing: When user browse products.	/v1/items/browse or /v1/items/browse/client
Product Purchase: When user purchase a product with its quantity.	/v1/items/purchase or /v1/items/purchase/client
Rating: When a user rates a product.	/v1/items/rating or /v1/items/rating/client
User: User information such as age, gender and other customized attributes depending on your website.	/v1/users or /v1/users/client

You will find these endpoints listed in our Sandbox under section "User Data Collection". Please check the required parameters, value types and example request bodies for all the endpoints there.

There are 3 ways to integrate user behavior data collection with GAIP

Google Tag Manager
Server to server integration
Client to server integration

You can also bulk upload user behavior data from the past. For that, please refer to Import user behavior data section.

Comparison of each approach.
Approach	GTM	Server to Server	Client to Server
Description	Use Google Tag Manager (GTM) to collect data (User behavior) from your website and send it to GAIP via endpoint.	The data is captured in the backend server of your application and then sent to GAIP via endpoint.	The data is directly sent from your front end (Client side) to GAIP via endpoint.
Pros	Easy to implement, Minimum coding required, Flexible configuration	More secure, Data integrity, Controlled environment	Real-time data, Less server dependency
Cons	Limited customization, need to have basic knowledge about GTM, Dependency on third-party service, Might not work for certain browsers and plugins like AdBlockers	More complex to set up, potential latency, maintenance required	Less secure, Potential for inconsistent data, dependency on client-side behavior

Below we will show the implementation of each approach.

BE ADVISED: The following is a general guideline, and it may vary across different websites, contingent upon the specific implementation of your website.

Google Tag manager

Prerequisite: Your website must have GTM setup. If you do not have GTM setup, you can easily do the setup by following the guidelines here.

If you are not familiar with basic GTM concepts, such as Tags, Triggers and Variables, please familiarize yourself first with these concepts before proceeding with this approach. You can find more resources related to this here.

Generating user ID

In our sandbox, Notice that in the user data collection endpoints, every endpoint has a parameter called user_id, and member_id. These are vital to identify each user so that you can personalize their experience. user_id is generated by GAIP for each user of your site. The endpoint to generate and user_id is GET /v1/users/generate/id. You can generate the user_id using GTM using below code. This code can be used with every Tag, which checks if there is a user_id and creates one if there is none.

js // Function to get or generate 'gaip_user_id' using a function expression var getGaipUser = function() { return new Promise(function(resolve, reject) { if (gaipUser !== null) { resolve(gaipUser); } else { var idHeaders = new Headers(); idHeaders.append("project-key", "{{ YOUR_PROJECT_KEY_HERE }}"); idHeaders.append("api-key", "{{ YOUR_API_KEY_HERE }}"); fetch("None/v1/users/generate/id", { headers: idHeaders }) .then(function(response) { return response.json(); }) .then(function(data) { localStorage.setItem('gaip_user_id', data.detail.response); resolve(data.detail.response); }) .catch(reject); } }); };

member_id can be set by you depending on how your site identifies unique users such as user ID, phone number, email address etc.

Collecting and sending user browsing data

Set up a variable to capture the product name/title/ID, when the user goes a product details page or clicks on a product to enlarge it or open a pop-up etc.

Set up a trigger so that the tag would fire when the user goes browses an item (Go to product detail page or quick view options etc.).

Create a custom HTML Tag with the above trigger and variable and put the below code in the tag.

Collecting and sending user purchase data

Set up a variable to capture the all the purchase detail, when the user makes a purchase. This could be from the purchase confirmation page etc.

Set up a trigger so that the tag would fire when the user make the purchase.

Create a custom HTML Tag with the above trigger to send the information to endpoint POST /v1/purchase or POST /v1/purchase/client

Collecting and sending user rating data

Setup variables to capture the product name/title/ID and the rating, when the user rates an item positively or negatively. We can also consider an item is positively rated when user adds the item to wishlist.

Set up a trigger for the tag to fire when the user rates a product.

Create a custom HTML Tag with the above trigger and variables to send the information to the endpoint POST /v1/rating or POST /v1/rating/client

Collecting and sending user data

Setup variable to capture the user information.

For this, the trigger could be setup up when the user logs or update their information.

Create a custom HTML Tag with the above trigger and variables to send the information to the endpoint POST /v1/user or POST /v1/user/client

Server to server integration

For server to server integration, you will need to generate Project key and API as mentioned in the credentials section.

User information

The below request path, takes user information, such as name, age, gender, address and saves them in the gaip database.

POST /v1/user

Here is an example request body

json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "user_info": { "address": "string", "gender": "integer --> 1 for male or 2 for female or 3 for others", "age": 25, "user_type": [ { "key_name1": "value1_value2", "separator": "_" }, { "key_name2": "value3" } ] } } You can find the sample code for implementation here

Product browse

You can use the below endpoint to capture user browsing information and save them in GAIP database js POST /v1/items/browse

It takes user_id and item_id as required parameters.

Here is an example value of the request body JSON { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_id": "1000764491" }

You can find sample code here

Product purchase

You can use the below endpoint to capture user's product purchase information and save them in GAIP database

POST /v1/purchase It takes user_id, item_list which includes item_id, price, quantity for a specific item as required parameters.

Here is an example request body json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_list": [ { "item_id": "1000757666", "price": 5000, "quantity": 1 }, { "item_id": "1000764491", "price": 400, "quantity": 7 } ] }

You a find sample code for this implementation here

Product rating

You can use the below endpoint to capture user's product rating information and save them in GAIP database

POST /v1/rating

It takes user_id, item_id, and rating for the specific item as required parameters.

Here is a sample request body

json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_id": "1000764491", "rating": "1" }

You can find the sample code for this implementation here

If you want to save your data with bulk upload you can use above-mentioned endpoint.

Client to server integration

For client to server integration, you will need to generate client key as described in the Credentials sections. Once the client key is ready, you can directly send the request from your client side to GAIP, using the client key provided.

Note that while generating client key, you can add whitelisted domains, which whitelists the request origin. This is recommended to enhance security.

The rest of the implementation method is same as server to server integration.

Import user behavior data

Similar to data integration, all 4 kinds of user information (browse, purchase, rating, user) can be bulk uploaded. This could be useful if you already have this information from the past and want to import it into GAIP.

To import user behavior and user information in bulk, first you need to create mapper to match the keys with GAIP.

To create the mapper, the endpoints with the example request bodies can be found here in the gigalogy recommender page. You can also find the sample codes for mapper creation here in the API documentation page

Next we will use the below 4 endpoints to upload each category of data in bulk

Request path for product browsing history: POST /v1/items/browse/save
Request path for purchase history: POST /v1/items/purchase/save
Request path for rating history: POST /v1/items/rating/save
Request path to upload user information in bulk: POST /v1/users/save

You can find these endpoints with the example request bodies here. The sample code can be found here in the API documentation page

Previous
Environment setup
Next
Training your data
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/03_Data_integration_%26_user_behavior_collection/#collecting-and-sending-user-purchase-data
Skip to content
Gigalogy Tutorial
Integration of Catalogue information and user behavior data
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Catalogue information integration.
Uploading data using a file
Fetch item from external API
Search Items in GAIP after import
User behavior data collection integration
Comparison of each approach.
Google Tag manager
Generating user ID
Collecting and sending user browsing data
Collecting and sending user purchase data
Collecting and sending user rating data
Collecting and sending user data
Server to server integration
User information
Product browse
Product purchase
Product rating
Client to server integration
Import user behavior data
Integration of Catalogue information and user behavior data

This tutorial will cover how to integrate your catalogue information into GAIP and how to set up user behavior tracking and integrate with GAIP.

Catalogue information integration.

Prerequisite: Mapping creation and the index creation is done.

Info

This step can also be done from our platform (GAIP). Refer here for detail.

There are two ways to import your catalogue information into GAIP.

Upload you catalogue information as a CSV or JSON file using endpoint POST /v1/item/save.
Fetch data from your API (or any external API) using endpoint POST /v1/item/save/remote.
Uploading data using a file

For uploading the data using a CSV file or JSON file, please use the POST /v1/item/save endpoint. Simply upload the file and confirm the server response is success. Confirm the task was successful using the GET/v1/tasks/{task_id} endpoint.

Info

This will throw an error and task will fail, if the keys during the Mapping creation step does not match with the keys in the file, OR if the indices were not created succesfully.

Fetch item from external API

To fetch data from external API, use the POST /v1/item/save/remote endpoint. The key and value types and an example request body for the endpoint can be found here in our sandbox.

After hitting either of the endpoint above to import your data into GAIP, you will get a task ID in the response. Use this task ID and hit the /v1/tasks/{task_id} endpoint to confirm the operation was successful. In case it fails, you can also find the details there. For API documentation, please refer <>

Search Items in GAIP after import

You can search items by passing list of item ids fromPOST /v1/items/search endpoint. This endpoint will return searched items with item details.

It is recommended to use this endpoint to confirm that the item catalogue is successfully imported into your project.

User behavior data collection integration

GAIP can collect different user behavior related information to optimize the recommendation for the user. Types of data collected are listed below with their endpoints.

Data type	Endpoint
Product browsing: When user browse products.	/v1/items/browse or /v1/items/browse/client
Product Purchase: When user purchase a product with its quantity.	/v1/items/purchase or /v1/items/purchase/client
Rating: When a user rates a product.	/v1/items/rating or /v1/items/rating/client
User: User information such as age, gender and other customized attributes depending on your website.	/v1/users or /v1/users/client

You will find these endpoints listed in our Sandbox under section "User Data Collection". Please check the required parameters, value types and example request bodies for all the endpoints there.

There are 3 ways to integrate user behavior data collection with GAIP

Google Tag Manager
Server to server integration
Client to server integration

You can also bulk upload user behavior data from the past. For that, please refer to Import user behavior data section.

Comparison of each approach.
Approach	GTM	Server to Server	Client to Server
Description	Use Google Tag Manager (GTM) to collect data (User behavior) from your website and send it to GAIP via endpoint.	The data is captured in the backend server of your application and then sent to GAIP via endpoint.	The data is directly sent from your front end (Client side) to GAIP via endpoint.
Pros	Easy to implement, Minimum coding required, Flexible configuration	More secure, Data integrity, Controlled environment	Real-time data, Less server dependency
Cons	Limited customization, need to have basic knowledge about GTM, Dependency on third-party service, Might not work for certain browsers and plugins like AdBlockers	More complex to set up, potential latency, maintenance required	Less secure, Potential for inconsistent data, dependency on client-side behavior

Below we will show the implementation of each approach.

BE ADVISED: The following is a general guideline, and it may vary across different websites, contingent upon the specific implementation of your website.

Google Tag manager

Prerequisite: Your website must have GTM setup. If you do not have GTM setup, you can easily do the setup by following the guidelines here.

If you are not familiar with basic GTM concepts, such as Tags, Triggers and Variables, please familiarize yourself first with these concepts before proceeding with this approach. You can find more resources related to this here.

Generating user ID

In our sandbox, Notice that in the user data collection endpoints, every endpoint has a parameter called user_id, and member_id. These are vital to identify each user so that you can personalize their experience. user_id is generated by GAIP for each user of your site. The endpoint to generate and user_id is GET /v1/users/generate/id. You can generate the user_id using GTM using below code. This code can be used with every Tag, which checks if there is a user_id and creates one if there is none.

js // Function to get or generate 'gaip_user_id' using a function expression var getGaipUser = function() { return new Promise(function(resolve, reject) { if (gaipUser !== null) { resolve(gaipUser); } else { var idHeaders = new Headers(); idHeaders.append("project-key", "{{ YOUR_PROJECT_KEY_HERE }}"); idHeaders.append("api-key", "{{ YOUR_API_KEY_HERE }}"); fetch("None/v1/users/generate/id", { headers: idHeaders }) .then(function(response) { return response.json(); }) .then(function(data) { localStorage.setItem('gaip_user_id', data.detail.response); resolve(data.detail.response); }) .catch(reject); } }); };

member_id can be set by you depending on how your site identifies unique users such as user ID, phone number, email address etc.

Collecting and sending user browsing data

Set up a variable to capture the product name/title/ID, when the user goes a product details page or clicks on a product to enlarge it or open a pop-up etc.

Set up a trigger so that the tag would fire when the user goes browses an item (Go to product detail page or quick view options etc.).

Create a custom HTML Tag with the above trigger and variable and put the below code in the tag.

Collecting and sending user purchase data

Set up a variable to capture the all the purchase detail, when the user makes a purchase. This could be from the purchase confirmation page etc.

Set up a trigger so that the tag would fire when the user make the purchase.

Create a custom HTML Tag with the above trigger to send the information to endpoint POST /v1/purchase or POST /v1/purchase/client

Collecting and sending user rating data

Setup variables to capture the product name/title/ID and the rating, when the user rates an item positively or negatively. We can also consider an item is positively rated when user adds the item to wishlist.

Set up a trigger for the tag to fire when the user rates a product.

Create a custom HTML Tag with the above trigger and variables to send the information to the endpoint POST /v1/rating or POST /v1/rating/client

Collecting and sending user data

Setup variable to capture the user information.

For this, the trigger could be setup up when the user logs or update their information.

Create a custom HTML Tag with the above trigger and variables to send the information to the endpoint POST /v1/user or POST /v1/user/client

Server to server integration

For server to server integration, you will need to generate Project key and API as mentioned in the credentials section.

User information

The below request path, takes user information, such as name, age, gender, address and saves them in the gaip database.

POST /v1/user

Here is an example request body

json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "user_info": { "address": "string", "gender": "integer --> 1 for male or 2 for female or 3 for others", "age": 25, "user_type": [ { "key_name1": "value1_value2", "separator": "_" }, { "key_name2": "value3" } ] } } You can find the sample code for implementation here

Product browse

You can use the below endpoint to capture user browsing information and save them in GAIP database js POST /v1/items/browse

It takes user_id and item_id as required parameters.

Here is an example value of the request body JSON { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_id": "1000764491" }

You can find sample code here

Product purchase

You can use the below endpoint to capture user's product purchase information and save them in GAIP database

POST /v1/purchase It takes user_id, item_list which includes item_id, price, quantity for a specific item as required parameters.

Here is an example request body json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_list": [ { "item_id": "1000757666", "price": 5000, "quantity": 1 }, { "item_id": "1000764491", "price": 400, "quantity": 7 } ] }

You a find sample code for this implementation here

Product rating

You can use the below endpoint to capture user's product rating information and save them in GAIP database

POST /v1/rating

It takes user_id, item_id, and rating for the specific item as required parameters.

Here is a sample request body

json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_id": "1000764491", "rating": "1" }

You can find the sample code for this implementation here

If you want to save your data with bulk upload you can use above-mentioned endpoint.

Client to server integration

For client to server integration, you will need to generate client key as described in the Credentials sections. Once the client key is ready, you can directly send the request from your client side to GAIP, using the client key provided.

Note that while generating client key, you can add whitelisted domains, which whitelists the request origin. This is recommended to enhance security.

The rest of the implementation method is same as server to server integration.

Import user behavior data

Similar to data integration, all 4 kinds of user information (browse, purchase, rating, user) can be bulk uploaded. This could be useful if you already have this information from the past and want to import it into GAIP.

To import user behavior and user information in bulk, first you need to create mapper to match the keys with GAIP.

To create the mapper, the endpoints with the example request bodies can be found here in the gigalogy recommender page. You can also find the sample codes for mapper creation here in the API documentation page

Next we will use the below 4 endpoints to upload each category of data in bulk

Request path for product browsing history: POST /v1/items/browse/save
Request path for purchase history: POST /v1/items/purchase/save
Request path for rating history: POST /v1/items/rating/save
Request path to upload user information in bulk: POST /v1/users/save

You can find these endpoints with the example request bodies here. The sample code can be found here in the API documentation page

Previous
Environment setup
Next
Training your data
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/03_Data_integration_%26_user_behavior_collection/#collecting-and-sending-user-rating-data
Skip to content
Gigalogy Tutorial
Integration of Catalogue information and user behavior data
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Catalogue information integration.
Uploading data using a file
Fetch item from external API
Search Items in GAIP after import
User behavior data collection integration
Comparison of each approach.
Google Tag manager
Generating user ID
Collecting and sending user browsing data
Collecting and sending user purchase data
Collecting and sending user rating data
Collecting and sending user data
Server to server integration
User information
Product browse
Product purchase
Product rating
Client to server integration
Import user behavior data
Integration of Catalogue information and user behavior data

This tutorial will cover how to integrate your catalogue information into GAIP and how to set up user behavior tracking and integrate with GAIP.

Catalogue information integration.

Prerequisite: Mapping creation and the index creation is done.

Info

This step can also be done from our platform (GAIP). Refer here for detail.

There are two ways to import your catalogue information into GAIP.

Upload you catalogue information as a CSV or JSON file using endpoint POST /v1/item/save.
Fetch data from your API (or any external API) using endpoint POST /v1/item/save/remote.
Uploading data using a file

For uploading the data using a CSV file or JSON file, please use the POST /v1/item/save endpoint. Simply upload the file and confirm the server response is success. Confirm the task was successful using the GET/v1/tasks/{task_id} endpoint.

Info

This will throw an error and task will fail, if the keys during the Mapping creation step does not match with the keys in the file, OR if the indices were not created succesfully.

Fetch item from external API

To fetch data from external API, use the POST /v1/item/save/remote endpoint. The key and value types and an example request body for the endpoint can be found here in our sandbox.

After hitting either of the endpoint above to import your data into GAIP, you will get a task ID in the response. Use this task ID and hit the /v1/tasks/{task_id} endpoint to confirm the operation was successful. In case it fails, you can also find the details there. For API documentation, please refer <>

Search Items in GAIP after import

You can search items by passing list of item ids fromPOST /v1/items/search endpoint. This endpoint will return searched items with item details.

It is recommended to use this endpoint to confirm that the item catalogue is successfully imported into your project.

User behavior data collection integration

GAIP can collect different user behavior related information to optimize the recommendation for the user. Types of data collected are listed below with their endpoints.

Data type	Endpoint
Product browsing: When user browse products.	/v1/items/browse or /v1/items/browse/client
Product Purchase: When user purchase a product with its quantity.	/v1/items/purchase or /v1/items/purchase/client
Rating: When a user rates a product.	/v1/items/rating or /v1/items/rating/client
User: User information such as age, gender and other customized attributes depending on your website.	/v1/users or /v1/users/client

You will find these endpoints listed in our Sandbox under section "User Data Collection". Please check the required parameters, value types and example request bodies for all the endpoints there.

There are 3 ways to integrate user behavior data collection with GAIP

Google Tag Manager
Server to server integration
Client to server integration

You can also bulk upload user behavior data from the past. For that, please refer to Import user behavior data section.

Comparison of each approach.
Approach	GTM	Server to Server	Client to Server
Description	Use Google Tag Manager (GTM) to collect data (User behavior) from your website and send it to GAIP via endpoint.	The data is captured in the backend server of your application and then sent to GAIP via endpoint.	The data is directly sent from your front end (Client side) to GAIP via endpoint.
Pros	Easy to implement, Minimum coding required, Flexible configuration	More secure, Data integrity, Controlled environment	Real-time data, Less server dependency
Cons	Limited customization, need to have basic knowledge about GTM, Dependency on third-party service, Might not work for certain browsers and plugins like AdBlockers	More complex to set up, potential latency, maintenance required	Less secure, Potential for inconsistent data, dependency on client-side behavior

Below we will show the implementation of each approach.

BE ADVISED: The following is a general guideline, and it may vary across different websites, contingent upon the specific implementation of your website.

Google Tag manager

Prerequisite: Your website must have GTM setup. If you do not have GTM setup, you can easily do the setup by following the guidelines here.

If you are not familiar with basic GTM concepts, such as Tags, Triggers and Variables, please familiarize yourself first with these concepts before proceeding with this approach. You can find more resources related to this here.

Generating user ID

In our sandbox, Notice that in the user data collection endpoints, every endpoint has a parameter called user_id, and member_id. These are vital to identify each user so that you can personalize their experience. user_id is generated by GAIP for each user of your site. The endpoint to generate and user_id is GET /v1/users/generate/id. You can generate the user_id using GTM using below code. This code can be used with every Tag, which checks if there is a user_id and creates one if there is none.

js // Function to get or generate 'gaip_user_id' using a function expression var getGaipUser = function() { return new Promise(function(resolve, reject) { if (gaipUser !== null) { resolve(gaipUser); } else { var idHeaders = new Headers(); idHeaders.append("project-key", "{{ YOUR_PROJECT_KEY_HERE }}"); idHeaders.append("api-key", "{{ YOUR_API_KEY_HERE }}"); fetch("None/v1/users/generate/id", { headers: idHeaders }) .then(function(response) { return response.json(); }) .then(function(data) { localStorage.setItem('gaip_user_id', data.detail.response); resolve(data.detail.response); }) .catch(reject); } }); };

member_id can be set by you depending on how your site identifies unique users such as user ID, phone number, email address etc.

Collecting and sending user browsing data

Set up a variable to capture the product name/title/ID, when the user goes a product details page or clicks on a product to enlarge it or open a pop-up etc.

Set up a trigger so that the tag would fire when the user goes browses an item (Go to product detail page or quick view options etc.).

Create a custom HTML Tag with the above trigger and variable and put the below code in the tag.

Collecting and sending user purchase data

Set up a variable to capture the all the purchase detail, when the user makes a purchase. This could be from the purchase confirmation page etc.

Set up a trigger so that the tag would fire when the user make the purchase.

Create a custom HTML Tag with the above trigger to send the information to endpoint POST /v1/purchase or POST /v1/purchase/client

Collecting and sending user rating data

Setup variables to capture the product name/title/ID and the rating, when the user rates an item positively or negatively. We can also consider an item is positively rated when user adds the item to wishlist.

Set up a trigger for the tag to fire when the user rates a product.

Create a custom HTML Tag with the above trigger and variables to send the information to the endpoint POST /v1/rating or POST /v1/rating/client

Collecting and sending user data

Setup variable to capture the user information.

For this, the trigger could be setup up when the user logs or update their information.

Create a custom HTML Tag with the above trigger and variables to send the information to the endpoint POST /v1/user or POST /v1/user/client

Server to server integration

For server to server integration, you will need to generate Project key and API as mentioned in the credentials section.

User information

The below request path, takes user information, such as name, age, gender, address and saves them in the gaip database.

POST /v1/user

Here is an example request body

json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "user_info": { "address": "string", "gender": "integer --> 1 for male or 2 for female or 3 for others", "age": 25, "user_type": [ { "key_name1": "value1_value2", "separator": "_" }, { "key_name2": "value3" } ] } } You can find the sample code for implementation here

Product browse

You can use the below endpoint to capture user browsing information and save them in GAIP database js POST /v1/items/browse

It takes user_id and item_id as required parameters.

Here is an example value of the request body JSON { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_id": "1000764491" }

You can find sample code here

Product purchase

You can use the below endpoint to capture user's product purchase information and save them in GAIP database

POST /v1/purchase It takes user_id, item_list which includes item_id, price, quantity for a specific item as required parameters.

Here is an example request body json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_list": [ { "item_id": "1000757666", "price": 5000, "quantity": 1 }, { "item_id": "1000764491", "price": 400, "quantity": 7 } ] }

You a find sample code for this implementation here

Product rating

You can use the below endpoint to capture user's product rating information and save them in GAIP database

POST /v1/rating

It takes user_id, item_id, and rating for the specific item as required parameters.

Here is a sample request body

json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_id": "1000764491", "rating": "1" }

You can find the sample code for this implementation here

If you want to save your data with bulk upload you can use above-mentioned endpoint.

Client to server integration

For client to server integration, you will need to generate client key as described in the Credentials sections. Once the client key is ready, you can directly send the request from your client side to GAIP, using the client key provided.

Note that while generating client key, you can add whitelisted domains, which whitelists the request origin. This is recommended to enhance security.

The rest of the implementation method is same as server to server integration.

Import user behavior data

Similar to data integration, all 4 kinds of user information (browse, purchase, rating, user) can be bulk uploaded. This could be useful if you already have this information from the past and want to import it into GAIP.

To import user behavior and user information in bulk, first you need to create mapper to match the keys with GAIP.

To create the mapper, the endpoints with the example request bodies can be found here in the gigalogy recommender page. You can also find the sample codes for mapper creation here in the API documentation page

Next we will use the below 4 endpoints to upload each category of data in bulk

Request path for product browsing history: POST /v1/items/browse/save
Request path for purchase history: POST /v1/items/purchase/save
Request path for rating history: POST /v1/items/rating/save
Request path to upload user information in bulk: POST /v1/users/save

You can find these endpoints with the example request bodies here. The sample code can be found here in the API documentation page

Previous
Environment setup
Next
Training your data
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/03_Data_integration_%26_user_behavior_collection/#collecting-and-sending-user-data
Skip to content
Gigalogy Tutorial
Integration of Catalogue information and user behavior data
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Catalogue information integration.
Uploading data using a file
Fetch item from external API
Search Items in GAIP after import
User behavior data collection integration
Comparison of each approach.
Google Tag manager
Generating user ID
Collecting and sending user browsing data
Collecting and sending user purchase data
Collecting and sending user rating data
Collecting and sending user data
Server to server integration
User information
Product browse
Product purchase
Product rating
Client to server integration
Import user behavior data
Integration of Catalogue information and user behavior data

This tutorial will cover how to integrate your catalogue information into GAIP and how to set up user behavior tracking and integrate with GAIP.

Catalogue information integration.

Prerequisite: Mapping creation and the index creation is done.

Info

This step can also be done from our platform (GAIP). Refer here for detail.

There are two ways to import your catalogue information into GAIP.

Upload you catalogue information as a CSV or JSON file using endpoint POST /v1/item/save.
Fetch data from your API (or any external API) using endpoint POST /v1/item/save/remote.
Uploading data using a file

For uploading the data using a CSV file or JSON file, please use the POST /v1/item/save endpoint. Simply upload the file and confirm the server response is success. Confirm the task was successful using the GET/v1/tasks/{task_id} endpoint.

Info

This will throw an error and task will fail, if the keys during the Mapping creation step does not match with the keys in the file, OR if the indices were not created succesfully.

Fetch item from external API

To fetch data from external API, use the POST /v1/item/save/remote endpoint. The key and value types and an example request body for the endpoint can be found here in our sandbox.

After hitting either of the endpoint above to import your data into GAIP, you will get a task ID in the response. Use this task ID and hit the /v1/tasks/{task_id} endpoint to confirm the operation was successful. In case it fails, you can also find the details there. For API documentation, please refer <>

Search Items in GAIP after import

You can search items by passing list of item ids fromPOST /v1/items/search endpoint. This endpoint will return searched items with item details.

It is recommended to use this endpoint to confirm that the item catalogue is successfully imported into your project.

User behavior data collection integration

GAIP can collect different user behavior related information to optimize the recommendation for the user. Types of data collected are listed below with their endpoints.

Data type	Endpoint
Product browsing: When user browse products.	/v1/items/browse or /v1/items/browse/client
Product Purchase: When user purchase a product with its quantity.	/v1/items/purchase or /v1/items/purchase/client
Rating: When a user rates a product.	/v1/items/rating or /v1/items/rating/client
User: User information such as age, gender and other customized attributes depending on your website.	/v1/users or /v1/users/client

You will find these endpoints listed in our Sandbox under section "User Data Collection". Please check the required parameters, value types and example request bodies for all the endpoints there.

There are 3 ways to integrate user behavior data collection with GAIP

Google Tag Manager
Server to server integration
Client to server integration

You can also bulk upload user behavior data from the past. For that, please refer to Import user behavior data section.

Comparison of each approach.
Approach	GTM	Server to Server	Client to Server
Description	Use Google Tag Manager (GTM) to collect data (User behavior) from your website and send it to GAIP via endpoint.	The data is captured in the backend server of your application and then sent to GAIP via endpoint.	The data is directly sent from your front end (Client side) to GAIP via endpoint.
Pros	Easy to implement, Minimum coding required, Flexible configuration	More secure, Data integrity, Controlled environment	Real-time data, Less server dependency
Cons	Limited customization, need to have basic knowledge about GTM, Dependency on third-party service, Might not work for certain browsers and plugins like AdBlockers	More complex to set up, potential latency, maintenance required	Less secure, Potential for inconsistent data, dependency on client-side behavior

Below we will show the implementation of each approach.

BE ADVISED: The following is a general guideline, and it may vary across different websites, contingent upon the specific implementation of your website.

Google Tag manager

Prerequisite: Your website must have GTM setup. If you do not have GTM setup, you can easily do the setup by following the guidelines here.

If you are not familiar with basic GTM concepts, such as Tags, Triggers and Variables, please familiarize yourself first with these concepts before proceeding with this approach. You can find more resources related to this here.

Generating user ID

In our sandbox, Notice that in the user data collection endpoints, every endpoint has a parameter called user_id, and member_id. These are vital to identify each user so that you can personalize their experience. user_id is generated by GAIP for each user of your site. The endpoint to generate and user_id is GET /v1/users/generate/id. You can generate the user_id using GTM using below code. This code can be used with every Tag, which checks if there is a user_id and creates one if there is none.

js // Function to get or generate 'gaip_user_id' using a function expression var getGaipUser = function() { return new Promise(function(resolve, reject) { if (gaipUser !== null) { resolve(gaipUser); } else { var idHeaders = new Headers(); idHeaders.append("project-key", "{{ YOUR_PROJECT_KEY_HERE }}"); idHeaders.append("api-key", "{{ YOUR_API_KEY_HERE }}"); fetch("None/v1/users/generate/id", { headers: idHeaders }) .then(function(response) { return response.json(); }) .then(function(data) { localStorage.setItem('gaip_user_id', data.detail.response); resolve(data.detail.response); }) .catch(reject); } }); };

member_id can be set by you depending on how your site identifies unique users such as user ID, phone number, email address etc.

Collecting and sending user browsing data

Set up a variable to capture the product name/title/ID, when the user goes a product details page or clicks on a product to enlarge it or open a pop-up etc.

Set up a trigger so that the tag would fire when the user goes browses an item (Go to product detail page or quick view options etc.).

Create a custom HTML Tag with the above trigger and variable and put the below code in the tag.

Collecting and sending user purchase data

Set up a variable to capture the all the purchase detail, when the user makes a purchase. This could be from the purchase confirmation page etc.

Set up a trigger so that the tag would fire when the user make the purchase.

Create a custom HTML Tag with the above trigger to send the information to endpoint POST /v1/purchase or POST /v1/purchase/client

Collecting and sending user rating data

Setup variables to capture the product name/title/ID and the rating, when the user rates an item positively or negatively. We can also consider an item is positively rated when user adds the item to wishlist.

Set up a trigger for the tag to fire when the user rates a product.

Create a custom HTML Tag with the above trigger and variables to send the information to the endpoint POST /v1/rating or POST /v1/rating/client

Collecting and sending user data

Setup variable to capture the user information.

For this, the trigger could be setup up when the user logs or update their information.

Create a custom HTML Tag with the above trigger and variables to send the information to the endpoint POST /v1/user or POST /v1/user/client

Server to server integration

For server to server integration, you will need to generate Project key and API as mentioned in the credentials section.

User information

The below request path, takes user information, such as name, age, gender, address and saves them in the gaip database.

POST /v1/user

Here is an example request body

json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "user_info": { "address": "string", "gender": "integer --> 1 for male or 2 for female or 3 for others", "age": 25, "user_type": [ { "key_name1": "value1_value2", "separator": "_" }, { "key_name2": "value3" } ] } } You can find the sample code for implementation here

Product browse

You can use the below endpoint to capture user browsing information and save them in GAIP database js POST /v1/items/browse

It takes user_id and item_id as required parameters.

Here is an example value of the request body JSON { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_id": "1000764491" }

You can find sample code here

Product purchase

You can use the below endpoint to capture user's product purchase information and save them in GAIP database

POST /v1/purchase It takes user_id, item_list which includes item_id, price, quantity for a specific item as required parameters.

Here is an example request body json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_list": [ { "item_id": "1000757666", "price": 5000, "quantity": 1 }, { "item_id": "1000764491", "price": 400, "quantity": 7 } ] }

You a find sample code for this implementation here

Product rating

You can use the below endpoint to capture user's product rating information and save them in GAIP database

POST /v1/rating

It takes user_id, item_id, and rating for the specific item as required parameters.

Here is a sample request body

json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_id": "1000764491", "rating": "1" }

You can find the sample code for this implementation here

If you want to save your data with bulk upload you can use above-mentioned endpoint.

Client to server integration

For client to server integration, you will need to generate client key as described in the Credentials sections. Once the client key is ready, you can directly send the request from your client side to GAIP, using the client key provided.

Note that while generating client key, you can add whitelisted domains, which whitelists the request origin. This is recommended to enhance security.

The rest of the implementation method is same as server to server integration.

Import user behavior data

Similar to data integration, all 4 kinds of user information (browse, purchase, rating, user) can be bulk uploaded. This could be useful if you already have this information from the past and want to import it into GAIP.

To import user behavior and user information in bulk, first you need to create mapper to match the keys with GAIP.

To create the mapper, the endpoints with the example request bodies can be found here in the gigalogy recommender page. You can also find the sample codes for mapper creation here in the API documentation page

Next we will use the below 4 endpoints to upload each category of data in bulk

Request path for product browsing history: POST /v1/items/browse/save
Request path for purchase history: POST /v1/items/purchase/save
Request path for rating history: POST /v1/items/rating/save
Request path to upload user information in bulk: POST /v1/users/save

You can find these endpoints with the example request bodies here. The sample code can be found here in the API documentation page

Previous
Environment setup
Next
Training your data
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/03_Data_integration_%26_user_behavior_collection/#server-to-server-integration
Skip to content
Gigalogy Tutorial
Integration of Catalogue information and user behavior data
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Catalogue information integration.
Uploading data using a file
Fetch item from external API
Search Items in GAIP after import
User behavior data collection integration
Comparison of each approach.
Google Tag manager
Generating user ID
Collecting and sending user browsing data
Collecting and sending user purchase data
Collecting and sending user rating data
Collecting and sending user data
Server to server integration
User information
Product browse
Product purchase
Product rating
Client to server integration
Import user behavior data
Integration of Catalogue information and user behavior data

This tutorial will cover how to integrate your catalogue information into GAIP and how to set up user behavior tracking and integrate with GAIP.

Catalogue information integration.

Prerequisite: Mapping creation and the index creation is done.

Info

This step can also be done from our platform (GAIP). Refer here for detail.

There are two ways to import your catalogue information into GAIP.

Upload you catalogue information as a CSV or JSON file using endpoint POST /v1/item/save.
Fetch data from your API (or any external API) using endpoint POST /v1/item/save/remote.
Uploading data using a file

For uploading the data using a CSV file or JSON file, please use the POST /v1/item/save endpoint. Simply upload the file and confirm the server response is success. Confirm the task was successful using the GET/v1/tasks/{task_id} endpoint.

Info

This will throw an error and task will fail, if the keys during the Mapping creation step does not match with the keys in the file, OR if the indices were not created succesfully.

Fetch item from external API

To fetch data from external API, use the POST /v1/item/save/remote endpoint. The key and value types and an example request body for the endpoint can be found here in our sandbox.

After hitting either of the endpoint above to import your data into GAIP, you will get a task ID in the response. Use this task ID and hit the /v1/tasks/{task_id} endpoint to confirm the operation was successful. In case it fails, you can also find the details there. For API documentation, please refer <>

Search Items in GAIP after import

You can search items by passing list of item ids fromPOST /v1/items/search endpoint. This endpoint will return searched items with item details.

It is recommended to use this endpoint to confirm that the item catalogue is successfully imported into your project.

User behavior data collection integration

GAIP can collect different user behavior related information to optimize the recommendation for the user. Types of data collected are listed below with their endpoints.

Data type	Endpoint
Product browsing: When user browse products.	/v1/items/browse or /v1/items/browse/client
Product Purchase: When user purchase a product with its quantity.	/v1/items/purchase or /v1/items/purchase/client
Rating: When a user rates a product.	/v1/items/rating or /v1/items/rating/client
User: User information such as age, gender and other customized attributes depending on your website.	/v1/users or /v1/users/client

You will find these endpoints listed in our Sandbox under section "User Data Collection". Please check the required parameters, value types and example request bodies for all the endpoints there.

There are 3 ways to integrate user behavior data collection with GAIP

Google Tag Manager
Server to server integration
Client to server integration

You can also bulk upload user behavior data from the past. For that, please refer to Import user behavior data section.

Comparison of each approach.
Approach	GTM	Server to Server	Client to Server
Description	Use Google Tag Manager (GTM) to collect data (User behavior) from your website and send it to GAIP via endpoint.	The data is captured in the backend server of your application and then sent to GAIP via endpoint.	The data is directly sent from your front end (Client side) to GAIP via endpoint.
Pros	Easy to implement, Minimum coding required, Flexible configuration	More secure, Data integrity, Controlled environment	Real-time data, Less server dependency
Cons	Limited customization, need to have basic knowledge about GTM, Dependency on third-party service, Might not work for certain browsers and plugins like AdBlockers	More complex to set up, potential latency, maintenance required	Less secure, Potential for inconsistent data, dependency on client-side behavior

Below we will show the implementation of each approach.

BE ADVISED: The following is a general guideline, and it may vary across different websites, contingent upon the specific implementation of your website.

Google Tag manager

Prerequisite: Your website must have GTM setup. If you do not have GTM setup, you can easily do the setup by following the guidelines here.

If you are not familiar with basic GTM concepts, such as Tags, Triggers and Variables, please familiarize yourself first with these concepts before proceeding with this approach. You can find more resources related to this here.

Generating user ID

In our sandbox, Notice that in the user data collection endpoints, every endpoint has a parameter called user_id, and member_id. These are vital to identify each user so that you can personalize their experience. user_id is generated by GAIP for each user of your site. The endpoint to generate and user_id is GET /v1/users/generate/id. You can generate the user_id using GTM using below code. This code can be used with every Tag, which checks if there is a user_id and creates one if there is none.

js // Function to get or generate 'gaip_user_id' using a function expression var getGaipUser = function() { return new Promise(function(resolve, reject) { if (gaipUser !== null) { resolve(gaipUser); } else { var idHeaders = new Headers(); idHeaders.append("project-key", "{{ YOUR_PROJECT_KEY_HERE }}"); idHeaders.append("api-key", "{{ YOUR_API_KEY_HERE }}"); fetch("None/v1/users/generate/id", { headers: idHeaders }) .then(function(response) { return response.json(); }) .then(function(data) { localStorage.setItem('gaip_user_id', data.detail.response); resolve(data.detail.response); }) .catch(reject); } }); };

member_id can be set by you depending on how your site identifies unique users such as user ID, phone number, email address etc.

Collecting and sending user browsing data

Set up a variable to capture the product name/title/ID, when the user goes a product details page or clicks on a product to enlarge it or open a pop-up etc.

Set up a trigger so that the tag would fire when the user goes browses an item (Go to product detail page or quick view options etc.).

Create a custom HTML Tag with the above trigger and variable and put the below code in the tag.

Collecting and sending user purchase data

Set up a variable to capture the all the purchase detail, when the user makes a purchase. This could be from the purchase confirmation page etc.

Set up a trigger so that the tag would fire when the user make the purchase.

Create a custom HTML Tag with the above trigger to send the information to endpoint POST /v1/purchase or POST /v1/purchase/client

Collecting and sending user rating data

Setup variables to capture the product name/title/ID and the rating, when the user rates an item positively or negatively. We can also consider an item is positively rated when user adds the item to wishlist.

Set up a trigger for the tag to fire when the user rates a product.

Create a custom HTML Tag with the above trigger and variables to send the information to the endpoint POST /v1/rating or POST /v1/rating/client

Collecting and sending user data

Setup variable to capture the user information.

For this, the trigger could be setup up when the user logs or update their information.

Create a custom HTML Tag with the above trigger and variables to send the information to the endpoint POST /v1/user or POST /v1/user/client

Server to server integration

For server to server integration, you will need to generate Project key and API as mentioned in the credentials section.

User information

The below request path, takes user information, such as name, age, gender, address and saves them in the gaip database.

POST /v1/user

Here is an example request body

json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "user_info": { "address": "string", "gender": "integer --> 1 for male or 2 for female or 3 for others", "age": 25, "user_type": [ { "key_name1": "value1_value2", "separator": "_" }, { "key_name2": "value3" } ] } } You can find the sample code for implementation here

Product browse

You can use the below endpoint to capture user browsing information and save them in GAIP database js POST /v1/items/browse

It takes user_id and item_id as required parameters.

Here is an example value of the request body JSON { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_id": "1000764491" }

You can find sample code here

Product purchase

You can use the below endpoint to capture user's product purchase information and save them in GAIP database

POST /v1/purchase It takes user_id, item_list which includes item_id, price, quantity for a specific item as required parameters.

Here is an example request body json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_list": [ { "item_id": "1000757666", "price": 5000, "quantity": 1 }, { "item_id": "1000764491", "price": 400, "quantity": 7 } ] }

You a find sample code for this implementation here

Product rating

You can use the below endpoint to capture user's product rating information and save them in GAIP database

POST /v1/rating

It takes user_id, item_id, and rating for the specific item as required parameters.

Here is a sample request body

json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_id": "1000764491", "rating": "1" }

You can find the sample code for this implementation here

If you want to save your data with bulk upload you can use above-mentioned endpoint.

Client to server integration

For client to server integration, you will need to generate client key as described in the Credentials sections. Once the client key is ready, you can directly send the request from your client side to GAIP, using the client key provided.

Note that while generating client key, you can add whitelisted domains, which whitelists the request origin. This is recommended to enhance security.

The rest of the implementation method is same as server to server integration.

Import user behavior data

Similar to data integration, all 4 kinds of user information (browse, purchase, rating, user) can be bulk uploaded. This could be useful if you already have this information from the past and want to import it into GAIP.

To import user behavior and user information in bulk, first you need to create mapper to match the keys with GAIP.

To create the mapper, the endpoints with the example request bodies can be found here in the gigalogy recommender page. You can also find the sample codes for mapper creation here in the API documentation page

Next we will use the below 4 endpoints to upload each category of data in bulk

Request path for product browsing history: POST /v1/items/browse/save
Request path for purchase history: POST /v1/items/purchase/save
Request path for rating history: POST /v1/items/rating/save
Request path to upload user information in bulk: POST /v1/users/save

You can find these endpoints with the example request bodies here. The sample code can be found here in the API documentation page

Previous
Environment setup
Next
Training your data
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/03_Data_integration_%26_user_behavior_collection/#user-information
Skip to content
Gigalogy Tutorial
Integration of Catalogue information and user behavior data
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Catalogue information integration.
Uploading data using a file
Fetch item from external API
Search Items in GAIP after import
User behavior data collection integration
Comparison of each approach.
Google Tag manager
Generating user ID
Collecting and sending user browsing data
Collecting and sending user purchase data
Collecting and sending user rating data
Collecting and sending user data
Server to server integration
User information
Product browse
Product purchase
Product rating
Client to server integration
Import user behavior data
Integration of Catalogue information and user behavior data

This tutorial will cover how to integrate your catalogue information into GAIP and how to set up user behavior tracking and integrate with GAIP.

Catalogue information integration.

Prerequisite: Mapping creation and the index creation is done.

Info

This step can also be done from our platform (GAIP). Refer here for detail.

There are two ways to import your catalogue information into GAIP.

Upload you catalogue information as a CSV or JSON file using endpoint POST /v1/item/save.
Fetch data from your API (or any external API) using endpoint POST /v1/item/save/remote.
Uploading data using a file

For uploading the data using a CSV file or JSON file, please use the POST /v1/item/save endpoint. Simply upload the file and confirm the server response is success. Confirm the task was successful using the GET/v1/tasks/{task_id} endpoint.

Info

This will throw an error and task will fail, if the keys during the Mapping creation step does not match with the keys in the file, OR if the indices were not created succesfully.

Fetch item from external API

To fetch data from external API, use the POST /v1/item/save/remote endpoint. The key and value types and an example request body for the endpoint can be found here in our sandbox.

After hitting either of the endpoint above to import your data into GAIP, you will get a task ID in the response. Use this task ID and hit the /v1/tasks/{task_id} endpoint to confirm the operation was successful. In case it fails, you can also find the details there. For API documentation, please refer <>

Search Items in GAIP after import

You can search items by passing list of item ids fromPOST /v1/items/search endpoint. This endpoint will return searched items with item details.

It is recommended to use this endpoint to confirm that the item catalogue is successfully imported into your project.

User behavior data collection integration

GAIP can collect different user behavior related information to optimize the recommendation for the user. Types of data collected are listed below with their endpoints.

Data type	Endpoint
Product browsing: When user browse products.	/v1/items/browse or /v1/items/browse/client
Product Purchase: When user purchase a product with its quantity.	/v1/items/purchase or /v1/items/purchase/client
Rating: When a user rates a product.	/v1/items/rating or /v1/items/rating/client
User: User information such as age, gender and other customized attributes depending on your website.	/v1/users or /v1/users/client

You will find these endpoints listed in our Sandbox under section "User Data Collection". Please check the required parameters, value types and example request bodies for all the endpoints there.

There are 3 ways to integrate user behavior data collection with GAIP

Google Tag Manager
Server to server integration
Client to server integration

You can also bulk upload user behavior data from the past. For that, please refer to Import user behavior data section.

Comparison of each approach.
Approach	GTM	Server to Server	Client to Server
Description	Use Google Tag Manager (GTM) to collect data (User behavior) from your website and send it to GAIP via endpoint.	The data is captured in the backend server of your application and then sent to GAIP via endpoint.	The data is directly sent from your front end (Client side) to GAIP via endpoint.
Pros	Easy to implement, Minimum coding required, Flexible configuration	More secure, Data integrity, Controlled environment	Real-time data, Less server dependency
Cons	Limited customization, need to have basic knowledge about GTM, Dependency on third-party service, Might not work for certain browsers and plugins like AdBlockers	More complex to set up, potential latency, maintenance required	Less secure, Potential for inconsistent data, dependency on client-side behavior

Below we will show the implementation of each approach.

BE ADVISED: The following is a general guideline, and it may vary across different websites, contingent upon the specific implementation of your website.

Google Tag manager

Prerequisite: Your website must have GTM setup. If you do not have GTM setup, you can easily do the setup by following the guidelines here.

If you are not familiar with basic GTM concepts, such as Tags, Triggers and Variables, please familiarize yourself first with these concepts before proceeding with this approach. You can find more resources related to this here.

Generating user ID

In our sandbox, Notice that in the user data collection endpoints, every endpoint has a parameter called user_id, and member_id. These are vital to identify each user so that you can personalize their experience. user_id is generated by GAIP for each user of your site. The endpoint to generate and user_id is GET /v1/users/generate/id. You can generate the user_id using GTM using below code. This code can be used with every Tag, which checks if there is a user_id and creates one if there is none.

js // Function to get or generate 'gaip_user_id' using a function expression var getGaipUser = function() { return new Promise(function(resolve, reject) { if (gaipUser !== null) { resolve(gaipUser); } else { var idHeaders = new Headers(); idHeaders.append("project-key", "{{ YOUR_PROJECT_KEY_HERE }}"); idHeaders.append("api-key", "{{ YOUR_API_KEY_HERE }}"); fetch("None/v1/users/generate/id", { headers: idHeaders }) .then(function(response) { return response.json(); }) .then(function(data) { localStorage.setItem('gaip_user_id', data.detail.response); resolve(data.detail.response); }) .catch(reject); } }); };

member_id can be set by you depending on how your site identifies unique users such as user ID, phone number, email address etc.

Collecting and sending user browsing data

Set up a variable to capture the product name/title/ID, when the user goes a product details page or clicks on a product to enlarge it or open a pop-up etc.

Set up a trigger so that the tag would fire when the user goes browses an item (Go to product detail page or quick view options etc.).

Create a custom HTML Tag with the above trigger and variable and put the below code in the tag.

Collecting and sending user purchase data

Set up a variable to capture the all the purchase detail, when the user makes a purchase. This could be from the purchase confirmation page etc.

Set up a trigger so that the tag would fire when the user make the purchase.

Create a custom HTML Tag with the above trigger to send the information to endpoint POST /v1/purchase or POST /v1/purchase/client

Collecting and sending user rating data

Setup variables to capture the product name/title/ID and the rating, when the user rates an item positively or negatively. We can also consider an item is positively rated when user adds the item to wishlist.

Set up a trigger for the tag to fire when the user rates a product.

Create a custom HTML Tag with the above trigger and variables to send the information to the endpoint POST /v1/rating or POST /v1/rating/client

Collecting and sending user data

Setup variable to capture the user information.

For this, the trigger could be setup up when the user logs or update their information.

Create a custom HTML Tag with the above trigger and variables to send the information to the endpoint POST /v1/user or POST /v1/user/client

Server to server integration

For server to server integration, you will need to generate Project key and API as mentioned in the credentials section.

User information

The below request path, takes user information, such as name, age, gender, address and saves them in the gaip database.

POST /v1/user

Here is an example request body

json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "user_info": { "address": "string", "gender": "integer --> 1 for male or 2 for female or 3 for others", "age": 25, "user_type": [ { "key_name1": "value1_value2", "separator": "_" }, { "key_name2": "value3" } ] } } You can find the sample code for implementation here

Product browse

You can use the below endpoint to capture user browsing information and save them in GAIP database js POST /v1/items/browse

It takes user_id and item_id as required parameters.

Here is an example value of the request body JSON { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_id": "1000764491" }

You can find sample code here

Product purchase

You can use the below endpoint to capture user's product purchase information and save them in GAIP database

POST /v1/purchase It takes user_id, item_list which includes item_id, price, quantity for a specific item as required parameters.

Here is an example request body json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_list": [ { "item_id": "1000757666", "price": 5000, "quantity": 1 }, { "item_id": "1000764491", "price": 400, "quantity": 7 } ] }

You a find sample code for this implementation here

Product rating

You can use the below endpoint to capture user's product rating information and save them in GAIP database

POST /v1/rating

It takes user_id, item_id, and rating for the specific item as required parameters.

Here is a sample request body

json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_id": "1000764491", "rating": "1" }

You can find the sample code for this implementation here

If you want to save your data with bulk upload you can use above-mentioned endpoint.

Client to server integration

For client to server integration, you will need to generate client key as described in the Credentials sections. Once the client key is ready, you can directly send the request from your client side to GAIP, using the client key provided.

Note that while generating client key, you can add whitelisted domains, which whitelists the request origin. This is recommended to enhance security.

The rest of the implementation method is same as server to server integration.

Import user behavior data

Similar to data integration, all 4 kinds of user information (browse, purchase, rating, user) can be bulk uploaded. This could be useful if you already have this information from the past and want to import it into GAIP.

To import user behavior and user information in bulk, first you need to create mapper to match the keys with GAIP.

To create the mapper, the endpoints with the example request bodies can be found here in the gigalogy recommender page. You can also find the sample codes for mapper creation here in the API documentation page

Next we will use the below 4 endpoints to upload each category of data in bulk

Request path for product browsing history: POST /v1/items/browse/save
Request path for purchase history: POST /v1/items/purchase/save
Request path for rating history: POST /v1/items/rating/save
Request path to upload user information in bulk: POST /v1/users/save

You can find these endpoints with the example request bodies here. The sample code can be found here in the API documentation page

Previous
Environment setup
Next
Training your data
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/03_Data_integration_%26_user_behavior_collection/#product-browse
Skip to content
Gigalogy Tutorial
Integration of Catalogue information and user behavior data
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Catalogue information integration.
Uploading data using a file
Fetch item from external API
Search Items in GAIP after import
User behavior data collection integration
Comparison of each approach.
Google Tag manager
Generating user ID
Collecting and sending user browsing data
Collecting and sending user purchase data
Collecting and sending user rating data
Collecting and sending user data
Server to server integration
User information
Product browse
Product purchase
Product rating
Client to server integration
Import user behavior data
Integration of Catalogue information and user behavior data

This tutorial will cover how to integrate your catalogue information into GAIP and how to set up user behavior tracking and integrate with GAIP.

Catalogue information integration.

Prerequisite: Mapping creation and the index creation is done.

Info

This step can also be done from our platform (GAIP). Refer here for detail.

There are two ways to import your catalogue information into GAIP.

Upload you catalogue information as a CSV or JSON file using endpoint POST /v1/item/save.
Fetch data from your API (or any external API) using endpoint POST /v1/item/save/remote.
Uploading data using a file

For uploading the data using a CSV file or JSON file, please use the POST /v1/item/save endpoint. Simply upload the file and confirm the server response is success. Confirm the task was successful using the GET/v1/tasks/{task_id} endpoint.

Info

This will throw an error and task will fail, if the keys during the Mapping creation step does not match with the keys in the file, OR if the indices were not created succesfully.

Fetch item from external API

To fetch data from external API, use the POST /v1/item/save/remote endpoint. The key and value types and an example request body for the endpoint can be found here in our sandbox.

After hitting either of the endpoint above to import your data into GAIP, you will get a task ID in the response. Use this task ID and hit the /v1/tasks/{task_id} endpoint to confirm the operation was successful. In case it fails, you can also find the details there. For API documentation, please refer <>

Search Items in GAIP after import

You can search items by passing list of item ids fromPOST /v1/items/search endpoint. This endpoint will return searched items with item details.

It is recommended to use this endpoint to confirm that the item catalogue is successfully imported into your project.

User behavior data collection integration

GAIP can collect different user behavior related information to optimize the recommendation for the user. Types of data collected are listed below with their endpoints.

Data type	Endpoint
Product browsing: When user browse products.	/v1/items/browse or /v1/items/browse/client
Product Purchase: When user purchase a product with its quantity.	/v1/items/purchase or /v1/items/purchase/client
Rating: When a user rates a product.	/v1/items/rating or /v1/items/rating/client
User: User information such as age, gender and other customized attributes depending on your website.	/v1/users or /v1/users/client

You will find these endpoints listed in our Sandbox under section "User Data Collection". Please check the required parameters, value types and example request bodies for all the endpoints there.

There are 3 ways to integrate user behavior data collection with GAIP

Google Tag Manager
Server to server integration
Client to server integration

You can also bulk upload user behavior data from the past. For that, please refer to Import user behavior data section.

Comparison of each approach.
Approach	GTM	Server to Server	Client to Server
Description	Use Google Tag Manager (GTM) to collect data (User behavior) from your website and send it to GAIP via endpoint.	The data is captured in the backend server of your application and then sent to GAIP via endpoint.	The data is directly sent from your front end (Client side) to GAIP via endpoint.
Pros	Easy to implement, Minimum coding required, Flexible configuration	More secure, Data integrity, Controlled environment	Real-time data, Less server dependency
Cons	Limited customization, need to have basic knowledge about GTM, Dependency on third-party service, Might not work for certain browsers and plugins like AdBlockers	More complex to set up, potential latency, maintenance required	Less secure, Potential for inconsistent data, dependency on client-side behavior

Below we will show the implementation of each approach.

BE ADVISED: The following is a general guideline, and it may vary across different websites, contingent upon the specific implementation of your website.

Google Tag manager

Prerequisite: Your website must have GTM setup. If you do not have GTM setup, you can easily do the setup by following the guidelines here.

If you are not familiar with basic GTM concepts, such as Tags, Triggers and Variables, please familiarize yourself first with these concepts before proceeding with this approach. You can find more resources related to this here.

Generating user ID

In our sandbox, Notice that in the user data collection endpoints, every endpoint has a parameter called user_id, and member_id. These are vital to identify each user so that you can personalize their experience. user_id is generated by GAIP for each user of your site. The endpoint to generate and user_id is GET /v1/users/generate/id. You can generate the user_id using GTM using below code. This code can be used with every Tag, which checks if there is a user_id and creates one if there is none.

js // Function to get or generate 'gaip_user_id' using a function expression var getGaipUser = function() { return new Promise(function(resolve, reject) { if (gaipUser !== null) { resolve(gaipUser); } else { var idHeaders = new Headers(); idHeaders.append("project-key", "{{ YOUR_PROJECT_KEY_HERE }}"); idHeaders.append("api-key", "{{ YOUR_API_KEY_HERE }}"); fetch("None/v1/users/generate/id", { headers: idHeaders }) .then(function(response) { return response.json(); }) .then(function(data) { localStorage.setItem('gaip_user_id', data.detail.response); resolve(data.detail.response); }) .catch(reject); } }); };

member_id can be set by you depending on how your site identifies unique users such as user ID, phone number, email address etc.

Collecting and sending user browsing data

Set up a variable to capture the product name/title/ID, when the user goes a product details page or clicks on a product to enlarge it or open a pop-up etc.

Set up a trigger so that the tag would fire when the user goes browses an item (Go to product detail page or quick view options etc.).

Create a custom HTML Tag with the above trigger and variable and put the below code in the tag.

Collecting and sending user purchase data

Set up a variable to capture the all the purchase detail, when the user makes a purchase. This could be from the purchase confirmation page etc.

Set up a trigger so that the tag would fire when the user make the purchase.

Create a custom HTML Tag with the above trigger to send the information to endpoint POST /v1/purchase or POST /v1/purchase/client

Collecting and sending user rating data

Setup variables to capture the product name/title/ID and the rating, when the user rates an item positively or negatively. We can also consider an item is positively rated when user adds the item to wishlist.

Set up a trigger for the tag to fire when the user rates a product.

Create a custom HTML Tag with the above trigger and variables to send the information to the endpoint POST /v1/rating or POST /v1/rating/client

Collecting and sending user data

Setup variable to capture the user information.

For this, the trigger could be setup up when the user logs or update their information.

Create a custom HTML Tag with the above trigger and variables to send the information to the endpoint POST /v1/user or POST /v1/user/client

Server to server integration

For server to server integration, you will need to generate Project key and API as mentioned in the credentials section.

User information

The below request path, takes user information, such as name, age, gender, address and saves them in the gaip database.

POST /v1/user

Here is an example request body

json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "user_info": { "address": "string", "gender": "integer --> 1 for male or 2 for female or 3 for others", "age": 25, "user_type": [ { "key_name1": "value1_value2", "separator": "_" }, { "key_name2": "value3" } ] } } You can find the sample code for implementation here

Product browse

You can use the below endpoint to capture user browsing information and save them in GAIP database js POST /v1/items/browse

It takes user_id and item_id as required parameters.

Here is an example value of the request body JSON { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_id": "1000764491" }

You can find sample code here

Product purchase

You can use the below endpoint to capture user's product purchase information and save them in GAIP database

POST /v1/purchase It takes user_id, item_list which includes item_id, price, quantity for a specific item as required parameters.

Here is an example request body json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_list": [ { "item_id": "1000757666", "price": 5000, "quantity": 1 }, { "item_id": "1000764491", "price": 400, "quantity": 7 } ] }

You a find sample code for this implementation here

Product rating

You can use the below endpoint to capture user's product rating information and save them in GAIP database

POST /v1/rating

It takes user_id, item_id, and rating for the specific item as required parameters.

Here is a sample request body

json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_id": "1000764491", "rating": "1" }

You can find the sample code for this implementation here

If you want to save your data with bulk upload you can use above-mentioned endpoint.

Client to server integration

For client to server integration, you will need to generate client key as described in the Credentials sections. Once the client key is ready, you can directly send the request from your client side to GAIP, using the client key provided.

Note that while generating client key, you can add whitelisted domains, which whitelists the request origin. This is recommended to enhance security.

The rest of the implementation method is same as server to server integration.

Import user behavior data

Similar to data integration, all 4 kinds of user information (browse, purchase, rating, user) can be bulk uploaded. This could be useful if you already have this information from the past and want to import it into GAIP.

To import user behavior and user information in bulk, first you need to create mapper to match the keys with GAIP.

To create the mapper, the endpoints with the example request bodies can be found here in the gigalogy recommender page. You can also find the sample codes for mapper creation here in the API documentation page

Next we will use the below 4 endpoints to upload each category of data in bulk

Request path for product browsing history: POST /v1/items/browse/save
Request path for purchase history: POST /v1/items/purchase/save
Request path for rating history: POST /v1/items/rating/save
Request path to upload user information in bulk: POST /v1/users/save

You can find these endpoints with the example request bodies here. The sample code can be found here in the API documentation page

Previous
Environment setup
Next
Training your data
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/03_Data_integration_%26_user_behavior_collection/#product-purchase
Skip to content
Gigalogy Tutorial
Integration of Catalogue information and user behavior data
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Catalogue information integration.
Uploading data using a file
Fetch item from external API
Search Items in GAIP after import
User behavior data collection integration
Comparison of each approach.
Google Tag manager
Generating user ID
Collecting and sending user browsing data
Collecting and sending user purchase data
Collecting and sending user rating data
Collecting and sending user data
Server to server integration
User information
Product browse
Product purchase
Product rating
Client to server integration
Import user behavior data
Integration of Catalogue information and user behavior data

This tutorial will cover how to integrate your catalogue information into GAIP and how to set up user behavior tracking and integrate with GAIP.

Catalogue information integration.

Prerequisite: Mapping creation and the index creation is done.

Info

This step can also be done from our platform (GAIP). Refer here for detail.

There are two ways to import your catalogue information into GAIP.

Upload you catalogue information as a CSV or JSON file using endpoint POST /v1/item/save.
Fetch data from your API (or any external API) using endpoint POST /v1/item/save/remote.
Uploading data using a file

For uploading the data using a CSV file or JSON file, please use the POST /v1/item/save endpoint. Simply upload the file and confirm the server response is success. Confirm the task was successful using the GET/v1/tasks/{task_id} endpoint.

Info

This will throw an error and task will fail, if the keys during the Mapping creation step does not match with the keys in the file, OR if the indices were not created succesfully.

Fetch item from external API

To fetch data from external API, use the POST /v1/item/save/remote endpoint. The key and value types and an example request body for the endpoint can be found here in our sandbox.

After hitting either of the endpoint above to import your data into GAIP, you will get a task ID in the response. Use this task ID and hit the /v1/tasks/{task_id} endpoint to confirm the operation was successful. In case it fails, you can also find the details there. For API documentation, please refer <>

Search Items in GAIP after import

You can search items by passing list of item ids fromPOST /v1/items/search endpoint. This endpoint will return searched items with item details.

It is recommended to use this endpoint to confirm that the item catalogue is successfully imported into your project.

User behavior data collection integration

GAIP can collect different user behavior related information to optimize the recommendation for the user. Types of data collected are listed below with their endpoints.

Data type	Endpoint
Product browsing: When user browse products.	/v1/items/browse or /v1/items/browse/client
Product Purchase: When user purchase a product with its quantity.	/v1/items/purchase or /v1/items/purchase/client
Rating: When a user rates a product.	/v1/items/rating or /v1/items/rating/client
User: User information such as age, gender and other customized attributes depending on your website.	/v1/users or /v1/users/client

You will find these endpoints listed in our Sandbox under section "User Data Collection". Please check the required parameters, value types and example request bodies for all the endpoints there.

There are 3 ways to integrate user behavior data collection with GAIP

Google Tag Manager
Server to server integration
Client to server integration

You can also bulk upload user behavior data from the past. For that, please refer to Import user behavior data section.

Comparison of each approach.
Approach	GTM	Server to Server	Client to Server
Description	Use Google Tag Manager (GTM) to collect data (User behavior) from your website and send it to GAIP via endpoint.	The data is captured in the backend server of your application and then sent to GAIP via endpoint.	The data is directly sent from your front end (Client side) to GAIP via endpoint.
Pros	Easy to implement, Minimum coding required, Flexible configuration	More secure, Data integrity, Controlled environment	Real-time data, Less server dependency
Cons	Limited customization, need to have basic knowledge about GTM, Dependency on third-party service, Might not work for certain browsers and plugins like AdBlockers	More complex to set up, potential latency, maintenance required	Less secure, Potential for inconsistent data, dependency on client-side behavior

Below we will show the implementation of each approach.

BE ADVISED: The following is a general guideline, and it may vary across different websites, contingent upon the specific implementation of your website.

Google Tag manager

Prerequisite: Your website must have GTM setup. If you do not have GTM setup, you can easily do the setup by following the guidelines here.

If you are not familiar with basic GTM concepts, such as Tags, Triggers and Variables, please familiarize yourself first with these concepts before proceeding with this approach. You can find more resources related to this here.

Generating user ID

In our sandbox, Notice that in the user data collection endpoints, every endpoint has a parameter called user_id, and member_id. These are vital to identify each user so that you can personalize their experience. user_id is generated by GAIP for each user of your site. The endpoint to generate and user_id is GET /v1/users/generate/id. You can generate the user_id using GTM using below code. This code can be used with every Tag, which checks if there is a user_id and creates one if there is none.

js // Function to get or generate 'gaip_user_id' using a function expression var getGaipUser = function() { return new Promise(function(resolve, reject) { if (gaipUser !== null) { resolve(gaipUser); } else { var idHeaders = new Headers(); idHeaders.append("project-key", "{{ YOUR_PROJECT_KEY_HERE }}"); idHeaders.append("api-key", "{{ YOUR_API_KEY_HERE }}"); fetch("None/v1/users/generate/id", { headers: idHeaders }) .then(function(response) { return response.json(); }) .then(function(data) { localStorage.setItem('gaip_user_id', data.detail.response); resolve(data.detail.response); }) .catch(reject); } }); };

member_id can be set by you depending on how your site identifies unique users such as user ID, phone number, email address etc.

Collecting and sending user browsing data

Set up a variable to capture the product name/title/ID, when the user goes a product details page or clicks on a product to enlarge it or open a pop-up etc.

Set up a trigger so that the tag would fire when the user goes browses an item (Go to product detail page or quick view options etc.).

Create a custom HTML Tag with the above trigger and variable and put the below code in the tag.

Collecting and sending user purchase data

Set up a variable to capture the all the purchase detail, when the user makes a purchase. This could be from the purchase confirmation page etc.

Set up a trigger so that the tag would fire when the user make the purchase.

Create a custom HTML Tag with the above trigger to send the information to endpoint POST /v1/purchase or POST /v1/purchase/client

Collecting and sending user rating data

Setup variables to capture the product name/title/ID and the rating, when the user rates an item positively or negatively. We can also consider an item is positively rated when user adds the item to wishlist.

Set up a trigger for the tag to fire when the user rates a product.

Create a custom HTML Tag with the above trigger and variables to send the information to the endpoint POST /v1/rating or POST /v1/rating/client

Collecting and sending user data

Setup variable to capture the user information.

For this, the trigger could be setup up when the user logs or update their information.

Create a custom HTML Tag with the above trigger and variables to send the information to the endpoint POST /v1/user or POST /v1/user/client

Server to server integration

For server to server integration, you will need to generate Project key and API as mentioned in the credentials section.

User information

The below request path, takes user information, such as name, age, gender, address and saves them in the gaip database.

POST /v1/user

Here is an example request body

json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "user_info": { "address": "string", "gender": "integer --> 1 for male or 2 for female or 3 for others", "age": 25, "user_type": [ { "key_name1": "value1_value2", "separator": "_" }, { "key_name2": "value3" } ] } } You can find the sample code for implementation here

Product browse

You can use the below endpoint to capture user browsing information and save them in GAIP database js POST /v1/items/browse

It takes user_id and item_id as required parameters.

Here is an example value of the request body JSON { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_id": "1000764491" }

You can find sample code here

Product purchase

You can use the below endpoint to capture user's product purchase information and save them in GAIP database

POST /v1/purchase It takes user_id, item_list which includes item_id, price, quantity for a specific item as required parameters.

Here is an example request body json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_list": [ { "item_id": "1000757666", "price": 5000, "quantity": 1 }, { "item_id": "1000764491", "price": 400, "quantity": 7 } ] }

You a find sample code for this implementation here

Product rating

You can use the below endpoint to capture user's product rating information and save them in GAIP database

POST /v1/rating

It takes user_id, item_id, and rating for the specific item as required parameters.

Here is a sample request body

json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_id": "1000764491", "rating": "1" }

You can find the sample code for this implementation here

If you want to save your data with bulk upload you can use above-mentioned endpoint.

Client to server integration

For client to server integration, you will need to generate client key as described in the Credentials sections. Once the client key is ready, you can directly send the request from your client side to GAIP, using the client key provided.

Note that while generating client key, you can add whitelisted domains, which whitelists the request origin. This is recommended to enhance security.

The rest of the implementation method is same as server to server integration.

Import user behavior data

Similar to data integration, all 4 kinds of user information (browse, purchase, rating, user) can be bulk uploaded. This could be useful if you already have this information from the past and want to import it into GAIP.

To import user behavior and user information in bulk, first you need to create mapper to match the keys with GAIP.

To create the mapper, the endpoints with the example request bodies can be found here in the gigalogy recommender page. You can also find the sample codes for mapper creation here in the API documentation page

Next we will use the below 4 endpoints to upload each category of data in bulk

Request path for product browsing history: POST /v1/items/browse/save
Request path for purchase history: POST /v1/items/purchase/save
Request path for rating history: POST /v1/items/rating/save
Request path to upload user information in bulk: POST /v1/users/save

You can find these endpoints with the example request bodies here. The sample code can be found here in the API documentation page

Previous
Environment setup
Next
Training your data
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/03_Data_integration_%26_user_behavior_collection/#product-rating
Skip to content
Gigalogy Tutorial
Integration of Catalogue information and user behavior data
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Catalogue information integration.
Uploading data using a file
Fetch item from external API
Search Items in GAIP after import
User behavior data collection integration
Comparison of each approach.
Google Tag manager
Generating user ID
Collecting and sending user browsing data
Collecting and sending user purchase data
Collecting and sending user rating data
Collecting and sending user data
Server to server integration
User information
Product browse
Product purchase
Product rating
Client to server integration
Import user behavior data
Integration of Catalogue information and user behavior data

This tutorial will cover how to integrate your catalogue information into GAIP and how to set up user behavior tracking and integrate with GAIP.

Catalogue information integration.

Prerequisite: Mapping creation and the index creation is done.

Info

This step can also be done from our platform (GAIP). Refer here for detail.

There are two ways to import your catalogue information into GAIP.

Upload you catalogue information as a CSV or JSON file using endpoint POST /v1/item/save.
Fetch data from your API (or any external API) using endpoint POST /v1/item/save/remote.
Uploading data using a file

For uploading the data using a CSV file or JSON file, please use the POST /v1/item/save endpoint. Simply upload the file and confirm the server response is success. Confirm the task was successful using the GET/v1/tasks/{task_id} endpoint.

Info

This will throw an error and task will fail, if the keys during the Mapping creation step does not match with the keys in the file, OR if the indices were not created succesfully.

Fetch item from external API

To fetch data from external API, use the POST /v1/item/save/remote endpoint. The key and value types and an example request body for the endpoint can be found here in our sandbox.

After hitting either of the endpoint above to import your data into GAIP, you will get a task ID in the response. Use this task ID and hit the /v1/tasks/{task_id} endpoint to confirm the operation was successful. In case it fails, you can also find the details there. For API documentation, please refer <>

Search Items in GAIP after import

You can search items by passing list of item ids fromPOST /v1/items/search endpoint. This endpoint will return searched items with item details.

It is recommended to use this endpoint to confirm that the item catalogue is successfully imported into your project.

User behavior data collection integration

GAIP can collect different user behavior related information to optimize the recommendation for the user. Types of data collected are listed below with their endpoints.

Data type	Endpoint
Product browsing: When user browse products.	/v1/items/browse or /v1/items/browse/client
Product Purchase: When user purchase a product with its quantity.	/v1/items/purchase or /v1/items/purchase/client
Rating: When a user rates a product.	/v1/items/rating or /v1/items/rating/client
User: User information such as age, gender and other customized attributes depending on your website.	/v1/users or /v1/users/client

You will find these endpoints listed in our Sandbox under section "User Data Collection". Please check the required parameters, value types and example request bodies for all the endpoints there.

There are 3 ways to integrate user behavior data collection with GAIP

Google Tag Manager
Server to server integration
Client to server integration

You can also bulk upload user behavior data from the past. For that, please refer to Import user behavior data section.

Comparison of each approach.
Approach	GTM	Server to Server	Client to Server
Description	Use Google Tag Manager (GTM) to collect data (User behavior) from your website and send it to GAIP via endpoint.	The data is captured in the backend server of your application and then sent to GAIP via endpoint.	The data is directly sent from your front end (Client side) to GAIP via endpoint.
Pros	Easy to implement, Minimum coding required, Flexible configuration	More secure, Data integrity, Controlled environment	Real-time data, Less server dependency
Cons	Limited customization, need to have basic knowledge about GTM, Dependency on third-party service, Might not work for certain browsers and plugins like AdBlockers	More complex to set up, potential latency, maintenance required	Less secure, Potential for inconsistent data, dependency on client-side behavior

Below we will show the implementation of each approach.

BE ADVISED: The following is a general guideline, and it may vary across different websites, contingent upon the specific implementation of your website.

Google Tag manager

Prerequisite: Your website must have GTM setup. If you do not have GTM setup, you can easily do the setup by following the guidelines here.

If you are not familiar with basic GTM concepts, such as Tags, Triggers and Variables, please familiarize yourself first with these concepts before proceeding with this approach. You can find more resources related to this here.

Generating user ID

In our sandbox, Notice that in the user data collection endpoints, every endpoint has a parameter called user_id, and member_id. These are vital to identify each user so that you can personalize their experience. user_id is generated by GAIP for each user of your site. The endpoint to generate and user_id is GET /v1/users/generate/id. You can generate the user_id using GTM using below code. This code can be used with every Tag, which checks if there is a user_id and creates one if there is none.

js // Function to get or generate 'gaip_user_id' using a function expression var getGaipUser = function() { return new Promise(function(resolve, reject) { if (gaipUser !== null) { resolve(gaipUser); } else { var idHeaders = new Headers(); idHeaders.append("project-key", "{{ YOUR_PROJECT_KEY_HERE }}"); idHeaders.append("api-key", "{{ YOUR_API_KEY_HERE }}"); fetch("None/v1/users/generate/id", { headers: idHeaders }) .then(function(response) { return response.json(); }) .then(function(data) { localStorage.setItem('gaip_user_id', data.detail.response); resolve(data.detail.response); }) .catch(reject); } }); };

member_id can be set by you depending on how your site identifies unique users such as user ID, phone number, email address etc.

Collecting and sending user browsing data

Set up a variable to capture the product name/title/ID, when the user goes a product details page or clicks on a product to enlarge it or open a pop-up etc.

Set up a trigger so that the tag would fire when the user goes browses an item (Go to product detail page or quick view options etc.).

Create a custom HTML Tag with the above trigger and variable and put the below code in the tag.

Collecting and sending user purchase data

Set up a variable to capture the all the purchase detail, when the user makes a purchase. This could be from the purchase confirmation page etc.

Set up a trigger so that the tag would fire when the user make the purchase.

Create a custom HTML Tag with the above trigger to send the information to endpoint POST /v1/purchase or POST /v1/purchase/client

Collecting and sending user rating data

Setup variables to capture the product name/title/ID and the rating, when the user rates an item positively or negatively. We can also consider an item is positively rated when user adds the item to wishlist.

Set up a trigger for the tag to fire when the user rates a product.

Create a custom HTML Tag with the above trigger and variables to send the information to the endpoint POST /v1/rating or POST /v1/rating/client

Collecting and sending user data

Setup variable to capture the user information.

For this, the trigger could be setup up when the user logs or update their information.

Create a custom HTML Tag with the above trigger and variables to send the information to the endpoint POST /v1/user or POST /v1/user/client

Server to server integration

For server to server integration, you will need to generate Project key and API as mentioned in the credentials section.

User information

The below request path, takes user information, such as name, age, gender, address and saves them in the gaip database.

POST /v1/user

Here is an example request body

json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "user_info": { "address": "string", "gender": "integer --> 1 for male or 2 for female or 3 for others", "age": 25, "user_type": [ { "key_name1": "value1_value2", "separator": "_" }, { "key_name2": "value3" } ] } } You can find the sample code for implementation here

Product browse

You can use the below endpoint to capture user browsing information and save them in GAIP database js POST /v1/items/browse

It takes user_id and item_id as required parameters.

Here is an example value of the request body JSON { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_id": "1000764491" }

You can find sample code here

Product purchase

You can use the below endpoint to capture user's product purchase information and save them in GAIP database

POST /v1/purchase It takes user_id, item_list which includes item_id, price, quantity for a specific item as required parameters.

Here is an example request body json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_list": [ { "item_id": "1000757666", "price": 5000, "quantity": 1 }, { "item_id": "1000764491", "price": 400, "quantity": 7 } ] }

You a find sample code for this implementation here

Product rating

You can use the below endpoint to capture user's product rating information and save them in GAIP database

POST /v1/rating

It takes user_id, item_id, and rating for the specific item as required parameters.

Here is a sample request body

json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_id": "1000764491", "rating": "1" }

You can find the sample code for this implementation here

If you want to save your data with bulk upload you can use above-mentioned endpoint.

Client to server integration

For client to server integration, you will need to generate client key as described in the Credentials sections. Once the client key is ready, you can directly send the request from your client side to GAIP, using the client key provided.

Note that while generating client key, you can add whitelisted domains, which whitelists the request origin. This is recommended to enhance security.

The rest of the implementation method is same as server to server integration.

Import user behavior data

Similar to data integration, all 4 kinds of user information (browse, purchase, rating, user) can be bulk uploaded. This could be useful if you already have this information from the past and want to import it into GAIP.

To import user behavior and user information in bulk, first you need to create mapper to match the keys with GAIP.

To create the mapper, the endpoints with the example request bodies can be found here in the gigalogy recommender page. You can also find the sample codes for mapper creation here in the API documentation page

Next we will use the below 4 endpoints to upload each category of data in bulk

Request path for product browsing history: POST /v1/items/browse/save
Request path for purchase history: POST /v1/items/purchase/save
Request path for rating history: POST /v1/items/rating/save
Request path to upload user information in bulk: POST /v1/users/save

You can find these endpoints with the example request bodies here. The sample code can be found here in the API documentation page

Previous
Environment setup
Next
Training your data
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/03_Data_integration_%26_user_behavior_collection/#client-to-server-integration
Skip to content
Gigalogy Tutorial
Integration of Catalogue information and user behavior data
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Catalogue information integration.
Uploading data using a file
Fetch item from external API
Search Items in GAIP after import
User behavior data collection integration
Comparison of each approach.
Google Tag manager
Generating user ID
Collecting and sending user browsing data
Collecting and sending user purchase data
Collecting and sending user rating data
Collecting and sending user data
Server to server integration
User information
Product browse
Product purchase
Product rating
Client to server integration
Import user behavior data
Integration of Catalogue information and user behavior data

This tutorial will cover how to integrate your catalogue information into GAIP and how to set up user behavior tracking and integrate with GAIP.

Catalogue information integration.

Prerequisite: Mapping creation and the index creation is done.

Info

This step can also be done from our platform (GAIP). Refer here for detail.

There are two ways to import your catalogue information into GAIP.

Upload you catalogue information as a CSV or JSON file using endpoint POST /v1/item/save.
Fetch data from your API (or any external API) using endpoint POST /v1/item/save/remote.
Uploading data using a file

For uploading the data using a CSV file or JSON file, please use the POST /v1/item/save endpoint. Simply upload the file and confirm the server response is success. Confirm the task was successful using the GET/v1/tasks/{task_id} endpoint.

Info

This will throw an error and task will fail, if the keys during the Mapping creation step does not match with the keys in the file, OR if the indices were not created succesfully.

Fetch item from external API

To fetch data from external API, use the POST /v1/item/save/remote endpoint. The key and value types and an example request body for the endpoint can be found here in our sandbox.

After hitting either of the endpoint above to import your data into GAIP, you will get a task ID in the response. Use this task ID and hit the /v1/tasks/{task_id} endpoint to confirm the operation was successful. In case it fails, you can also find the details there. For API documentation, please refer <>

Search Items in GAIP after import

You can search items by passing list of item ids fromPOST /v1/items/search endpoint. This endpoint will return searched items with item details.

It is recommended to use this endpoint to confirm that the item catalogue is successfully imported into your project.

User behavior data collection integration

GAIP can collect different user behavior related information to optimize the recommendation for the user. Types of data collected are listed below with their endpoints.

Data type	Endpoint
Product browsing: When user browse products.	/v1/items/browse or /v1/items/browse/client
Product Purchase: When user purchase a product with its quantity.	/v1/items/purchase or /v1/items/purchase/client
Rating: When a user rates a product.	/v1/items/rating or /v1/items/rating/client
User: User information such as age, gender and other customized attributes depending on your website.	/v1/users or /v1/users/client

You will find these endpoints listed in our Sandbox under section "User Data Collection". Please check the required parameters, value types and example request bodies for all the endpoints there.

There are 3 ways to integrate user behavior data collection with GAIP

Google Tag Manager
Server to server integration
Client to server integration

You can also bulk upload user behavior data from the past. For that, please refer to Import user behavior data section.

Comparison of each approach.
Approach	GTM	Server to Server	Client to Server
Description	Use Google Tag Manager (GTM) to collect data (User behavior) from your website and send it to GAIP via endpoint.	The data is captured in the backend server of your application and then sent to GAIP via endpoint.	The data is directly sent from your front end (Client side) to GAIP via endpoint.
Pros	Easy to implement, Minimum coding required, Flexible configuration	More secure, Data integrity, Controlled environment	Real-time data, Less server dependency
Cons	Limited customization, need to have basic knowledge about GTM, Dependency on third-party service, Might not work for certain browsers and plugins like AdBlockers	More complex to set up, potential latency, maintenance required	Less secure, Potential for inconsistent data, dependency on client-side behavior

Below we will show the implementation of each approach.

BE ADVISED: The following is a general guideline, and it may vary across different websites, contingent upon the specific implementation of your website.

Google Tag manager

Prerequisite: Your website must have GTM setup. If you do not have GTM setup, you can easily do the setup by following the guidelines here.

If you are not familiar with basic GTM concepts, such as Tags, Triggers and Variables, please familiarize yourself first with these concepts before proceeding with this approach. You can find more resources related to this here.

Generating user ID

In our sandbox, Notice that in the user data collection endpoints, every endpoint has a parameter called user_id, and member_id. These are vital to identify each user so that you can personalize their experience. user_id is generated by GAIP for each user of your site. The endpoint to generate and user_id is GET /v1/users/generate/id. You can generate the user_id using GTM using below code. This code can be used with every Tag, which checks if there is a user_id and creates one if there is none.

js // Function to get or generate 'gaip_user_id' using a function expression var getGaipUser = function() { return new Promise(function(resolve, reject) { if (gaipUser !== null) { resolve(gaipUser); } else { var idHeaders = new Headers(); idHeaders.append("project-key", "{{ YOUR_PROJECT_KEY_HERE }}"); idHeaders.append("api-key", "{{ YOUR_API_KEY_HERE }}"); fetch("None/v1/users/generate/id", { headers: idHeaders }) .then(function(response) { return response.json(); }) .then(function(data) { localStorage.setItem('gaip_user_id', data.detail.response); resolve(data.detail.response); }) .catch(reject); } }); };

member_id can be set by you depending on how your site identifies unique users such as user ID, phone number, email address etc.

Collecting and sending user browsing data

Set up a variable to capture the product name/title/ID, when the user goes a product details page or clicks on a product to enlarge it or open a pop-up etc.

Set up a trigger so that the tag would fire when the user goes browses an item (Go to product detail page or quick view options etc.).

Create a custom HTML Tag with the above trigger and variable and put the below code in the tag.

Collecting and sending user purchase data

Set up a variable to capture the all the purchase detail, when the user makes a purchase. This could be from the purchase confirmation page etc.

Set up a trigger so that the tag would fire when the user make the purchase.

Create a custom HTML Tag with the above trigger to send the information to endpoint POST /v1/purchase or POST /v1/purchase/client

Collecting and sending user rating data

Setup variables to capture the product name/title/ID and the rating, when the user rates an item positively or negatively. We can also consider an item is positively rated when user adds the item to wishlist.

Set up a trigger for the tag to fire when the user rates a product.

Create a custom HTML Tag with the above trigger and variables to send the information to the endpoint POST /v1/rating or POST /v1/rating/client

Collecting and sending user data

Setup variable to capture the user information.

For this, the trigger could be setup up when the user logs or update their information.

Create a custom HTML Tag with the above trigger and variables to send the information to the endpoint POST /v1/user or POST /v1/user/client

Server to server integration

For server to server integration, you will need to generate Project key and API as mentioned in the credentials section.

User information

The below request path, takes user information, such as name, age, gender, address and saves them in the gaip database.

POST /v1/user

Here is an example request body

json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "user_info": { "address": "string", "gender": "integer --> 1 for male or 2 for female or 3 for others", "age": 25, "user_type": [ { "key_name1": "value1_value2", "separator": "_" }, { "key_name2": "value3" } ] } } You can find the sample code for implementation here

Product browse

You can use the below endpoint to capture user browsing information and save them in GAIP database js POST /v1/items/browse

It takes user_id and item_id as required parameters.

Here is an example value of the request body JSON { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_id": "1000764491" }

You can find sample code here

Product purchase

You can use the below endpoint to capture user's product purchase information and save them in GAIP database

POST /v1/purchase It takes user_id, item_list which includes item_id, price, quantity for a specific item as required parameters.

Here is an example request body json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_list": [ { "item_id": "1000757666", "price": 5000, "quantity": 1 }, { "item_id": "1000764491", "price": 400, "quantity": 7 } ] }

You a find sample code for this implementation here

Product rating

You can use the below endpoint to capture user's product rating information and save them in GAIP database

POST /v1/rating

It takes user_id, item_id, and rating for the specific item as required parameters.

Here is a sample request body

json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_id": "1000764491", "rating": "1" }

You can find the sample code for this implementation here

If you want to save your data with bulk upload you can use above-mentioned endpoint.

Client to server integration

For client to server integration, you will need to generate client key as described in the Credentials sections. Once the client key is ready, you can directly send the request from your client side to GAIP, using the client key provided.

Note that while generating client key, you can add whitelisted domains, which whitelists the request origin. This is recommended to enhance security.

The rest of the implementation method is same as server to server integration.

Import user behavior data

Similar to data integration, all 4 kinds of user information (browse, purchase, rating, user) can be bulk uploaded. This could be useful if you already have this information from the past and want to import it into GAIP.

To import user behavior and user information in bulk, first you need to create mapper to match the keys with GAIP.

To create the mapper, the endpoints with the example request bodies can be found here in the gigalogy recommender page. You can also find the sample codes for mapper creation here in the API documentation page

Next we will use the below 4 endpoints to upload each category of data in bulk

Request path for product browsing history: POST /v1/items/browse/save
Request path for purchase history: POST /v1/items/purchase/save
Request path for rating history: POST /v1/items/rating/save
Request path to upload user information in bulk: POST /v1/users/save

You can find these endpoints with the example request bodies here. The sample code can be found here in the API documentation page

Previous
Environment setup
Next
Training your data
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/03_Data_integration_%26_user_behavior_collection/#import-user-behavior-data
Skip to content
Gigalogy Tutorial
Integration of Catalogue information and user behavior data
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Catalogue information integration.
Uploading data using a file
Fetch item from external API
Search Items in GAIP after import
User behavior data collection integration
Comparison of each approach.
Google Tag manager
Generating user ID
Collecting and sending user browsing data
Collecting and sending user purchase data
Collecting and sending user rating data
Collecting and sending user data
Server to server integration
User information
Product browse
Product purchase
Product rating
Client to server integration
Import user behavior data
Integration of Catalogue information and user behavior data

This tutorial will cover how to integrate your catalogue information into GAIP and how to set up user behavior tracking and integrate with GAIP.

Catalogue information integration.

Prerequisite: Mapping creation and the index creation is done.

Info

This step can also be done from our platform (GAIP). Refer here for detail.

There are two ways to import your catalogue information into GAIP.

Upload you catalogue information as a CSV or JSON file using endpoint POST /v1/item/save.
Fetch data from your API (or any external API) using endpoint POST /v1/item/save/remote.
Uploading data using a file

For uploading the data using a CSV file or JSON file, please use the POST /v1/item/save endpoint. Simply upload the file and confirm the server response is success. Confirm the task was successful using the GET/v1/tasks/{task_id} endpoint.

Info

This will throw an error and task will fail, if the keys during the Mapping creation step does not match with the keys in the file, OR if the indices were not created succesfully.

Fetch item from external API

To fetch data from external API, use the POST /v1/item/save/remote endpoint. The key and value types and an example request body for the endpoint can be found here in our sandbox.

After hitting either of the endpoint above to import your data into GAIP, you will get a task ID in the response. Use this task ID and hit the /v1/tasks/{task_id} endpoint to confirm the operation was successful. In case it fails, you can also find the details there. For API documentation, please refer <>

Search Items in GAIP after import

You can search items by passing list of item ids fromPOST /v1/items/search endpoint. This endpoint will return searched items with item details.

It is recommended to use this endpoint to confirm that the item catalogue is successfully imported into your project.

User behavior data collection integration

GAIP can collect different user behavior related information to optimize the recommendation for the user. Types of data collected are listed below with their endpoints.

Data type	Endpoint
Product browsing: When user browse products.	/v1/items/browse or /v1/items/browse/client
Product Purchase: When user purchase a product with its quantity.	/v1/items/purchase or /v1/items/purchase/client
Rating: When a user rates a product.	/v1/items/rating or /v1/items/rating/client
User: User information such as age, gender and other customized attributes depending on your website.	/v1/users or /v1/users/client

You will find these endpoints listed in our Sandbox under section "User Data Collection". Please check the required parameters, value types and example request bodies for all the endpoints there.

There are 3 ways to integrate user behavior data collection with GAIP

Google Tag Manager
Server to server integration
Client to server integration

You can also bulk upload user behavior data from the past. For that, please refer to Import user behavior data section.

Comparison of each approach.
Approach	GTM	Server to Server	Client to Server
Description	Use Google Tag Manager (GTM) to collect data (User behavior) from your website and send it to GAIP via endpoint.	The data is captured in the backend server of your application and then sent to GAIP via endpoint.	The data is directly sent from your front end (Client side) to GAIP via endpoint.
Pros	Easy to implement, Minimum coding required, Flexible configuration	More secure, Data integrity, Controlled environment	Real-time data, Less server dependency
Cons	Limited customization, need to have basic knowledge about GTM, Dependency on third-party service, Might not work for certain browsers and plugins like AdBlockers	More complex to set up, potential latency, maintenance required	Less secure, Potential for inconsistent data, dependency on client-side behavior

Below we will show the implementation of each approach.

BE ADVISED: The following is a general guideline, and it may vary across different websites, contingent upon the specific implementation of your website.

Google Tag manager

Prerequisite: Your website must have GTM setup. If you do not have GTM setup, you can easily do the setup by following the guidelines here.

If you are not familiar with basic GTM concepts, such as Tags, Triggers and Variables, please familiarize yourself first with these concepts before proceeding with this approach. You can find more resources related to this here.

Generating user ID

In our sandbox, Notice that in the user data collection endpoints, every endpoint has a parameter called user_id, and member_id. These are vital to identify each user so that you can personalize their experience. user_id is generated by GAIP for each user of your site. The endpoint to generate and user_id is GET /v1/users/generate/id. You can generate the user_id using GTM using below code. This code can be used with every Tag, which checks if there is a user_id and creates one if there is none.

js // Function to get or generate 'gaip_user_id' using a function expression var getGaipUser = function() { return new Promise(function(resolve, reject) { if (gaipUser !== null) { resolve(gaipUser); } else { var idHeaders = new Headers(); idHeaders.append("project-key", "{{ YOUR_PROJECT_KEY_HERE }}"); idHeaders.append("api-key", "{{ YOUR_API_KEY_HERE }}"); fetch("None/v1/users/generate/id", { headers: idHeaders }) .then(function(response) { return response.json(); }) .then(function(data) { localStorage.setItem('gaip_user_id', data.detail.response); resolve(data.detail.response); }) .catch(reject); } }); };

member_id can be set by you depending on how your site identifies unique users such as user ID, phone number, email address etc.

Collecting and sending user browsing data

Set up a variable to capture the product name/title/ID, when the user goes a product details page or clicks on a product to enlarge it or open a pop-up etc.

Set up a trigger so that the tag would fire when the user goes browses an item (Go to product detail page or quick view options etc.).

Create a custom HTML Tag with the above trigger and variable and put the below code in the tag.

Collecting and sending user purchase data

Set up a variable to capture the all the purchase detail, when the user makes a purchase. This could be from the purchase confirmation page etc.

Set up a trigger so that the tag would fire when the user make the purchase.

Create a custom HTML Tag with the above trigger to send the information to endpoint POST /v1/purchase or POST /v1/purchase/client

Collecting and sending user rating data

Setup variables to capture the product name/title/ID and the rating, when the user rates an item positively or negatively. We can also consider an item is positively rated when user adds the item to wishlist.

Set up a trigger for the tag to fire when the user rates a product.

Create a custom HTML Tag with the above trigger and variables to send the information to the endpoint POST /v1/rating or POST /v1/rating/client

Collecting and sending user data

Setup variable to capture the user information.

For this, the trigger could be setup up when the user logs or update their information.

Create a custom HTML Tag with the above trigger and variables to send the information to the endpoint POST /v1/user or POST /v1/user/client

Server to server integration

For server to server integration, you will need to generate Project key and API as mentioned in the credentials section.

User information

The below request path, takes user information, such as name, age, gender, address and saves them in the gaip database.

POST /v1/user

Here is an example request body

json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "user_info": { "address": "string", "gender": "integer --> 1 for male or 2 for female or 3 for others", "age": 25, "user_type": [ { "key_name1": "value1_value2", "separator": "_" }, { "key_name2": "value3" } ] } } You can find the sample code for implementation here

Product browse

You can use the below endpoint to capture user browsing information and save them in GAIP database js POST /v1/items/browse

It takes user_id and item_id as required parameters.

Here is an example value of the request body JSON { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_id": "1000764491" }

You can find sample code here

Product purchase

You can use the below endpoint to capture user's product purchase information and save them in GAIP database

POST /v1/purchase It takes user_id, item_list which includes item_id, price, quantity for a specific item as required parameters.

Here is an example request body json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_list": [ { "item_id": "1000757666", "price": 5000, "quantity": 1 }, { "item_id": "1000764491", "price": 400, "quantity": 7 } ] }

You a find sample code for this implementation here

Product rating

You can use the below endpoint to capture user's product rating information and save them in GAIP database

POST /v1/rating

It takes user_id, item_id, and rating for the specific item as required parameters.

Here is a sample request body

json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_id": "1000764491", "rating": "1" }

You can find the sample code for this implementation here

If you want to save your data with bulk upload you can use above-mentioned endpoint.

Client to server integration

For client to server integration, you will need to generate client key as described in the Credentials sections. Once the client key is ready, you can directly send the request from your client side to GAIP, using the client key provided.

Note that while generating client key, you can add whitelisted domains, which whitelists the request origin. This is recommended to enhance security.

The rest of the implementation method is same as server to server integration.

Import user behavior data

Similar to data integration, all 4 kinds of user information (browse, purchase, rating, user) can be bulk uploaded. This could be useful if you already have this information from the past and want to import it into GAIP.

To import user behavior and user information in bulk, first you need to create mapper to match the keys with GAIP.

To create the mapper, the endpoints with the example request bodies can be found here in the gigalogy recommender page. You can also find the sample codes for mapper creation here in the API documentation page

Next we will use the below 4 endpoints to upload each category of data in bulk

Request path for product browsing history: POST /v1/items/browse/save
Request path for purchase history: POST /v1/items/purchase/save
Request path for rating history: POST /v1/items/rating/save
Request path to upload user information in bulk: POST /v1/users/save

You can find these endpoints with the example request bodies here. The sample code can be found here in the API documentation page

Previous
Environment setup
Next
Training your data
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/04_Training/
Skip to content
Gigalogy Tutorial
Training your data
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Item Training
Image Training
Rank Training
Training your data

After you have integrated and saved your data in the GAIP database, you can train and generate intelligence from your data. You need to train several models to get your desired output. In GAIP, there are three different models that have been designed to serve different purposes.

All models are trained at regular intervals. You can also train the models manually using the following endpoints.

Info

It is recommended to run all the trainings once manually after importing the item catalog/data into GAIP. After that you can check the result immediately

Info

These steps can also be done from our platform (GAIP). Refer here for detail.

Item Training

Item model takes your items from the database and generates intelligence from it so that our solution could recommend items from user behavior and filter out items with personalized search results.

Request endpoint for training item model:

GET /v1/item/train

Image Training

Image model takes image url from item database and generates feature vectors. Feature vectors leads to calculate the similarity score and predict similar image.

Request endpoint for training image model:

GET /v1/image/train

Rank Training

Rank model considers user behavior, item relations from item data to predict most trending items.

Note

You will need user data to train the rank model. Follow user behavior section to save user data.

Request endpoint for training rank model:

GET /v1/rank/train

While hitting the training endpoints, you will get a task id in response. You can check the training status of the model using that task id from GET v1/tasks/{task_id} endpoint.

It is recommended to always check and confirm with the Task endpoints to confirm the status of the training. When using it for the first time, it might take some time to complete.

You could also check the status of any training model with task type from GET/v1/tasks endpoint. By default, you will get the latest 100 tasks with their details.

You can train each model once in 24 hours time frame.

Previous
Integration of Catalogue information and user behavior data
Next
Personalized search
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/04_Training/#item-training
Skip to content
Gigalogy Tutorial
Training your data
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Item Training
Image Training
Rank Training
Training your data

After you have integrated and saved your data in the GAIP database, you can train and generate intelligence from your data. You need to train several models to get your desired output. In GAIP, there are three different models that have been designed to serve different purposes.

All models are trained at regular intervals. You can also train the models manually using the following endpoints.

Info

It is recommended to run all the trainings once manually after importing the item catalog/data into GAIP. After that you can check the result immediately

Info

These steps can also be done from our platform (GAIP). Refer here for detail.

Item Training

Item model takes your items from the database and generates intelligence from it so that our solution could recommend items from user behavior and filter out items with personalized search results.

Request endpoint for training item model:

GET /v1/item/train

Image Training

Image model takes image url from item database and generates feature vectors. Feature vectors leads to calculate the similarity score and predict similar image.

Request endpoint for training image model:

GET /v1/image/train

Rank Training

Rank model considers user behavior, item relations from item data to predict most trending items.

Note

You will need user data to train the rank model. Follow user behavior section to save user data.

Request endpoint for training rank model:

GET /v1/rank/train

While hitting the training endpoints, you will get a task id in response. You can check the training status of the model using that task id from GET v1/tasks/{task_id} endpoint.

It is recommended to always check and confirm with the Task endpoints to confirm the status of the training. When using it for the first time, it might take some time to complete.

You could also check the status of any training model with task type from GET/v1/tasks endpoint. By default, you will get the latest 100 tasks with their details.

You can train each model once in 24 hours time frame.

Previous
Integration of Catalogue information and user behavior data
Next
Personalized search
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/04_Training/#image-training
Skip to content
Gigalogy Tutorial
Training your data
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Item Training
Image Training
Rank Training
Training your data

After you have integrated and saved your data in the GAIP database, you can train and generate intelligence from your data. You need to train several models to get your desired output. In GAIP, there are three different models that have been designed to serve different purposes.

All models are trained at regular intervals. You can also train the models manually using the following endpoints.

Info

It is recommended to run all the trainings once manually after importing the item catalog/data into GAIP. After that you can check the result immediately

Info

These steps can also be done from our platform (GAIP). Refer here for detail.

Item Training

Item model takes your items from the database and generates intelligence from it so that our solution could recommend items from user behavior and filter out items with personalized search results.

Request endpoint for training item model:

GET /v1/item/train

Image Training

Image model takes image url from item database and generates feature vectors. Feature vectors leads to calculate the similarity score and predict similar image.

Request endpoint for training image model:

GET /v1/image/train

Rank Training

Rank model considers user behavior, item relations from item data to predict most trending items.

Note

You will need user data to train the rank model. Follow user behavior section to save user data.

Request endpoint for training rank model:

GET /v1/rank/train

While hitting the training endpoints, you will get a task id in response. You can check the training status of the model using that task id from GET v1/tasks/{task_id} endpoint.

It is recommended to always check and confirm with the Task endpoints to confirm the status of the training. When using it for the first time, it might take some time to complete.

You could also check the status of any training model with task type from GET/v1/tasks endpoint. By default, you will get the latest 100 tasks with their details.

You can train each model once in 24 hours time frame.

Previous
Integration of Catalogue information and user behavior data
Next
Personalized search
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/04_Training/#rank-training
Skip to content
Gigalogy Tutorial
Training your data
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Item Training
Image Training
Rank Training
Training your data

After you have integrated and saved your data in the GAIP database, you can train and generate intelligence from your data. You need to train several models to get your desired output. In GAIP, there are three different models that have been designed to serve different purposes.

All models are trained at regular intervals. You can also train the models manually using the following endpoints.

Info

It is recommended to run all the trainings once manually after importing the item catalog/data into GAIP. After that you can check the result immediately

Info

These steps can also be done from our platform (GAIP). Refer here for detail.

Item Training

Item model takes your items from the database and generates intelligence from it so that our solution could recommend items from user behavior and filter out items with personalized search results.

Request endpoint for training item model:

GET /v1/item/train

Image Training

Image model takes image url from item database and generates feature vectors. Feature vectors leads to calculate the similarity score and predict similar image.

Request endpoint for training image model:

GET /v1/image/train

Rank Training

Rank model considers user behavior, item relations from item data to predict most trending items.

Note

You will need user data to train the rank model. Follow user behavior section to save user data.

Request endpoint for training rank model:

GET /v1/rank/train

While hitting the training endpoints, you will get a task id in response. You can check the training status of the model using that task id from GET v1/tasks/{task_id} endpoint.

It is recommended to always check and confirm with the Task endpoints to confirm the status of the training. When using it for the first time, it might take some time to complete.

You could also check the status of any training model with task type from GET/v1/tasks endpoint. By default, you will get the latest 100 tasks with their details.

You can train each model once in 24 hours time frame.

Previous
Integration of Catalogue information and user behavior data
Next
Personalized search
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/01_Integration/00_Personalized_search_engine/
Skip to content
Gigalogy Tutorial
Personalized search
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Personalized search engine

Our endpoint /v1/items/search, generates personalized search results tailored for each user in your application.

It considers user behavior, current trends, and more and returns a customized search result for each individual.

This endpoint takes several parameters to generate customized search results.

To implement this solution, please use the endpoint /v1/items/search. The required parameters and an example request body can be found in our sandbox here.

You can find sample code for this implementation here.

Please note

category : User can search the items using category field using the value that you have mapped in the environment setup (mapper creation) with the "category" key.
flag : If you have any boolean values in your dataset and set as flag in mapper settings you can pass those values to filter the items using flag field.
sort : In sort field you have to pass columns that doesn't contain text field. You couldn't pass title, second_title, third_title, fourth_title, description as the column name.
Previous
Training your data
Next
Personalized Image Search
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/01_Integration/00_Personalized_search_engine/#personalized-search-engine
Skip to content
Gigalogy Tutorial
Personalized search
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Personalized search engine

Our endpoint /v1/items/search, generates personalized search results tailored for each user in your application.

It considers user behavior, current trends, and more and returns a customized search result for each individual.

This endpoint takes several parameters to generate customized search results.

To implement this solution, please use the endpoint /v1/items/search. The required parameters and an example request body can be found in our sandbox here.

You can find sample code for this implementation here.

Please note

category : User can search the items using category field using the value that you have mapped in the environment setup (mapper creation) with the "category" key.
flag : If you have any boolean values in your dataset and set as flag in mapper settings you can pass those values to filter the items using flag field.
sort : In sort field you have to pass columns that doesn't contain text field. You couldn't pass title, second_title, third_title, fourth_title, description as the column name.
Previous
Training your data
Next
Personalized Image Search
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/01_Integration/06_Image_Search_Engine/
Skip to content
Gigalogy Tutorial
Personalized Image Search
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Image Search Engine

Image search endpoint generates search results based on image similarity. You have to upload an image it will search most similar items for you and retrieve them as response.

For example, this endpoint could be used in the product details page if you want to show similar product that matches the image of the product in the detail page.

Request path:

POST /v1/images/search

You can find the list and description of required parameters here.

You can find the sample codes for this implementation here.

Previous
Personalized search
Next
Personalized Feed
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/01_Integration/06_Image_Search_Engine/#image-search-engine
Skip to content
Gigalogy Tutorial
Personalized Image Search
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Image Search Engine

Image search endpoint generates search results based on image similarity. You have to upload an image it will search most similar items for you and retrieve them as response.

For example, this endpoint could be used in the product details page if you want to show similar product that matches the image of the product in the detail page.

Request path:

POST /v1/images/search

You can find the list and description of required parameters here.

You can find the sample codes for this implementation here.

Previous
Personalized search
Next
Personalized Feed
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/01_Integration/02_Personalized_Feed/
Skip to content
Gigalogy Tutorial
Personalized Feed
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
1. users/recommend
2. users/search/recommend
Personalized Feed

These solutions can be used to personalize the user feed when they log into your website, open the homepage, or search for products, among other actions.

There are two endpoints here that you can use at different sections of the page

1. users/recommend

POST /v1/users/recommend can be used to personalize the user feed such as the home page or some other general pages.

v1/users/recommend endpoint understands user behavior such as browse history, purchase history, favorites/wishlist, time spent in your application and returns the most suitable items for them as a response.

The details of required parameters and a sample request body for this can be found here.

You can find sample code for implementation here.

2. users/search/recommend

This endpoint analyzes user search history and, based on that, returns recommended items to the user. It can be integrated into the user feed, the top page, personalized suggestions, or as a widget, among other applications.

Additionally, the response from this endpoint can be utilized to send users emails about products recommended based on their search history.

Request path:

POST /v1/users/search/recommend To implement this solution, you can find the description of the required parameters and an example request body here. You can find sample codes for implementation here.

Previous
Personalized Image Search
Next
Recommend trending items
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/01_Integration/02_Personalized_Feed/#personalized-feed
Skip to content
Gigalogy Tutorial
Personalized Feed
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
1. users/recommend
2. users/search/recommend
Personalized Feed

These solutions can be used to personalize the user feed when they log into your website, open the homepage, or search for products, among other actions.

There are two endpoints here that you can use at different sections of the page

1. users/recommend

POST /v1/users/recommend can be used to personalize the user feed such as the home page or some other general pages.

v1/users/recommend endpoint understands user behavior such as browse history, purchase history, favorites/wishlist, time spent in your application and returns the most suitable items for them as a response.

The details of required parameters and a sample request body for this can be found here.

You can find sample code for implementation here.

2. users/search/recommend

This endpoint analyzes user search history and, based on that, returns recommended items to the user. It can be integrated into the user feed, the top page, personalized suggestions, or as a widget, among other applications.

Additionally, the response from this endpoint can be utilized to send users emails about products recommended based on their search history.

Request path:

POST /v1/users/search/recommend To implement this solution, you can find the description of the required parameters and an example request body here. You can find sample codes for implementation here.

Previous
Personalized Image Search
Next
Recommend trending items
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/01_Integration/02_Personalized_Feed/#1-usersrecommend
Skip to content
Gigalogy Tutorial
Personalized Feed
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
1. users/recommend
2. users/search/recommend
Personalized Feed

These solutions can be used to personalize the user feed when they log into your website, open the homepage, or search for products, among other actions.

There are two endpoints here that you can use at different sections of the page

1. users/recommend

POST /v1/users/recommend can be used to personalize the user feed such as the home page or some other general pages.

v1/users/recommend endpoint understands user behavior such as browse history, purchase history, favorites/wishlist, time spent in your application and returns the most suitable items for them as a response.

The details of required parameters and a sample request body for this can be found here.

You can find sample code for implementation here.

2. users/search/recommend

This endpoint analyzes user search history and, based on that, returns recommended items to the user. It can be integrated into the user feed, the top page, personalized suggestions, or as a widget, among other applications.

Additionally, the response from this endpoint can be utilized to send users emails about products recommended based on their search history.

Request path:

POST /v1/users/search/recommend To implement this solution, you can find the description of the required parameters and an example request body here. You can find sample codes for implementation here.

Previous
Personalized Image Search
Next
Recommend trending items
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/01_Integration/02_Personalized_Feed/#2-userssearchrecommend
Skip to content
Gigalogy Tutorial
Personalized Feed
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
1. users/recommend
2. users/search/recommend
Personalized Feed

These solutions can be used to personalize the user feed when they log into your website, open the homepage, or search for products, among other actions.

There are two endpoints here that you can use at different sections of the page

1. users/recommend

POST /v1/users/recommend can be used to personalize the user feed such as the home page or some other general pages.

v1/users/recommend endpoint understands user behavior such as browse history, purchase history, favorites/wishlist, time spent in your application and returns the most suitable items for them as a response.

The details of required parameters and a sample request body for this can be found here.

You can find sample code for implementation here.

2. users/search/recommend

This endpoint analyzes user search history and, based on that, returns recommended items to the user. It can be integrated into the user feed, the top page, personalized suggestions, or as a widget, among other applications.

Additionally, the response from this endpoint can be utilized to send users emails about products recommended based on their search history.

Request path:

POST /v1/users/search/recommend To implement this solution, you can find the description of the required parameters and an example request body here. You can find sample codes for implementation here.

Previous
Personalized Image Search
Next
Recommend trending items
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/01_Integration/03_Trending_Items_Recommendation_Engine/
Skip to content
Gigalogy Tutorial
Recommend trending items
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Trending items recommendation engine

Our /v1/items/trending endpoint considers all kind of possible parameters in your application such as user behavior, item co-relation, external impact, business impact and returns the trending items for a given interval as response. You can use it on user feed, top page, personalized suggestions etc.

To use this feature, you must first set up the key parameters for the ranking model via the endpoint POST /v1/ranks/settings.

You can find the required parameters with their description and an example request body here.

You can always check the current setting using GET /v1/ranks/settings and update the setting using PUT /v1/ranks/settings.

Once the above setting is done, then you can set up the feature for showing trending items using the below request path:

POST /v1/items/trending

To implement this solution, the required parameters with their description and an example request body can be found here. You can find sample code for implementation here.

Previous
Personalized Feed
Next
Recommend similar items
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/01_Integration/03_Trending_Items_Recommendation_Engine/#trending-items-recommendation-engine
Skip to content
Gigalogy Tutorial
Recommend trending items
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Trending items recommendation engine

Our /v1/items/trending endpoint considers all kind of possible parameters in your application such as user behavior, item co-relation, external impact, business impact and returns the trending items for a given interval as response. You can use it on user feed, top page, personalized suggestions etc.

To use this feature, you must first set up the key parameters for the ranking model via the endpoint POST /v1/ranks/settings.

You can find the required parameters with their description and an example request body here.

You can always check the current setting using GET /v1/ranks/settings and update the setting using PUT /v1/ranks/settings.

Once the above setting is done, then you can set up the feature for showing trending items using the below request path:

POST /v1/items/trending

To implement this solution, the required parameters with their description and an example request body can be found here. You can find sample code for implementation here.

Previous
Personalized Feed
Next
Recommend similar items
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/01_Integration/04_Recommend_Similar_Products_on_Details_Page/
Skip to content
Gigalogy Tutorial
Recommend similar items
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Recommend Similar Products

This solution can be used to recommend similar products and could be used in your EC site's product details page or search result page to show similar products.

Item recommend endpoint understands the content of your item and returns the most similar items as response.

Request path for this solution is: POST /v1/items/recommend

To implement this solution, the required parameters with description and a sample request body can be found here. You can find sample code for implementation here.

Previous
Recommend trending items
Next
Recommend items purchase together
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/01_Integration/04_Recommend_Similar_Products_on_Details_Page/#recommend-similar-products
Skip to content
Gigalogy Tutorial
Recommend similar items
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Recommend Similar Products

This solution can be used to recommend similar products and could be used in your EC site's product details page or search result page to show similar products.

Item recommend endpoint understands the content of your item and returns the most similar items as response.

Request path for this solution is: POST /v1/items/recommend

To implement this solution, the required parameters with description and a sample request body can be found here. You can find sample code for implementation here.

Previous
Recommend trending items
Next
Recommend items purchase together
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/01_Integration/purchased_together/
Gigalogy Tutorial
Recommend items purchase together
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Recommend items purchase together

This endpoint considers the items that are frequently purchased together and recommend those items for particular products. The endpoint, along with other parameters, takes item_id as an input, and in response, gives items that are usually purchased together with the given item.

Request path for this solution is: POST /v1/items/purchased/together

You can find the required parameters with their description and an example request body here in our sandbox.

You can find sample code for implementation here in our API documentation.

Previous
Recommend similar items
Next
Dynamic Pricing
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/01_Integration/05_Dynamic_Pricing/
Skip to content
Gigalogy Tutorial
Dynamic Pricing
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Dynamic Pricing

Dynamic pricing is a pricing strategy in which the cost of a product or service is adjusted in real-time based on the popularity of the product.

You can maximize revenue and profit by finding the optimal price point that customers are willing to pay.

The lowest price is always the given price.

Info

If you provided the catalogue information is also shared, a customized dynamic pricing solution can be designed. Please contact in this regard.

This solution have to be setup while setting up the personalize search engine.

Please find the boolean value dynamic pricing in the request body. When you provide the value "True," this solution will display a dynamic price to the user.

Previous
Recommend items purchase together
Next
Questionnaire
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/01_Integration/05_Dynamic_Pricing/#dynamic-pricing
Skip to content
Gigalogy Tutorial
Dynamic Pricing
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Dynamic Pricing

Dynamic pricing is a pricing strategy in which the cost of a product or service is adjusted in real-time based on the popularity of the product.

You can maximize revenue and profit by finding the optimal price point that customers are willing to pay.

The lowest price is always the given price.

Info

If you provided the catalogue information is also shared, a customized dynamic pricing solution can be designed. Please contact in this regard.

This solution have to be setup while setting up the personalize search engine.

Please find the boolean value dynamic pricing in the request body. When you provide the value "True," this solution will display a dynamic price to the user.

Previous
Recommend items purchase together
Next
Questionnaire
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/01_Integration/Questionnaire/
Skip to content
Gigalogy Tutorial
Questionnaire
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Registration of Questionnaire
Registration of Questionnaire Mapper
Questions
Exclusivity
Prompt
Prompt Order
GPT Settings
Integration of Questionnaire Recommendation
Reviewing Questionnaire Recommendation Logs
Questionnaire

Our questionnaire software is specifically tailored to provide recommendations for novice and experienced users. Here is how it works:

Questionnaire Design: We meticulously design the questionnaire to encapsulate all necessary questions and decisions.
Questionnaire Registration: The questionnaire is seamlessly integrated into the project, enabling easy access and interaction.
User Response Collection: User responses are gathered through an interactive, user-friendly, UI-based questionnaire embedded in the website.
Response Processing: These responses are then processed by the GAIP system.
Product Recommendation: Finally, personalized product recommendations are suggested based on the processed responses.

Furthermore, the registration of the questionnaire into the project constitutes two crucial phases:

Registration of Questionnaire: Where the questionnaire is incorporated into the project.
Registration of Questionnaire Mapper: Which equips the AI system with the knowledge to accurately interpret decisions based on the users' responses to the questions.
Registration of Questionnaire

To manage the questionnaire, a series of RESTful API endpoints are provided which can be explored in our sandbox.

Creating a new questionnaire: Make a POST request to /v1/questionnaires to create a new questionnaire. The sandbox hosts detailed explanations and sample request bodies. The questionnaire can be divided into different sections, with at least one required.

Listing all questionnaires: Sending a GET request to /v1/questionnaires/list lists all the questionnaire's ID and names. A project can host multiple questionnaires that are differentiated through these IDs.

Viewing a questionnaire: Make a GET request to /v1/questionnaires/{questionnaire_id} to retrieve a specific questionnaire's details, including sections and questions.

Updating and deleting questionnaire: Use the PUT /v1/questionnaires/{questionnaire_id} and DELETE /v1/questionnaires/{questionnaire_id} endpoints to update or remove a questionnaire respectively.

Registration of Questionnaire Mapper

A questionnaire mapper is essential to instruct the system on making decisions based on user responses. Corresponding endpoints for creating, viewing, and editing a mapper are found at /v1/questionnaires/{questionnaire_id}/mapper.

To create a mapper, use POST /v1/questionnaires/{questionnaire_id}/mapper.

The mapper consists of five main components:

Questions: This section maps the decisions for all questionnaire questions.
Exclusivity: Handles duplicate removal and exclusivity requirements.
Prompt Generation: This section produces prompts that are used by the AI to generate recommendations.
Prompt Order: Sets the sequence in which prompts appear.
GPT settings: Provides settings and parameters to the GPT module for context understanding.

Each section will be explained in detail as we progress further.

Questions

Inside a question object we add mapper for different possible values.

A questionnaire can have three types of questions 1. Radio button: User can select a single answer, 2. Checkbox: User can select multiple answers, 3. Free text: User can enter string.

To add mapper for a single value, use the value key as the key for related decision objects. This approach allows you to define how the system should handle or process that value.

Example

json { "skincare_history": { "q2": { "mapper": { "a": { "exclude": ["Retinol", "Retinol based products"] } } } } } In the above example, the key a represents answer selected by user. The object associated with a specifies decisions related to it—in this case, an exclude list. This list contains the values Retinol and Retinol based products, defined by the creator of the mapper. How this Exclude list is used is covered in a latter section below.

To add mapper for a combination of value (Answers with multiple selection), - Use | sign as or to combine more than one values for a decision in mapper. - Use & sign as and to combine more than one values for a decision in mapper. Example

json { "a|b|c": { "exclude": ["Retinol", "Retinol palmitate"] } }

To add mapper for free-text value, use any key to add the decision rule. This can be useful for taking age, allergy information etc. Example

json { "any": { "age": "" } }

When a decision need to be taken if a set of value is not selected we should use negative selection.

An example scenario.

What of the following skincare you follow every day?

a. Cleanser b. Moisturizer c. Spf d. Serum e. Toner f. Night cream g. Eye cream

Decision: if cleanser, moisturizer and spf is chosen, AI will consider all products

If one of the basic step(cleanser, moisturizer, spf) is not chosen, Ai will exclude Retinol, Retinol palmitate, tranexamic acid, vitamin C, vitamin C derivative , arbutin

Here we need to take a decision if cleanser, moisturizer, spf is not selected, this is a candidate for negative selection.

Rules: Use ~ as key to add negative selection decisions in a question mapper. Use value serial as usual inside the negative selection object to add decision if a value is not selected, e.g. "a": "decision" Example

json { "q1": { "mapper" : { "a": { "tags": ["acne", "anti-aging"] }, "~": { "a|b|c": { "exclude": ["Retinol", "Retinol palmitate", "tranexamic acid", "vitamin c", "vitamin c derivative" , "arbutin"] }, "d": { "tags": ["Retinol"] } } } } }

Exclusivity

The integration of exclusivity in the mapper is crucial for eliminating duplicates and addressing exclusivity prerequisites.

To incorporate exclusivity, you utilize the exclusive key to append an object in the following format:

"prompt_key-1" :  "prompt_key-2"


Doing so ensures that any value present in both prompt_key-1 and prompt_key-2, within the final prompt, will be extracted exclusively from prompt_key-2.

For instance: json { "exclusive": { "exclude": "includes", "exclude_tag": "include_tag" } }

Prompt

Each prompt section is a decision point. A prompt section has two constituting parts, the sentence and the variable. Let's say we have a decision point to exclude products that has certain tags. we can add a prompt section named exclude_tags as follows

json { "prompt_sections": { "exclude_tags": "Exclude any products that has following tags {exclude_tags}." } }

And We need to add the exclude_tags in mapper wherever this decision point need to be mapped. An example mapper, like we have discussed above, would be

json { "q1": { "mapper": { "a": { "exclude_tags": ["retinol", "retinol based products"] } } } }

An example prompt section for a cosmetics company could be.

json { "prompt_sections": { "age": "I am {age} years old.", "gender": "I am a {gender}.", "skin_type": "My skin type is {skin_type}.", "include_tag": "Show products that contain the tag: {include_tag}", "exclude_tag": "Exclude products that do contain the tag: {exclude_tag}" } }

Prompt Order

This section is to decide the order of the prompts in the prompt section above. This section must contain all the prompts from the above section. This is the order by which the prompts will be sent to GPT model. Note: This order effects the quality of the response. User should try different orders to find the best result.

An example prompt order from the prompt_sections sample above sample could be:

json { "prompt_order": [ "age", "gender", "skin_type", "include_tag", "exclude_tag" ] }

GPT Settings

This section is required for the context and various parameter setting for GPT. The prompt generated til this point above, goes to our GPT model, which is familiar with the item catalogue. This GPT model, in turn generates this recommendation for the user. Below is an example.

json { "gpt_settings": { "model": "gpt-3.5-turbo-0613", "search_max_token": 2500, "intro": "Using given context make recommendation. make recommendation of top {{ recommend_count }} products in json format and your response should denote the json part with three tick notations(). Use we as your pronoun. json object should have following format. {'comment': 'make an overall comment about your recommendations', 'advice':'...', 'recommendations': [{'id':'...', 'comment': 'why you chose this product'},{'id':'...', 'comment': 'why you chose this product'}]}. if the query includes specialist_pretext then include that suggestion in your 'advice'", "system": "You are an expert on ...", "temperature": 0, "top_p": 1, "frequency_penalty": 0, "presence_penalty": 0, "stop": [ "AI:", "Human:" ], "recommend_count": 3, "result_includes": [ "key_name_1", "key_name_2" ], "top_k": 20, "is_keyword_enabled": true } } ```

Integration of Questionnaire Recommendation

The questionnaire recommendation function processes user responses, identifies the corresponding questionnaire and decision points, and generates personalized recommendations. The endpoint POST /v1/questionnaire/recommend is designed for this purpose.

Detailed examples, required parameters, and explanations can be found in our sandbox.

This endpoint requires questionnaire_id as input. This identifier helps determine which questionnaire and mapping to consult, and which user response to consider, leading to the generation of personalized results.

You will notice the recommend_count parameter also exist in the mapper. If it is set, the value of this parameter takes priority over recommend_count variable in questionnaire mapper.

The response format of this endpoint can be customized using the previously mentioned GPT settings.

Reviewing Questionnaire Recommendation Logs

By utilizing the endpoint GET /v1/questionnaires/{questionnaire_id}/logs, you're able to access the log of requests dispatched to the v1/questionnaire/recommend endpoint, along with the corresponding generated responses.

Previous
Dynamic Pricing
Next
GPT Project setup
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/01_Integration/Questionnaire/#registration-of-questionnaire
Skip to content
Gigalogy Tutorial
Questionnaire
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Registration of Questionnaire
Registration of Questionnaire Mapper
Questions
Exclusivity
Prompt
Prompt Order
GPT Settings
Integration of Questionnaire Recommendation
Reviewing Questionnaire Recommendation Logs
Questionnaire

Our questionnaire software is specifically tailored to provide recommendations for novice and experienced users. Here is how it works:

Questionnaire Design: We meticulously design the questionnaire to encapsulate all necessary questions and decisions.
Questionnaire Registration: The questionnaire is seamlessly integrated into the project, enabling easy access and interaction.
User Response Collection: User responses are gathered through an interactive, user-friendly, UI-based questionnaire embedded in the website.
Response Processing: These responses are then processed by the GAIP system.
Product Recommendation: Finally, personalized product recommendations are suggested based on the processed responses.

Furthermore, the registration of the questionnaire into the project constitutes two crucial phases:

Registration of Questionnaire: Where the questionnaire is incorporated into the project.
Registration of Questionnaire Mapper: Which equips the AI system with the knowledge to accurately interpret decisions based on the users' responses to the questions.
Registration of Questionnaire

To manage the questionnaire, a series of RESTful API endpoints are provided which can be explored in our sandbox.

Creating a new questionnaire: Make a POST request to /v1/questionnaires to create a new questionnaire. The sandbox hosts detailed explanations and sample request bodies. The questionnaire can be divided into different sections, with at least one required.

Listing all questionnaires: Sending a GET request to /v1/questionnaires/list lists all the questionnaire's ID and names. A project can host multiple questionnaires that are differentiated through these IDs.

Viewing a questionnaire: Make a GET request to /v1/questionnaires/{questionnaire_id} to retrieve a specific questionnaire's details, including sections and questions.

Updating and deleting questionnaire: Use the PUT /v1/questionnaires/{questionnaire_id} and DELETE /v1/questionnaires/{questionnaire_id} endpoints to update or remove a questionnaire respectively.

Registration of Questionnaire Mapper

A questionnaire mapper is essential to instruct the system on making decisions based on user responses. Corresponding endpoints for creating, viewing, and editing a mapper are found at /v1/questionnaires/{questionnaire_id}/mapper.

To create a mapper, use POST /v1/questionnaires/{questionnaire_id}/mapper.

The mapper consists of five main components:

Questions: This section maps the decisions for all questionnaire questions.
Exclusivity: Handles duplicate removal and exclusivity requirements.
Prompt Generation: This section produces prompts that are used by the AI to generate recommendations.
Prompt Order: Sets the sequence in which prompts appear.
GPT settings: Provides settings and parameters to the GPT module for context understanding.

Each section will be explained in detail as we progress further.

Questions

Inside a question object we add mapper for different possible values.

A questionnaire can have three types of questions 1. Radio button: User can select a single answer, 2. Checkbox: User can select multiple answers, 3. Free text: User can enter string.

To add mapper for a single value, use the value key as the key for related decision objects. This approach allows you to define how the system should handle or process that value.

Example

json { "skincare_history": { "q2": { "mapper": { "a": { "exclude": ["Retinol", "Retinol based products"] } } } } } In the above example, the key a represents answer selected by user. The object associated with a specifies decisions related to it—in this case, an exclude list. This list contains the values Retinol and Retinol based products, defined by the creator of the mapper. How this Exclude list is used is covered in a latter section below.

To add mapper for a combination of value (Answers with multiple selection), - Use | sign as or to combine more than one values for a decision in mapper. - Use & sign as and to combine more than one values for a decision in mapper. Example

json { "a|b|c": { "exclude": ["Retinol", "Retinol palmitate"] } }

To add mapper for free-text value, use any key to add the decision rule. This can be useful for taking age, allergy information etc. Example

json { "any": { "age": "" } }

When a decision need to be taken if a set of value is not selected we should use negative selection.

An example scenario.

What of the following skincare you follow every day?

a. Cleanser b. Moisturizer c. Spf d. Serum e. Toner f. Night cream g. Eye cream

Decision: if cleanser, moisturizer and spf is chosen, AI will consider all products

If one of the basic step(cleanser, moisturizer, spf) is not chosen, Ai will exclude Retinol, Retinol palmitate, tranexamic acid, vitamin C, vitamin C derivative , arbutin

Here we need to take a decision if cleanser, moisturizer, spf is not selected, this is a candidate for negative selection.

Rules: Use ~ as key to add negative selection decisions in a question mapper. Use value serial as usual inside the negative selection object to add decision if a value is not selected, e.g. "a": "decision" Example

json { "q1": { "mapper" : { "a": { "tags": ["acne", "anti-aging"] }, "~": { "a|b|c": { "exclude": ["Retinol", "Retinol palmitate", "tranexamic acid", "vitamin c", "vitamin c derivative" , "arbutin"] }, "d": { "tags": ["Retinol"] } } } } }

Exclusivity

The integration of exclusivity in the mapper is crucial for eliminating duplicates and addressing exclusivity prerequisites.

To incorporate exclusivity, you utilize the exclusive key to append an object in the following format:

"prompt_key-1" :  "prompt_key-2"


Doing so ensures that any value present in both prompt_key-1 and prompt_key-2, within the final prompt, will be extracted exclusively from prompt_key-2.

For instance: json { "exclusive": { "exclude": "includes", "exclude_tag": "include_tag" } }

Prompt

Each prompt section is a decision point. A prompt section has two constituting parts, the sentence and the variable. Let's say we have a decision point to exclude products that has certain tags. we can add a prompt section named exclude_tags as follows

json { "prompt_sections": { "exclude_tags": "Exclude any products that has following tags {exclude_tags}." } }

And We need to add the exclude_tags in mapper wherever this decision point need to be mapped. An example mapper, like we have discussed above, would be

json { "q1": { "mapper": { "a": { "exclude_tags": ["retinol", "retinol based products"] } } } }

An example prompt section for a cosmetics company could be.

json { "prompt_sections": { "age": "I am {age} years old.", "gender": "I am a {gender}.", "skin_type": "My skin type is {skin_type}.", "include_tag": "Show products that contain the tag: {include_tag}", "exclude_tag": "Exclude products that do contain the tag: {exclude_tag}" } }

Prompt Order

This section is to decide the order of the prompts in the prompt section above. This section must contain all the prompts from the above section. This is the order by which the prompts will be sent to GPT model. Note: This order effects the quality of the response. User should try different orders to find the best result.

An example prompt order from the prompt_sections sample above sample could be:

json { "prompt_order": [ "age", "gender", "skin_type", "include_tag", "exclude_tag" ] }

GPT Settings

This section is required for the context and various parameter setting for GPT. The prompt generated til this point above, goes to our GPT model, which is familiar with the item catalogue. This GPT model, in turn generates this recommendation for the user. Below is an example.

json { "gpt_settings": { "model": "gpt-3.5-turbo-0613", "search_max_token": 2500, "intro": "Using given context make recommendation. make recommendation of top {{ recommend_count }} products in json format and your response should denote the json part with three tick notations(). Use we as your pronoun. json object should have following format. {'comment': 'make an overall comment about your recommendations', 'advice':'...', 'recommendations': [{'id':'...', 'comment': 'why you chose this product'},{'id':'...', 'comment': 'why you chose this product'}]}. if the query includes specialist_pretext then include that suggestion in your 'advice'", "system": "You are an expert on ...", "temperature": 0, "top_p": 1, "frequency_penalty": 0, "presence_penalty": 0, "stop": [ "AI:", "Human:" ], "recommend_count": 3, "result_includes": [ "key_name_1", "key_name_2" ], "top_k": 20, "is_keyword_enabled": true } } ```

Integration of Questionnaire Recommendation

The questionnaire recommendation function processes user responses, identifies the corresponding questionnaire and decision points, and generates personalized recommendations. The endpoint POST /v1/questionnaire/recommend is designed for this purpose.

Detailed examples, required parameters, and explanations can be found in our sandbox.

This endpoint requires questionnaire_id as input. This identifier helps determine which questionnaire and mapping to consult, and which user response to consider, leading to the generation of personalized results.

You will notice the recommend_count parameter also exist in the mapper. If it is set, the value of this parameter takes priority over recommend_count variable in questionnaire mapper.

The response format of this endpoint can be customized using the previously mentioned GPT settings.

Reviewing Questionnaire Recommendation Logs

By utilizing the endpoint GET /v1/questionnaires/{questionnaire_id}/logs, you're able to access the log of requests dispatched to the v1/questionnaire/recommend endpoint, along with the corresponding generated responses.

Previous
Dynamic Pricing
Next
GPT Project setup
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/01_Integration/Questionnaire/#registration-of-questionnaire-mapper
Skip to content
Gigalogy Tutorial
Questionnaire
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Registration of Questionnaire
Registration of Questionnaire Mapper
Questions
Exclusivity
Prompt
Prompt Order
GPT Settings
Integration of Questionnaire Recommendation
Reviewing Questionnaire Recommendation Logs
Questionnaire

Our questionnaire software is specifically tailored to provide recommendations for novice and experienced users. Here is how it works:

Questionnaire Design: We meticulously design the questionnaire to encapsulate all necessary questions and decisions.
Questionnaire Registration: The questionnaire is seamlessly integrated into the project, enabling easy access and interaction.
User Response Collection: User responses are gathered through an interactive, user-friendly, UI-based questionnaire embedded in the website.
Response Processing: These responses are then processed by the GAIP system.
Product Recommendation: Finally, personalized product recommendations are suggested based on the processed responses.

Furthermore, the registration of the questionnaire into the project constitutes two crucial phases:

Registration of Questionnaire: Where the questionnaire is incorporated into the project.
Registration of Questionnaire Mapper: Which equips the AI system with the knowledge to accurately interpret decisions based on the users' responses to the questions.
Registration of Questionnaire

To manage the questionnaire, a series of RESTful API endpoints are provided which can be explored in our sandbox.

Creating a new questionnaire: Make a POST request to /v1/questionnaires to create a new questionnaire. The sandbox hosts detailed explanations and sample request bodies. The questionnaire can be divided into different sections, with at least one required.

Listing all questionnaires: Sending a GET request to /v1/questionnaires/list lists all the questionnaire's ID and names. A project can host multiple questionnaires that are differentiated through these IDs.

Viewing a questionnaire: Make a GET request to /v1/questionnaires/{questionnaire_id} to retrieve a specific questionnaire's details, including sections and questions.

Updating and deleting questionnaire: Use the PUT /v1/questionnaires/{questionnaire_id} and DELETE /v1/questionnaires/{questionnaire_id} endpoints to update or remove a questionnaire respectively.

Registration of Questionnaire Mapper

A questionnaire mapper is essential to instruct the system on making decisions based on user responses. Corresponding endpoints for creating, viewing, and editing a mapper are found at /v1/questionnaires/{questionnaire_id}/mapper.

To create a mapper, use POST /v1/questionnaires/{questionnaire_id}/mapper.

The mapper consists of five main components:

Questions: This section maps the decisions for all questionnaire questions.
Exclusivity: Handles duplicate removal and exclusivity requirements.
Prompt Generation: This section produces prompts that are used by the AI to generate recommendations.
Prompt Order: Sets the sequence in which prompts appear.
GPT settings: Provides settings and parameters to the GPT module for context understanding.

Each section will be explained in detail as we progress further.

Questions

Inside a question object we add mapper for different possible values.

A questionnaire can have three types of questions 1. Radio button: User can select a single answer, 2. Checkbox: User can select multiple answers, 3. Free text: User can enter string.

To add mapper for a single value, use the value key as the key for related decision objects. This approach allows you to define how the system should handle or process that value.

Example

json { "skincare_history": { "q2": { "mapper": { "a": { "exclude": ["Retinol", "Retinol based products"] } } } } } In the above example, the key a represents answer selected by user. The object associated with a specifies decisions related to it—in this case, an exclude list. This list contains the values Retinol and Retinol based products, defined by the creator of the mapper. How this Exclude list is used is covered in a latter section below.

To add mapper for a combination of value (Answers with multiple selection), - Use | sign as or to combine more than one values for a decision in mapper. - Use & sign as and to combine more than one values for a decision in mapper. Example

json { "a|b|c": { "exclude": ["Retinol", "Retinol palmitate"] } }

To add mapper for free-text value, use any key to add the decision rule. This can be useful for taking age, allergy information etc. Example

json { "any": { "age": "" } }

When a decision need to be taken if a set of value is not selected we should use negative selection.

An example scenario.

What of the following skincare you follow every day?

a. Cleanser b. Moisturizer c. Spf d. Serum e. Toner f. Night cream g. Eye cream

Decision: if cleanser, moisturizer and spf is chosen, AI will consider all products

If one of the basic step(cleanser, moisturizer, spf) is not chosen, Ai will exclude Retinol, Retinol palmitate, tranexamic acid, vitamin C, vitamin C derivative , arbutin

Here we need to take a decision if cleanser, moisturizer, spf is not selected, this is a candidate for negative selection.

Rules: Use ~ as key to add negative selection decisions in a question mapper. Use value serial as usual inside the negative selection object to add decision if a value is not selected, e.g. "a": "decision" Example

json { "q1": { "mapper" : { "a": { "tags": ["acne", "anti-aging"] }, "~": { "a|b|c": { "exclude": ["Retinol", "Retinol palmitate", "tranexamic acid", "vitamin c", "vitamin c derivative" , "arbutin"] }, "d": { "tags": ["Retinol"] } } } } }

Exclusivity

The integration of exclusivity in the mapper is crucial for eliminating duplicates and addressing exclusivity prerequisites.

To incorporate exclusivity, you utilize the exclusive key to append an object in the following format:

"prompt_key-1" :  "prompt_key-2"


Doing so ensures that any value present in both prompt_key-1 and prompt_key-2, within the final prompt, will be extracted exclusively from prompt_key-2.

For instance: json { "exclusive": { "exclude": "includes", "exclude_tag": "include_tag" } }

Prompt

Each prompt section is a decision point. A prompt section has two constituting parts, the sentence and the variable. Let's say we have a decision point to exclude products that has certain tags. we can add a prompt section named exclude_tags as follows

json { "prompt_sections": { "exclude_tags": "Exclude any products that has following tags {exclude_tags}." } }

And We need to add the exclude_tags in mapper wherever this decision point need to be mapped. An example mapper, like we have discussed above, would be

json { "q1": { "mapper": { "a": { "exclude_tags": ["retinol", "retinol based products"] } } } }

An example prompt section for a cosmetics company could be.

json { "prompt_sections": { "age": "I am {age} years old.", "gender": "I am a {gender}.", "skin_type": "My skin type is {skin_type}.", "include_tag": "Show products that contain the tag: {include_tag}", "exclude_tag": "Exclude products that do contain the tag: {exclude_tag}" } }

Prompt Order

This section is to decide the order of the prompts in the prompt section above. This section must contain all the prompts from the above section. This is the order by which the prompts will be sent to GPT model. Note: This order effects the quality of the response. User should try different orders to find the best result.

An example prompt order from the prompt_sections sample above sample could be:

json { "prompt_order": [ "age", "gender", "skin_type", "include_tag", "exclude_tag" ] }

GPT Settings

This section is required for the context and various parameter setting for GPT. The prompt generated til this point above, goes to our GPT model, which is familiar with the item catalogue. This GPT model, in turn generates this recommendation for the user. Below is an example.

json { "gpt_settings": { "model": "gpt-3.5-turbo-0613", "search_max_token": 2500, "intro": "Using given context make recommendation. make recommendation of top {{ recommend_count }} products in json format and your response should denote the json part with three tick notations(). Use we as your pronoun. json object should have following format. {'comment': 'make an overall comment about your recommendations', 'advice':'...', 'recommendations': [{'id':'...', 'comment': 'why you chose this product'},{'id':'...', 'comment': 'why you chose this product'}]}. if the query includes specialist_pretext then include that suggestion in your 'advice'", "system": "You are an expert on ...", "temperature": 0, "top_p": 1, "frequency_penalty": 0, "presence_penalty": 0, "stop": [ "AI:", "Human:" ], "recommend_count": 3, "result_includes": [ "key_name_1", "key_name_2" ], "top_k": 20, "is_keyword_enabled": true } } ```

Integration of Questionnaire Recommendation

The questionnaire recommendation function processes user responses, identifies the corresponding questionnaire and decision points, and generates personalized recommendations. The endpoint POST /v1/questionnaire/recommend is designed for this purpose.

Detailed examples, required parameters, and explanations can be found in our sandbox.

This endpoint requires questionnaire_id as input. This identifier helps determine which questionnaire and mapping to consult, and which user response to consider, leading to the generation of personalized results.

You will notice the recommend_count parameter also exist in the mapper. If it is set, the value of this parameter takes priority over recommend_count variable in questionnaire mapper.

The response format of this endpoint can be customized using the previously mentioned GPT settings.

Reviewing Questionnaire Recommendation Logs

By utilizing the endpoint GET /v1/questionnaires/{questionnaire_id}/logs, you're able to access the log of requests dispatched to the v1/questionnaire/recommend endpoint, along with the corresponding generated responses.

Previous
Dynamic Pricing
Next
GPT Project setup
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/01_Integration/Questionnaire/#questions
Skip to content
Gigalogy Tutorial
Questionnaire
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Registration of Questionnaire
Registration of Questionnaire Mapper
Questions
Exclusivity
Prompt
Prompt Order
GPT Settings
Integration of Questionnaire Recommendation
Reviewing Questionnaire Recommendation Logs
Questionnaire

Our questionnaire software is specifically tailored to provide recommendations for novice and experienced users. Here is how it works:

Questionnaire Design: We meticulously design the questionnaire to encapsulate all necessary questions and decisions.
Questionnaire Registration: The questionnaire is seamlessly integrated into the project, enabling easy access and interaction.
User Response Collection: User responses are gathered through an interactive, user-friendly, UI-based questionnaire embedded in the website.
Response Processing: These responses are then processed by the GAIP system.
Product Recommendation: Finally, personalized product recommendations are suggested based on the processed responses.

Furthermore, the registration of the questionnaire into the project constitutes two crucial phases:

Registration of Questionnaire: Where the questionnaire is incorporated into the project.
Registration of Questionnaire Mapper: Which equips the AI system with the knowledge to accurately interpret decisions based on the users' responses to the questions.
Registration of Questionnaire

To manage the questionnaire, a series of RESTful API endpoints are provided which can be explored in our sandbox.

Creating a new questionnaire: Make a POST request to /v1/questionnaires to create a new questionnaire. The sandbox hosts detailed explanations and sample request bodies. The questionnaire can be divided into different sections, with at least one required.

Listing all questionnaires: Sending a GET request to /v1/questionnaires/list lists all the questionnaire's ID and names. A project can host multiple questionnaires that are differentiated through these IDs.

Viewing a questionnaire: Make a GET request to /v1/questionnaires/{questionnaire_id} to retrieve a specific questionnaire's details, including sections and questions.

Updating and deleting questionnaire: Use the PUT /v1/questionnaires/{questionnaire_id} and DELETE /v1/questionnaires/{questionnaire_id} endpoints to update or remove a questionnaire respectively.

Registration of Questionnaire Mapper

A questionnaire mapper is essential to instruct the system on making decisions based on user responses. Corresponding endpoints for creating, viewing, and editing a mapper are found at /v1/questionnaires/{questionnaire_id}/mapper.

To create a mapper, use POST /v1/questionnaires/{questionnaire_id}/mapper.

The mapper consists of five main components:

Questions: This section maps the decisions for all questionnaire questions.
Exclusivity: Handles duplicate removal and exclusivity requirements.
Prompt Generation: This section produces prompts that are used by the AI to generate recommendations.
Prompt Order: Sets the sequence in which prompts appear.
GPT settings: Provides settings and parameters to the GPT module for context understanding.

Each section will be explained in detail as we progress further.

Questions

Inside a question object we add mapper for different possible values.

A questionnaire can have three types of questions 1. Radio button: User can select a single answer, 2. Checkbox: User can select multiple answers, 3. Free text: User can enter string.

To add mapper for a single value, use the value key as the key for related decision objects. This approach allows you to define how the system should handle or process that value.

Example

json { "skincare_history": { "q2": { "mapper": { "a": { "exclude": ["Retinol", "Retinol based products"] } } } } } In the above example, the key a represents answer selected by user. The object associated with a specifies decisions related to it—in this case, an exclude list. This list contains the values Retinol and Retinol based products, defined by the creator of the mapper. How this Exclude list is used is covered in a latter section below.

To add mapper for a combination of value (Answers with multiple selection), - Use | sign as or to combine more than one values for a decision in mapper. - Use & sign as and to combine more than one values for a decision in mapper. Example

json { "a|b|c": { "exclude": ["Retinol", "Retinol palmitate"] } }

To add mapper for free-text value, use any key to add the decision rule. This can be useful for taking age, allergy information etc. Example

json { "any": { "age": "" } }

When a decision need to be taken if a set of value is not selected we should use negative selection.

An example scenario.

What of the following skincare you follow every day?

a. Cleanser b. Moisturizer c. Spf d. Serum e. Toner f. Night cream g. Eye cream

Decision: if cleanser, moisturizer and spf is chosen, AI will consider all products

If one of the basic step(cleanser, moisturizer, spf) is not chosen, Ai will exclude Retinol, Retinol palmitate, tranexamic acid, vitamin C, vitamin C derivative , arbutin

Here we need to take a decision if cleanser, moisturizer, spf is not selected, this is a candidate for negative selection.

Rules: Use ~ as key to add negative selection decisions in a question mapper. Use value serial as usual inside the negative selection object to add decision if a value is not selected, e.g. "a": "decision" Example

json { "q1": { "mapper" : { "a": { "tags": ["acne", "anti-aging"] }, "~": { "a|b|c": { "exclude": ["Retinol", "Retinol palmitate", "tranexamic acid", "vitamin c", "vitamin c derivative" , "arbutin"] }, "d": { "tags": ["Retinol"] } } } } }

Exclusivity

The integration of exclusivity in the mapper is crucial for eliminating duplicates and addressing exclusivity prerequisites.

To incorporate exclusivity, you utilize the exclusive key to append an object in the following format:

"prompt_key-1" :  "prompt_key-2"


Doing so ensures that any value present in both prompt_key-1 and prompt_key-2, within the final prompt, will be extracted exclusively from prompt_key-2.

For instance: json { "exclusive": { "exclude": "includes", "exclude_tag": "include_tag" } }

Prompt

Each prompt section is a decision point. A prompt section has two constituting parts, the sentence and the variable. Let's say we have a decision point to exclude products that has certain tags. we can add a prompt section named exclude_tags as follows

json { "prompt_sections": { "exclude_tags": "Exclude any products that has following tags {exclude_tags}." } }

And We need to add the exclude_tags in mapper wherever this decision point need to be mapped. An example mapper, like we have discussed above, would be

json { "q1": { "mapper": { "a": { "exclude_tags": ["retinol", "retinol based products"] } } } }

An example prompt section for a cosmetics company could be.

json { "prompt_sections": { "age": "I am {age} years old.", "gender": "I am a {gender}.", "skin_type": "My skin type is {skin_type}.", "include_tag": "Show products that contain the tag: {include_tag}", "exclude_tag": "Exclude products that do contain the tag: {exclude_tag}" } }

Prompt Order

This section is to decide the order of the prompts in the prompt section above. This section must contain all the prompts from the above section. This is the order by which the prompts will be sent to GPT model. Note: This order effects the quality of the response. User should try different orders to find the best result.

An example prompt order from the prompt_sections sample above sample could be:

json { "prompt_order": [ "age", "gender", "skin_type", "include_tag", "exclude_tag" ] }

GPT Settings

This section is required for the context and various parameter setting for GPT. The prompt generated til this point above, goes to our GPT model, which is familiar with the item catalogue. This GPT model, in turn generates this recommendation for the user. Below is an example.

json { "gpt_settings": { "model": "gpt-3.5-turbo-0613", "search_max_token": 2500, "intro": "Using given context make recommendation. make recommendation of top {{ recommend_count }} products in json format and your response should denote the json part with three tick notations(). Use we as your pronoun. json object should have following format. {'comment': 'make an overall comment about your recommendations', 'advice':'...', 'recommendations': [{'id':'...', 'comment': 'why you chose this product'},{'id':'...', 'comment': 'why you chose this product'}]}. if the query includes specialist_pretext then include that suggestion in your 'advice'", "system": "You are an expert on ...", "temperature": 0, "top_p": 1, "frequency_penalty": 0, "presence_penalty": 0, "stop": [ "AI:", "Human:" ], "recommend_count": 3, "result_includes": [ "key_name_1", "key_name_2" ], "top_k": 20, "is_keyword_enabled": true } } ```

Integration of Questionnaire Recommendation

The questionnaire recommendation function processes user responses, identifies the corresponding questionnaire and decision points, and generates personalized recommendations. The endpoint POST /v1/questionnaire/recommend is designed for this purpose.

Detailed examples, required parameters, and explanations can be found in our sandbox.

This endpoint requires questionnaire_id as input. This identifier helps determine which questionnaire and mapping to consult, and which user response to consider, leading to the generation of personalized results.

You will notice the recommend_count parameter also exist in the mapper. If it is set, the value of this parameter takes priority over recommend_count variable in questionnaire mapper.

The response format of this endpoint can be customized using the previously mentioned GPT settings.

Reviewing Questionnaire Recommendation Logs

By utilizing the endpoint GET /v1/questionnaires/{questionnaire_id}/logs, you're able to access the log of requests dispatched to the v1/questionnaire/recommend endpoint, along with the corresponding generated responses.

Previous
Dynamic Pricing
Next
GPT Project setup
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/01_Integration/Questionnaire/#exclusivity
Skip to content
Gigalogy Tutorial
Questionnaire
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Registration of Questionnaire
Registration of Questionnaire Mapper
Questions
Exclusivity
Prompt
Prompt Order
GPT Settings
Integration of Questionnaire Recommendation
Reviewing Questionnaire Recommendation Logs
Questionnaire

Our questionnaire software is specifically tailored to provide recommendations for novice and experienced users. Here is how it works:

Questionnaire Design: We meticulously design the questionnaire to encapsulate all necessary questions and decisions.
Questionnaire Registration: The questionnaire is seamlessly integrated into the project, enabling easy access and interaction.
User Response Collection: User responses are gathered through an interactive, user-friendly, UI-based questionnaire embedded in the website.
Response Processing: These responses are then processed by the GAIP system.
Product Recommendation: Finally, personalized product recommendations are suggested based on the processed responses.

Furthermore, the registration of the questionnaire into the project constitutes two crucial phases:

Registration of Questionnaire: Where the questionnaire is incorporated into the project.
Registration of Questionnaire Mapper: Which equips the AI system with the knowledge to accurately interpret decisions based on the users' responses to the questions.
Registration of Questionnaire

To manage the questionnaire, a series of RESTful API endpoints are provided which can be explored in our sandbox.

Creating a new questionnaire: Make a POST request to /v1/questionnaires to create a new questionnaire. The sandbox hosts detailed explanations and sample request bodies. The questionnaire can be divided into different sections, with at least one required.

Listing all questionnaires: Sending a GET request to /v1/questionnaires/list lists all the questionnaire's ID and names. A project can host multiple questionnaires that are differentiated through these IDs.

Viewing a questionnaire: Make a GET request to /v1/questionnaires/{questionnaire_id} to retrieve a specific questionnaire's details, including sections and questions.

Updating and deleting questionnaire: Use the PUT /v1/questionnaires/{questionnaire_id} and DELETE /v1/questionnaires/{questionnaire_id} endpoints to update or remove a questionnaire respectively.

Registration of Questionnaire Mapper

A questionnaire mapper is essential to instruct the system on making decisions based on user responses. Corresponding endpoints for creating, viewing, and editing a mapper are found at /v1/questionnaires/{questionnaire_id}/mapper.

To create a mapper, use POST /v1/questionnaires/{questionnaire_id}/mapper.

The mapper consists of five main components:

Questions: This section maps the decisions for all questionnaire questions.
Exclusivity: Handles duplicate removal and exclusivity requirements.
Prompt Generation: This section produces prompts that are used by the AI to generate recommendations.
Prompt Order: Sets the sequence in which prompts appear.
GPT settings: Provides settings and parameters to the GPT module for context understanding.

Each section will be explained in detail as we progress further.

Questions

Inside a question object we add mapper for different possible values.

A questionnaire can have three types of questions 1. Radio button: User can select a single answer, 2. Checkbox: User can select multiple answers, 3. Free text: User can enter string.

To add mapper for a single value, use the value key as the key for related decision objects. This approach allows you to define how the system should handle or process that value.

Example

json { "skincare_history": { "q2": { "mapper": { "a": { "exclude": ["Retinol", "Retinol based products"] } } } } } In the above example, the key a represents answer selected by user. The object associated with a specifies decisions related to it—in this case, an exclude list. This list contains the values Retinol and Retinol based products, defined by the creator of the mapper. How this Exclude list is used is covered in a latter section below.

To add mapper for a combination of value (Answers with multiple selection), - Use | sign as or to combine more than one values for a decision in mapper. - Use & sign as and to combine more than one values for a decision in mapper. Example

json { "a|b|c": { "exclude": ["Retinol", "Retinol palmitate"] } }

To add mapper for free-text value, use any key to add the decision rule. This can be useful for taking age, allergy information etc. Example

json { "any": { "age": "" } }

When a decision need to be taken if a set of value is not selected we should use negative selection.

An example scenario.

What of the following skincare you follow every day?

a. Cleanser b. Moisturizer c. Spf d. Serum e. Toner f. Night cream g. Eye cream

Decision: if cleanser, moisturizer and spf is chosen, AI will consider all products

If one of the basic step(cleanser, moisturizer, spf) is not chosen, Ai will exclude Retinol, Retinol palmitate, tranexamic acid, vitamin C, vitamin C derivative , arbutin

Here we need to take a decision if cleanser, moisturizer, spf is not selected, this is a candidate for negative selection.

Rules: Use ~ as key to add negative selection decisions in a question mapper. Use value serial as usual inside the negative selection object to add decision if a value is not selected, e.g. "a": "decision" Example

json { "q1": { "mapper" : { "a": { "tags": ["acne", "anti-aging"] }, "~": { "a|b|c": { "exclude": ["Retinol", "Retinol palmitate", "tranexamic acid", "vitamin c", "vitamin c derivative" , "arbutin"] }, "d": { "tags": ["Retinol"] } } } } }

Exclusivity

The integration of exclusivity in the mapper is crucial for eliminating duplicates and addressing exclusivity prerequisites.

To incorporate exclusivity, you utilize the exclusive key to append an object in the following format:

"prompt_key-1" :  "prompt_key-2"


Doing so ensures that any value present in both prompt_key-1 and prompt_key-2, within the final prompt, will be extracted exclusively from prompt_key-2.

For instance: json { "exclusive": { "exclude": "includes", "exclude_tag": "include_tag" } }

Prompt

Each prompt section is a decision point. A prompt section has two constituting parts, the sentence and the variable. Let's say we have a decision point to exclude products that has certain tags. we can add a prompt section named exclude_tags as follows

json { "prompt_sections": { "exclude_tags": "Exclude any products that has following tags {exclude_tags}." } }

And We need to add the exclude_tags in mapper wherever this decision point need to be mapped. An example mapper, like we have discussed above, would be

json { "q1": { "mapper": { "a": { "exclude_tags": ["retinol", "retinol based products"] } } } }

An example prompt section for a cosmetics company could be.

json { "prompt_sections": { "age": "I am {age} years old.", "gender": "I am a {gender}.", "skin_type": "My skin type is {skin_type}.", "include_tag": "Show products that contain the tag: {include_tag}", "exclude_tag": "Exclude products that do contain the tag: {exclude_tag}" } }

Prompt Order

This section is to decide the order of the prompts in the prompt section above. This section must contain all the prompts from the above section. This is the order by which the prompts will be sent to GPT model. Note: This order effects the quality of the response. User should try different orders to find the best result.

An example prompt order from the prompt_sections sample above sample could be:

json { "prompt_order": [ "age", "gender", "skin_type", "include_tag", "exclude_tag" ] }

GPT Settings

This section is required for the context and various parameter setting for GPT. The prompt generated til this point above, goes to our GPT model, which is familiar with the item catalogue. This GPT model, in turn generates this recommendation for the user. Below is an example.

json { "gpt_settings": { "model": "gpt-3.5-turbo-0613", "search_max_token": 2500, "intro": "Using given context make recommendation. make recommendation of top {{ recommend_count }} products in json format and your response should denote the json part with three tick notations(). Use we as your pronoun. json object should have following format. {'comment': 'make an overall comment about your recommendations', 'advice':'...', 'recommendations': [{'id':'...', 'comment': 'why you chose this product'},{'id':'...', 'comment': 'why you chose this product'}]}. if the query includes specialist_pretext then include that suggestion in your 'advice'", "system": "You are an expert on ...", "temperature": 0, "top_p": 1, "frequency_penalty": 0, "presence_penalty": 0, "stop": [ "AI:", "Human:" ], "recommend_count": 3, "result_includes": [ "key_name_1", "key_name_2" ], "top_k": 20, "is_keyword_enabled": true } } ```

Integration of Questionnaire Recommendation

The questionnaire recommendation function processes user responses, identifies the corresponding questionnaire and decision points, and generates personalized recommendations. The endpoint POST /v1/questionnaire/recommend is designed for this purpose.

Detailed examples, required parameters, and explanations can be found in our sandbox.

This endpoint requires questionnaire_id as input. This identifier helps determine which questionnaire and mapping to consult, and which user response to consider, leading to the generation of personalized results.

You will notice the recommend_count parameter also exist in the mapper. If it is set, the value of this parameter takes priority over recommend_count variable in questionnaire mapper.

The response format of this endpoint can be customized using the previously mentioned GPT settings.

Reviewing Questionnaire Recommendation Logs

By utilizing the endpoint GET /v1/questionnaires/{questionnaire_id}/logs, you're able to access the log of requests dispatched to the v1/questionnaire/recommend endpoint, along with the corresponding generated responses.

Previous
Dynamic Pricing
Next
GPT Project setup
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/01_Integration/Questionnaire/#prompt
Skip to content
Gigalogy Tutorial
Questionnaire
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Registration of Questionnaire
Registration of Questionnaire Mapper
Questions
Exclusivity
Prompt
Prompt Order
GPT Settings
Integration of Questionnaire Recommendation
Reviewing Questionnaire Recommendation Logs
Questionnaire

Our questionnaire software is specifically tailored to provide recommendations for novice and experienced users. Here is how it works:

Questionnaire Design: We meticulously design the questionnaire to encapsulate all necessary questions and decisions.
Questionnaire Registration: The questionnaire is seamlessly integrated into the project, enabling easy access and interaction.
User Response Collection: User responses are gathered through an interactive, user-friendly, UI-based questionnaire embedded in the website.
Response Processing: These responses are then processed by the GAIP system.
Product Recommendation: Finally, personalized product recommendations are suggested based on the processed responses.

Furthermore, the registration of the questionnaire into the project constitutes two crucial phases:

Registration of Questionnaire: Where the questionnaire is incorporated into the project.
Registration of Questionnaire Mapper: Which equips the AI system with the knowledge to accurately interpret decisions based on the users' responses to the questions.
Registration of Questionnaire

To manage the questionnaire, a series of RESTful API endpoints are provided which can be explored in our sandbox.

Creating a new questionnaire: Make a POST request to /v1/questionnaires to create a new questionnaire. The sandbox hosts detailed explanations and sample request bodies. The questionnaire can be divided into different sections, with at least one required.

Listing all questionnaires: Sending a GET request to /v1/questionnaires/list lists all the questionnaire's ID and names. A project can host multiple questionnaires that are differentiated through these IDs.

Viewing a questionnaire: Make a GET request to /v1/questionnaires/{questionnaire_id} to retrieve a specific questionnaire's details, including sections and questions.

Updating and deleting questionnaire: Use the PUT /v1/questionnaires/{questionnaire_id} and DELETE /v1/questionnaires/{questionnaire_id} endpoints to update or remove a questionnaire respectively.

Registration of Questionnaire Mapper

A questionnaire mapper is essential to instruct the system on making decisions based on user responses. Corresponding endpoints for creating, viewing, and editing a mapper are found at /v1/questionnaires/{questionnaire_id}/mapper.

To create a mapper, use POST /v1/questionnaires/{questionnaire_id}/mapper.

The mapper consists of five main components:

Questions: This section maps the decisions for all questionnaire questions.
Exclusivity: Handles duplicate removal and exclusivity requirements.
Prompt Generation: This section produces prompts that are used by the AI to generate recommendations.
Prompt Order: Sets the sequence in which prompts appear.
GPT settings: Provides settings and parameters to the GPT module for context understanding.

Each section will be explained in detail as we progress further.

Questions

Inside a question object we add mapper for different possible values.

A questionnaire can have three types of questions 1. Radio button: User can select a single answer, 2. Checkbox: User can select multiple answers, 3. Free text: User can enter string.

To add mapper for a single value, use the value key as the key for related decision objects. This approach allows you to define how the system should handle or process that value.

Example

json { "skincare_history": { "q2": { "mapper": { "a": { "exclude": ["Retinol", "Retinol based products"] } } } } } In the above example, the key a represents answer selected by user. The object associated with a specifies decisions related to it—in this case, an exclude list. This list contains the values Retinol and Retinol based products, defined by the creator of the mapper. How this Exclude list is used is covered in a latter section below.

To add mapper for a combination of value (Answers with multiple selection), - Use | sign as or to combine more than one values for a decision in mapper. - Use & sign as and to combine more than one values for a decision in mapper. Example

json { "a|b|c": { "exclude": ["Retinol", "Retinol palmitate"] } }

To add mapper for free-text value, use any key to add the decision rule. This can be useful for taking age, allergy information etc. Example

json { "any": { "age": "" } }

When a decision need to be taken if a set of value is not selected we should use negative selection.

An example scenario.

What of the following skincare you follow every day?

a. Cleanser b. Moisturizer c. Spf d. Serum e. Toner f. Night cream g. Eye cream

Decision: if cleanser, moisturizer and spf is chosen, AI will consider all products

If one of the basic step(cleanser, moisturizer, spf) is not chosen, Ai will exclude Retinol, Retinol palmitate, tranexamic acid, vitamin C, vitamin C derivative , arbutin

Here we need to take a decision if cleanser, moisturizer, spf is not selected, this is a candidate for negative selection.

Rules: Use ~ as key to add negative selection decisions in a question mapper. Use value serial as usual inside the negative selection object to add decision if a value is not selected, e.g. "a": "decision" Example

json { "q1": { "mapper" : { "a": { "tags": ["acne", "anti-aging"] }, "~": { "a|b|c": { "exclude": ["Retinol", "Retinol palmitate", "tranexamic acid", "vitamin c", "vitamin c derivative" , "arbutin"] }, "d": { "tags": ["Retinol"] } } } } }

Exclusivity

The integration of exclusivity in the mapper is crucial for eliminating duplicates and addressing exclusivity prerequisites.

To incorporate exclusivity, you utilize the exclusive key to append an object in the following format:

"prompt_key-1" :  "prompt_key-2"


Doing so ensures that any value present in both prompt_key-1 and prompt_key-2, within the final prompt, will be extracted exclusively from prompt_key-2.

For instance: json { "exclusive": { "exclude": "includes", "exclude_tag": "include_tag" } }

Prompt

Each prompt section is a decision point. A prompt section has two constituting parts, the sentence and the variable. Let's say we have a decision point to exclude products that has certain tags. we can add a prompt section named exclude_tags as follows

json { "prompt_sections": { "exclude_tags": "Exclude any products that has following tags {exclude_tags}." } }

And We need to add the exclude_tags in mapper wherever this decision point need to be mapped. An example mapper, like we have discussed above, would be

json { "q1": { "mapper": { "a": { "exclude_tags": ["retinol", "retinol based products"] } } } }

An example prompt section for a cosmetics company could be.

json { "prompt_sections": { "age": "I am {age} years old.", "gender": "I am a {gender}.", "skin_type": "My skin type is {skin_type}.", "include_tag": "Show products that contain the tag: {include_tag}", "exclude_tag": "Exclude products that do contain the tag: {exclude_tag}" } }

Prompt Order

This section is to decide the order of the prompts in the prompt section above. This section must contain all the prompts from the above section. This is the order by which the prompts will be sent to GPT model. Note: This order effects the quality of the response. User should try different orders to find the best result.

An example prompt order from the prompt_sections sample above sample could be:

json { "prompt_order": [ "age", "gender", "skin_type", "include_tag", "exclude_tag" ] }

GPT Settings

This section is required for the context and various parameter setting for GPT. The prompt generated til this point above, goes to our GPT model, which is familiar with the item catalogue. This GPT model, in turn generates this recommendation for the user. Below is an example.

json { "gpt_settings": { "model": "gpt-3.5-turbo-0613", "search_max_token": 2500, "intro": "Using given context make recommendation. make recommendation of top {{ recommend_count }} products in json format and your response should denote the json part with three tick notations(). Use we as your pronoun. json object should have following format. {'comment': 'make an overall comment about your recommendations', 'advice':'...', 'recommendations': [{'id':'...', 'comment': 'why you chose this product'},{'id':'...', 'comment': 'why you chose this product'}]}. if the query includes specialist_pretext then include that suggestion in your 'advice'", "system": "You are an expert on ...", "temperature": 0, "top_p": 1, "frequency_penalty": 0, "presence_penalty": 0, "stop": [ "AI:", "Human:" ], "recommend_count": 3, "result_includes": [ "key_name_1", "key_name_2" ], "top_k": 20, "is_keyword_enabled": true } } ```

Integration of Questionnaire Recommendation

The questionnaire recommendation function processes user responses, identifies the corresponding questionnaire and decision points, and generates personalized recommendations. The endpoint POST /v1/questionnaire/recommend is designed for this purpose.

Detailed examples, required parameters, and explanations can be found in our sandbox.

This endpoint requires questionnaire_id as input. This identifier helps determine which questionnaire and mapping to consult, and which user response to consider, leading to the generation of personalized results.

You will notice the recommend_count parameter also exist in the mapper. If it is set, the value of this parameter takes priority over recommend_count variable in questionnaire mapper.

The response format of this endpoint can be customized using the previously mentioned GPT settings.

Reviewing Questionnaire Recommendation Logs

By utilizing the endpoint GET /v1/questionnaires/{questionnaire_id}/logs, you're able to access the log of requests dispatched to the v1/questionnaire/recommend endpoint, along with the corresponding generated responses.

Previous
Dynamic Pricing
Next
GPT Project setup
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/01_Integration/Questionnaire/#prompt-order
Skip to content
Gigalogy Tutorial
Questionnaire
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Registration of Questionnaire
Registration of Questionnaire Mapper
Questions
Exclusivity
Prompt
Prompt Order
GPT Settings
Integration of Questionnaire Recommendation
Reviewing Questionnaire Recommendation Logs
Questionnaire

Our questionnaire software is specifically tailored to provide recommendations for novice and experienced users. Here is how it works:

Questionnaire Design: We meticulously design the questionnaire to encapsulate all necessary questions and decisions.
Questionnaire Registration: The questionnaire is seamlessly integrated into the project, enabling easy access and interaction.
User Response Collection: User responses are gathered through an interactive, user-friendly, UI-based questionnaire embedded in the website.
Response Processing: These responses are then processed by the GAIP system.
Product Recommendation: Finally, personalized product recommendations are suggested based on the processed responses.

Furthermore, the registration of the questionnaire into the project constitutes two crucial phases:

Registration of Questionnaire: Where the questionnaire is incorporated into the project.
Registration of Questionnaire Mapper: Which equips the AI system with the knowledge to accurately interpret decisions based on the users' responses to the questions.
Registration of Questionnaire

To manage the questionnaire, a series of RESTful API endpoints are provided which can be explored in our sandbox.

Creating a new questionnaire: Make a POST request to /v1/questionnaires to create a new questionnaire. The sandbox hosts detailed explanations and sample request bodies. The questionnaire can be divided into different sections, with at least one required.

Listing all questionnaires: Sending a GET request to /v1/questionnaires/list lists all the questionnaire's ID and names. A project can host multiple questionnaires that are differentiated through these IDs.

Viewing a questionnaire: Make a GET request to /v1/questionnaires/{questionnaire_id} to retrieve a specific questionnaire's details, including sections and questions.

Updating and deleting questionnaire: Use the PUT /v1/questionnaires/{questionnaire_id} and DELETE /v1/questionnaires/{questionnaire_id} endpoints to update or remove a questionnaire respectively.

Registration of Questionnaire Mapper

A questionnaire mapper is essential to instruct the system on making decisions based on user responses. Corresponding endpoints for creating, viewing, and editing a mapper are found at /v1/questionnaires/{questionnaire_id}/mapper.

To create a mapper, use POST /v1/questionnaires/{questionnaire_id}/mapper.

The mapper consists of five main components:

Questions: This section maps the decisions for all questionnaire questions.
Exclusivity: Handles duplicate removal and exclusivity requirements.
Prompt Generation: This section produces prompts that are used by the AI to generate recommendations.
Prompt Order: Sets the sequence in which prompts appear.
GPT settings: Provides settings and parameters to the GPT module for context understanding.

Each section will be explained in detail as we progress further.

Questions

Inside a question object we add mapper for different possible values.

A questionnaire can have three types of questions 1. Radio button: User can select a single answer, 2. Checkbox: User can select multiple answers, 3. Free text: User can enter string.

To add mapper for a single value, use the value key as the key for related decision objects. This approach allows you to define how the system should handle or process that value.

Example

json { "skincare_history": { "q2": { "mapper": { "a": { "exclude": ["Retinol", "Retinol based products"] } } } } } In the above example, the key a represents answer selected by user. The object associated with a specifies decisions related to it—in this case, an exclude list. This list contains the values Retinol and Retinol based products, defined by the creator of the mapper. How this Exclude list is used is covered in a latter section below.

To add mapper for a combination of value (Answers with multiple selection), - Use | sign as or to combine more than one values for a decision in mapper. - Use & sign as and to combine more than one values for a decision in mapper. Example

json { "a|b|c": { "exclude": ["Retinol", "Retinol palmitate"] } }

To add mapper for free-text value, use any key to add the decision rule. This can be useful for taking age, allergy information etc. Example

json { "any": { "age": "" } }

When a decision need to be taken if a set of value is not selected we should use negative selection.

An example scenario.

What of the following skincare you follow every day?

a. Cleanser b. Moisturizer c. Spf d. Serum e. Toner f. Night cream g. Eye cream

Decision: if cleanser, moisturizer and spf is chosen, AI will consider all products

If one of the basic step(cleanser, moisturizer, spf) is not chosen, Ai will exclude Retinol, Retinol palmitate, tranexamic acid, vitamin C, vitamin C derivative , arbutin

Here we need to take a decision if cleanser, moisturizer, spf is not selected, this is a candidate for negative selection.

Rules: Use ~ as key to add negative selection decisions in a question mapper. Use value serial as usual inside the negative selection object to add decision if a value is not selected, e.g. "a": "decision" Example

json { "q1": { "mapper" : { "a": { "tags": ["acne", "anti-aging"] }, "~": { "a|b|c": { "exclude": ["Retinol", "Retinol palmitate", "tranexamic acid", "vitamin c", "vitamin c derivative" , "arbutin"] }, "d": { "tags": ["Retinol"] } } } } }

Exclusivity

The integration of exclusivity in the mapper is crucial for eliminating duplicates and addressing exclusivity prerequisites.

To incorporate exclusivity, you utilize the exclusive key to append an object in the following format:

"prompt_key-1" :  "prompt_key-2"


Doing so ensures that any value present in both prompt_key-1 and prompt_key-2, within the final prompt, will be extracted exclusively from prompt_key-2.

For instance: json { "exclusive": { "exclude": "includes", "exclude_tag": "include_tag" } }

Prompt

Each prompt section is a decision point. A prompt section has two constituting parts, the sentence and the variable. Let's say we have a decision point to exclude products that has certain tags. we can add a prompt section named exclude_tags as follows

json { "prompt_sections": { "exclude_tags": "Exclude any products that has following tags {exclude_tags}." } }

And We need to add the exclude_tags in mapper wherever this decision point need to be mapped. An example mapper, like we have discussed above, would be

json { "q1": { "mapper": { "a": { "exclude_tags": ["retinol", "retinol based products"] } } } }

An example prompt section for a cosmetics company could be.

json { "prompt_sections": { "age": "I am {age} years old.", "gender": "I am a {gender}.", "skin_type": "My skin type is {skin_type}.", "include_tag": "Show products that contain the tag: {include_tag}", "exclude_tag": "Exclude products that do contain the tag: {exclude_tag}" } }

Prompt Order

This section is to decide the order of the prompts in the prompt section above. This section must contain all the prompts from the above section. This is the order by which the prompts will be sent to GPT model. Note: This order effects the quality of the response. User should try different orders to find the best result.

An example prompt order from the prompt_sections sample above sample could be:

json { "prompt_order": [ "age", "gender", "skin_type", "include_tag", "exclude_tag" ] }

GPT Settings

This section is required for the context and various parameter setting for GPT. The prompt generated til this point above, goes to our GPT model, which is familiar with the item catalogue. This GPT model, in turn generates this recommendation for the user. Below is an example.

json { "gpt_settings": { "model": "gpt-3.5-turbo-0613", "search_max_token": 2500, "intro": "Using given context make recommendation. make recommendation of top {{ recommend_count }} products in json format and your response should denote the json part with three tick notations(). Use we as your pronoun. json object should have following format. {'comment': 'make an overall comment about your recommendations', 'advice':'...', 'recommendations': [{'id':'...', 'comment': 'why you chose this product'},{'id':'...', 'comment': 'why you chose this product'}]}. if the query includes specialist_pretext then include that suggestion in your 'advice'", "system": "You are an expert on ...", "temperature": 0, "top_p": 1, "frequency_penalty": 0, "presence_penalty": 0, "stop": [ "AI:", "Human:" ], "recommend_count": 3, "result_includes": [ "key_name_1", "key_name_2" ], "top_k": 20, "is_keyword_enabled": true } } ```

Integration of Questionnaire Recommendation

The questionnaire recommendation function processes user responses, identifies the corresponding questionnaire and decision points, and generates personalized recommendations. The endpoint POST /v1/questionnaire/recommend is designed for this purpose.

Detailed examples, required parameters, and explanations can be found in our sandbox.

This endpoint requires questionnaire_id as input. This identifier helps determine which questionnaire and mapping to consult, and which user response to consider, leading to the generation of personalized results.

You will notice the recommend_count parameter also exist in the mapper. If it is set, the value of this parameter takes priority over recommend_count variable in questionnaire mapper.

The response format of this endpoint can be customized using the previously mentioned GPT settings.

Reviewing Questionnaire Recommendation Logs

By utilizing the endpoint GET /v1/questionnaires/{questionnaire_id}/logs, you're able to access the log of requests dispatched to the v1/questionnaire/recommend endpoint, along with the corresponding generated responses.

Previous
Dynamic Pricing
Next
GPT Project setup
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/01_Integration/Questionnaire/#gpt-settings
Skip to content
Gigalogy Tutorial
Questionnaire
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Registration of Questionnaire
Registration of Questionnaire Mapper
Questions
Exclusivity
Prompt
Prompt Order
GPT Settings
Integration of Questionnaire Recommendation
Reviewing Questionnaire Recommendation Logs
Questionnaire

Our questionnaire software is specifically tailored to provide recommendations for novice and experienced users. Here is how it works:

Questionnaire Design: We meticulously design the questionnaire to encapsulate all necessary questions and decisions.
Questionnaire Registration: The questionnaire is seamlessly integrated into the project, enabling easy access and interaction.
User Response Collection: User responses are gathered through an interactive, user-friendly, UI-based questionnaire embedded in the website.
Response Processing: These responses are then processed by the GAIP system.
Product Recommendation: Finally, personalized product recommendations are suggested based on the processed responses.

Furthermore, the registration of the questionnaire into the project constitutes two crucial phases:

Registration of Questionnaire: Where the questionnaire is incorporated into the project.
Registration of Questionnaire Mapper: Which equips the AI system with the knowledge to accurately interpret decisions based on the users' responses to the questions.
Registration of Questionnaire

To manage the questionnaire, a series of RESTful API endpoints are provided which can be explored in our sandbox.

Creating a new questionnaire: Make a POST request to /v1/questionnaires to create a new questionnaire. The sandbox hosts detailed explanations and sample request bodies. The questionnaire can be divided into different sections, with at least one required.

Listing all questionnaires: Sending a GET request to /v1/questionnaires/list lists all the questionnaire's ID and names. A project can host multiple questionnaires that are differentiated through these IDs.

Viewing a questionnaire: Make a GET request to /v1/questionnaires/{questionnaire_id} to retrieve a specific questionnaire's details, including sections and questions.

Updating and deleting questionnaire: Use the PUT /v1/questionnaires/{questionnaire_id} and DELETE /v1/questionnaires/{questionnaire_id} endpoints to update or remove a questionnaire respectively.

Registration of Questionnaire Mapper

A questionnaire mapper is essential to instruct the system on making decisions based on user responses. Corresponding endpoints for creating, viewing, and editing a mapper are found at /v1/questionnaires/{questionnaire_id}/mapper.

To create a mapper, use POST /v1/questionnaires/{questionnaire_id}/mapper.

The mapper consists of five main components:

Questions: This section maps the decisions for all questionnaire questions.
Exclusivity: Handles duplicate removal and exclusivity requirements.
Prompt Generation: This section produces prompts that are used by the AI to generate recommendations.
Prompt Order: Sets the sequence in which prompts appear.
GPT settings: Provides settings and parameters to the GPT module for context understanding.

Each section will be explained in detail as we progress further.

Questions

Inside a question object we add mapper for different possible values.

A questionnaire can have three types of questions 1. Radio button: User can select a single answer, 2. Checkbox: User can select multiple answers, 3. Free text: User can enter string.

To add mapper for a single value, use the value key as the key for related decision objects. This approach allows you to define how the system should handle or process that value.

Example

json { "skincare_history": { "q2": { "mapper": { "a": { "exclude": ["Retinol", "Retinol based products"] } } } } } In the above example, the key a represents answer selected by user. The object associated with a specifies decisions related to it—in this case, an exclude list. This list contains the values Retinol and Retinol based products, defined by the creator of the mapper. How this Exclude list is used is covered in a latter section below.

To add mapper for a combination of value (Answers with multiple selection), - Use | sign as or to combine more than one values for a decision in mapper. - Use & sign as and to combine more than one values for a decision in mapper. Example

json { "a|b|c": { "exclude": ["Retinol", "Retinol palmitate"] } }

To add mapper for free-text value, use any key to add the decision rule. This can be useful for taking age, allergy information etc. Example

json { "any": { "age": "" } }

When a decision need to be taken if a set of value is not selected we should use negative selection.

An example scenario.

What of the following skincare you follow every day?

a. Cleanser b. Moisturizer c. Spf d. Serum e. Toner f. Night cream g. Eye cream

Decision: if cleanser, moisturizer and spf is chosen, AI will consider all products

If one of the basic step(cleanser, moisturizer, spf) is not chosen, Ai will exclude Retinol, Retinol palmitate, tranexamic acid, vitamin C, vitamin C derivative , arbutin

Here we need to take a decision if cleanser, moisturizer, spf is not selected, this is a candidate for negative selection.

Rules: Use ~ as key to add negative selection decisions in a question mapper. Use value serial as usual inside the negative selection object to add decision if a value is not selected, e.g. "a": "decision" Example

json { "q1": { "mapper" : { "a": { "tags": ["acne", "anti-aging"] }, "~": { "a|b|c": { "exclude": ["Retinol", "Retinol palmitate", "tranexamic acid", "vitamin c", "vitamin c derivative" , "arbutin"] }, "d": { "tags": ["Retinol"] } } } } }

Exclusivity

The integration of exclusivity in the mapper is crucial for eliminating duplicates and addressing exclusivity prerequisites.

To incorporate exclusivity, you utilize the exclusive key to append an object in the following format:

"prompt_key-1" :  "prompt_key-2"


Doing so ensures that any value present in both prompt_key-1 and prompt_key-2, within the final prompt, will be extracted exclusively from prompt_key-2.

For instance: json { "exclusive": { "exclude": "includes", "exclude_tag": "include_tag" } }

Prompt

Each prompt section is a decision point. A prompt section has two constituting parts, the sentence and the variable. Let's say we have a decision point to exclude products that has certain tags. we can add a prompt section named exclude_tags as follows

json { "prompt_sections": { "exclude_tags": "Exclude any products that has following tags {exclude_tags}." } }

And We need to add the exclude_tags in mapper wherever this decision point need to be mapped. An example mapper, like we have discussed above, would be

json { "q1": { "mapper": { "a": { "exclude_tags": ["retinol", "retinol based products"] } } } }

An example prompt section for a cosmetics company could be.

json { "prompt_sections": { "age": "I am {age} years old.", "gender": "I am a {gender}.", "skin_type": "My skin type is {skin_type}.", "include_tag": "Show products that contain the tag: {include_tag}", "exclude_tag": "Exclude products that do contain the tag: {exclude_tag}" } }

Prompt Order

This section is to decide the order of the prompts in the prompt section above. This section must contain all the prompts from the above section. This is the order by which the prompts will be sent to GPT model. Note: This order effects the quality of the response. User should try different orders to find the best result.

An example prompt order from the prompt_sections sample above sample could be:

json { "prompt_order": [ "age", "gender", "skin_type", "include_tag", "exclude_tag" ] }

GPT Settings

This section is required for the context and various parameter setting for GPT. The prompt generated til this point above, goes to our GPT model, which is familiar with the item catalogue. This GPT model, in turn generates this recommendation for the user. Below is an example.

json { "gpt_settings": { "model": "gpt-3.5-turbo-0613", "search_max_token": 2500, "intro": "Using given context make recommendation. make recommendation of top {{ recommend_count }} products in json format and your response should denote the json part with three tick notations(). Use we as your pronoun. json object should have following format. {'comment': 'make an overall comment about your recommendations', 'advice':'...', 'recommendations': [{'id':'...', 'comment': 'why you chose this product'},{'id':'...', 'comment': 'why you chose this product'}]}. if the query includes specialist_pretext then include that suggestion in your 'advice'", "system": "You are an expert on ...", "temperature": 0, "top_p": 1, "frequency_penalty": 0, "presence_penalty": 0, "stop": [ "AI:", "Human:" ], "recommend_count": 3, "result_includes": [ "key_name_1", "key_name_2" ], "top_k": 20, "is_keyword_enabled": true } } ```

Integration of Questionnaire Recommendation

The questionnaire recommendation function processes user responses, identifies the corresponding questionnaire and decision points, and generates personalized recommendations. The endpoint POST /v1/questionnaire/recommend is designed for this purpose.

Detailed examples, required parameters, and explanations can be found in our sandbox.

This endpoint requires questionnaire_id as input. This identifier helps determine which questionnaire and mapping to consult, and which user response to consider, leading to the generation of personalized results.

You will notice the recommend_count parameter also exist in the mapper. If it is set, the value of this parameter takes priority over recommend_count variable in questionnaire mapper.

The response format of this endpoint can be customized using the previously mentioned GPT settings.

Reviewing Questionnaire Recommendation Logs

By utilizing the endpoint GET /v1/questionnaires/{questionnaire_id}/logs, you're able to access the log of requests dispatched to the v1/questionnaire/recommend endpoint, along with the corresponding generated responses.

Previous
Dynamic Pricing
Next
GPT Project setup
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/01_Integration/Questionnaire/#integration-of-questionnaire-recommendation
Skip to content
Gigalogy Tutorial
Questionnaire
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Registration of Questionnaire
Registration of Questionnaire Mapper
Questions
Exclusivity
Prompt
Prompt Order
GPT Settings
Integration of Questionnaire Recommendation
Reviewing Questionnaire Recommendation Logs
Questionnaire

Our questionnaire software is specifically tailored to provide recommendations for novice and experienced users. Here is how it works:

Questionnaire Design: We meticulously design the questionnaire to encapsulate all necessary questions and decisions.
Questionnaire Registration: The questionnaire is seamlessly integrated into the project, enabling easy access and interaction.
User Response Collection: User responses are gathered through an interactive, user-friendly, UI-based questionnaire embedded in the website.
Response Processing: These responses are then processed by the GAIP system.
Product Recommendation: Finally, personalized product recommendations are suggested based on the processed responses.

Furthermore, the registration of the questionnaire into the project constitutes two crucial phases:

Registration of Questionnaire: Where the questionnaire is incorporated into the project.
Registration of Questionnaire Mapper: Which equips the AI system with the knowledge to accurately interpret decisions based on the users' responses to the questions.
Registration of Questionnaire

To manage the questionnaire, a series of RESTful API endpoints are provided which can be explored in our sandbox.

Creating a new questionnaire: Make a POST request to /v1/questionnaires to create a new questionnaire. The sandbox hosts detailed explanations and sample request bodies. The questionnaire can be divided into different sections, with at least one required.

Listing all questionnaires: Sending a GET request to /v1/questionnaires/list lists all the questionnaire's ID and names. A project can host multiple questionnaires that are differentiated through these IDs.

Viewing a questionnaire: Make a GET request to /v1/questionnaires/{questionnaire_id} to retrieve a specific questionnaire's details, including sections and questions.

Updating and deleting questionnaire: Use the PUT /v1/questionnaires/{questionnaire_id} and DELETE /v1/questionnaires/{questionnaire_id} endpoints to update or remove a questionnaire respectively.

Registration of Questionnaire Mapper

A questionnaire mapper is essential to instruct the system on making decisions based on user responses. Corresponding endpoints for creating, viewing, and editing a mapper are found at /v1/questionnaires/{questionnaire_id}/mapper.

To create a mapper, use POST /v1/questionnaires/{questionnaire_id}/mapper.

The mapper consists of five main components:

Questions: This section maps the decisions for all questionnaire questions.
Exclusivity: Handles duplicate removal and exclusivity requirements.
Prompt Generation: This section produces prompts that are used by the AI to generate recommendations.
Prompt Order: Sets the sequence in which prompts appear.
GPT settings: Provides settings and parameters to the GPT module for context understanding.

Each section will be explained in detail as we progress further.

Questions

Inside a question object we add mapper for different possible values.

A questionnaire can have three types of questions 1. Radio button: User can select a single answer, 2. Checkbox: User can select multiple answers, 3. Free text: User can enter string.

To add mapper for a single value, use the value key as the key for related decision objects. This approach allows you to define how the system should handle or process that value.

Example

json { "skincare_history": { "q2": { "mapper": { "a": { "exclude": ["Retinol", "Retinol based products"] } } } } } In the above example, the key a represents answer selected by user. The object associated with a specifies decisions related to it—in this case, an exclude list. This list contains the values Retinol and Retinol based products, defined by the creator of the mapper. How this Exclude list is used is covered in a latter section below.

To add mapper for a combination of value (Answers with multiple selection), - Use | sign as or to combine more than one values for a decision in mapper. - Use & sign as and to combine more than one values for a decision in mapper. Example

json { "a|b|c": { "exclude": ["Retinol", "Retinol palmitate"] } }

To add mapper for free-text value, use any key to add the decision rule. This can be useful for taking age, allergy information etc. Example

json { "any": { "age": "" } }

When a decision need to be taken if a set of value is not selected we should use negative selection.

An example scenario.

What of the following skincare you follow every day?

a. Cleanser b. Moisturizer c. Spf d. Serum e. Toner f. Night cream g. Eye cream

Decision: if cleanser, moisturizer and spf is chosen, AI will consider all products

If one of the basic step(cleanser, moisturizer, spf) is not chosen, Ai will exclude Retinol, Retinol palmitate, tranexamic acid, vitamin C, vitamin C derivative , arbutin

Here we need to take a decision if cleanser, moisturizer, spf is not selected, this is a candidate for negative selection.

Rules: Use ~ as key to add negative selection decisions in a question mapper. Use value serial as usual inside the negative selection object to add decision if a value is not selected, e.g. "a": "decision" Example

json { "q1": { "mapper" : { "a": { "tags": ["acne", "anti-aging"] }, "~": { "a|b|c": { "exclude": ["Retinol", "Retinol palmitate", "tranexamic acid", "vitamin c", "vitamin c derivative" , "arbutin"] }, "d": { "tags": ["Retinol"] } } } } }

Exclusivity

The integration of exclusivity in the mapper is crucial for eliminating duplicates and addressing exclusivity prerequisites.

To incorporate exclusivity, you utilize the exclusive key to append an object in the following format:

"prompt_key-1" :  "prompt_key-2"


Doing so ensures that any value present in both prompt_key-1 and prompt_key-2, within the final prompt, will be extracted exclusively from prompt_key-2.

For instance: json { "exclusive": { "exclude": "includes", "exclude_tag": "include_tag" } }

Prompt

Each prompt section is a decision point. A prompt section has two constituting parts, the sentence and the variable. Let's say we have a decision point to exclude products that has certain tags. we can add a prompt section named exclude_tags as follows

json { "prompt_sections": { "exclude_tags": "Exclude any products that has following tags {exclude_tags}." } }

And We need to add the exclude_tags in mapper wherever this decision point need to be mapped. An example mapper, like we have discussed above, would be

json { "q1": { "mapper": { "a": { "exclude_tags": ["retinol", "retinol based products"] } } } }

An example prompt section for a cosmetics company could be.

json { "prompt_sections": { "age": "I am {age} years old.", "gender": "I am a {gender}.", "skin_type": "My skin type is {skin_type}.", "include_tag": "Show products that contain the tag: {include_tag}", "exclude_tag": "Exclude products that do contain the tag: {exclude_tag}" } }

Prompt Order

This section is to decide the order of the prompts in the prompt section above. This section must contain all the prompts from the above section. This is the order by which the prompts will be sent to GPT model. Note: This order effects the quality of the response. User should try different orders to find the best result.

An example prompt order from the prompt_sections sample above sample could be:

json { "prompt_order": [ "age", "gender", "skin_type", "include_tag", "exclude_tag" ] }

GPT Settings

This section is required for the context and various parameter setting for GPT. The prompt generated til this point above, goes to our GPT model, which is familiar with the item catalogue. This GPT model, in turn generates this recommendation for the user. Below is an example.

json { "gpt_settings": { "model": "gpt-3.5-turbo-0613", "search_max_token": 2500, "intro": "Using given context make recommendation. make recommendation of top {{ recommend_count }} products in json format and your response should denote the json part with three tick notations(). Use we as your pronoun. json object should have following format. {'comment': 'make an overall comment about your recommendations', 'advice':'...', 'recommendations': [{'id':'...', 'comment': 'why you chose this product'},{'id':'...', 'comment': 'why you chose this product'}]}. if the query includes specialist_pretext then include that suggestion in your 'advice'", "system": "You are an expert on ...", "temperature": 0, "top_p": 1, "frequency_penalty": 0, "presence_penalty": 0, "stop": [ "AI:", "Human:" ], "recommend_count": 3, "result_includes": [ "key_name_1", "key_name_2" ], "top_k": 20, "is_keyword_enabled": true } } ```

Integration of Questionnaire Recommendation

The questionnaire recommendation function processes user responses, identifies the corresponding questionnaire and decision points, and generates personalized recommendations. The endpoint POST /v1/questionnaire/recommend is designed for this purpose.

Detailed examples, required parameters, and explanations can be found in our sandbox.

This endpoint requires questionnaire_id as input. This identifier helps determine which questionnaire and mapping to consult, and which user response to consider, leading to the generation of personalized results.

You will notice the recommend_count parameter also exist in the mapper. If it is set, the value of this parameter takes priority over recommend_count variable in questionnaire mapper.

The response format of this endpoint can be customized using the previously mentioned GPT settings.

Reviewing Questionnaire Recommendation Logs

By utilizing the endpoint GET /v1/questionnaires/{questionnaire_id}/logs, you're able to access the log of requests dispatched to the v1/questionnaire/recommend endpoint, along with the corresponding generated responses.

Previous
Dynamic Pricing
Next
GPT Project setup
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/01_Integration/Questionnaire/#reviewing-questionnaire-recommendation-logs
Skip to content
Gigalogy Tutorial
Questionnaire
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Registration of Questionnaire
Registration of Questionnaire Mapper
Questions
Exclusivity
Prompt
Prompt Order
GPT Settings
Integration of Questionnaire Recommendation
Reviewing Questionnaire Recommendation Logs
Questionnaire

Our questionnaire software is specifically tailored to provide recommendations for novice and experienced users. Here is how it works:

Questionnaire Design: We meticulously design the questionnaire to encapsulate all necessary questions and decisions.
Questionnaire Registration: The questionnaire is seamlessly integrated into the project, enabling easy access and interaction.
User Response Collection: User responses are gathered through an interactive, user-friendly, UI-based questionnaire embedded in the website.
Response Processing: These responses are then processed by the GAIP system.
Product Recommendation: Finally, personalized product recommendations are suggested based on the processed responses.

Furthermore, the registration of the questionnaire into the project constitutes two crucial phases:

Registration of Questionnaire: Where the questionnaire is incorporated into the project.
Registration of Questionnaire Mapper: Which equips the AI system with the knowledge to accurately interpret decisions based on the users' responses to the questions.
Registration of Questionnaire

To manage the questionnaire, a series of RESTful API endpoints are provided which can be explored in our sandbox.

Creating a new questionnaire: Make a POST request to /v1/questionnaires to create a new questionnaire. The sandbox hosts detailed explanations and sample request bodies. The questionnaire can be divided into different sections, with at least one required.

Listing all questionnaires: Sending a GET request to /v1/questionnaires/list lists all the questionnaire's ID and names. A project can host multiple questionnaires that are differentiated through these IDs.

Viewing a questionnaire: Make a GET request to /v1/questionnaires/{questionnaire_id} to retrieve a specific questionnaire's details, including sections and questions.

Updating and deleting questionnaire: Use the PUT /v1/questionnaires/{questionnaire_id} and DELETE /v1/questionnaires/{questionnaire_id} endpoints to update or remove a questionnaire respectively.

Registration of Questionnaire Mapper

A questionnaire mapper is essential to instruct the system on making decisions based on user responses. Corresponding endpoints for creating, viewing, and editing a mapper are found at /v1/questionnaires/{questionnaire_id}/mapper.

To create a mapper, use POST /v1/questionnaires/{questionnaire_id}/mapper.

The mapper consists of five main components:

Questions: This section maps the decisions for all questionnaire questions.
Exclusivity: Handles duplicate removal and exclusivity requirements.
Prompt Generation: This section produces prompts that are used by the AI to generate recommendations.
Prompt Order: Sets the sequence in which prompts appear.
GPT settings: Provides settings and parameters to the GPT module for context understanding.

Each section will be explained in detail as we progress further.

Questions

Inside a question object we add mapper for different possible values.

A questionnaire can have three types of questions 1. Radio button: User can select a single answer, 2. Checkbox: User can select multiple answers, 3. Free text: User can enter string.

To add mapper for a single value, use the value key as the key for related decision objects. This approach allows you to define how the system should handle or process that value.

Example

json { "skincare_history": { "q2": { "mapper": { "a": { "exclude": ["Retinol", "Retinol based products"] } } } } } In the above example, the key a represents answer selected by user. The object associated with a specifies decisions related to it—in this case, an exclude list. This list contains the values Retinol and Retinol based products, defined by the creator of the mapper. How this Exclude list is used is covered in a latter section below.

To add mapper for a combination of value (Answers with multiple selection), - Use | sign as or to combine more than one values for a decision in mapper. - Use & sign as and to combine more than one values for a decision in mapper. Example

json { "a|b|c": { "exclude": ["Retinol", "Retinol palmitate"] } }

To add mapper for free-text value, use any key to add the decision rule. This can be useful for taking age, allergy information etc. Example

json { "any": { "age": "" } }

When a decision need to be taken if a set of value is not selected we should use negative selection.

An example scenario.

What of the following skincare you follow every day?

a. Cleanser b. Moisturizer c. Spf d. Serum e. Toner f. Night cream g. Eye cream

Decision: if cleanser, moisturizer and spf is chosen, AI will consider all products

If one of the basic step(cleanser, moisturizer, spf) is not chosen, Ai will exclude Retinol, Retinol palmitate, tranexamic acid, vitamin C, vitamin C derivative , arbutin

Here we need to take a decision if cleanser, moisturizer, spf is not selected, this is a candidate for negative selection.

Rules: Use ~ as key to add negative selection decisions in a question mapper. Use value serial as usual inside the negative selection object to add decision if a value is not selected, e.g. "a": "decision" Example

json { "q1": { "mapper" : { "a": { "tags": ["acne", "anti-aging"] }, "~": { "a|b|c": { "exclude": ["Retinol", "Retinol palmitate", "tranexamic acid", "vitamin c", "vitamin c derivative" , "arbutin"] }, "d": { "tags": ["Retinol"] } } } } }

Exclusivity

The integration of exclusivity in the mapper is crucial for eliminating duplicates and addressing exclusivity prerequisites.

To incorporate exclusivity, you utilize the exclusive key to append an object in the following format:

"prompt_key-1" :  "prompt_key-2"


Doing so ensures that any value present in both prompt_key-1 and prompt_key-2, within the final prompt, will be extracted exclusively from prompt_key-2.

For instance: json { "exclusive": { "exclude": "includes", "exclude_tag": "include_tag" } }

Prompt

Each prompt section is a decision point. A prompt section has two constituting parts, the sentence and the variable. Let's say we have a decision point to exclude products that has certain tags. we can add a prompt section named exclude_tags as follows

json { "prompt_sections": { "exclude_tags": "Exclude any products that has following tags {exclude_tags}." } }

And We need to add the exclude_tags in mapper wherever this decision point need to be mapped. An example mapper, like we have discussed above, would be

json { "q1": { "mapper": { "a": { "exclude_tags": ["retinol", "retinol based products"] } } } }

An example prompt section for a cosmetics company could be.

json { "prompt_sections": { "age": "I am {age} years old.", "gender": "I am a {gender}.", "skin_type": "My skin type is {skin_type}.", "include_tag": "Show products that contain the tag: {include_tag}", "exclude_tag": "Exclude products that do contain the tag: {exclude_tag}" } }

Prompt Order

This section is to decide the order of the prompts in the prompt section above. This section must contain all the prompts from the above section. This is the order by which the prompts will be sent to GPT model. Note: This order effects the quality of the response. User should try different orders to find the best result.

An example prompt order from the prompt_sections sample above sample could be:

json { "prompt_order": [ "age", "gender", "skin_type", "include_tag", "exclude_tag" ] }

GPT Settings

This section is required for the context and various parameter setting for GPT. The prompt generated til this point above, goes to our GPT model, which is familiar with the item catalogue. This GPT model, in turn generates this recommendation for the user. Below is an example.

json { "gpt_settings": { "model": "gpt-3.5-turbo-0613", "search_max_token": 2500, "intro": "Using given context make recommendation. make recommendation of top {{ recommend_count }} products in json format and your response should denote the json part with three tick notations(). Use we as your pronoun. json object should have following format. {'comment': 'make an overall comment about your recommendations', 'advice':'...', 'recommendations': [{'id':'...', 'comment': 'why you chose this product'},{'id':'...', 'comment': 'why you chose this product'}]}. if the query includes specialist_pretext then include that suggestion in your 'advice'", "system": "You are an expert on ...", "temperature": 0, "top_p": 1, "frequency_penalty": 0, "presence_penalty": 0, "stop": [ "AI:", "Human:" ], "recommend_count": 3, "result_includes": [ "key_name_1", "key_name_2" ], "top_k": 20, "is_keyword_enabled": true } } ```

Integration of Questionnaire Recommendation

The questionnaire recommendation function processes user responses, identifies the corresponding questionnaire and decision points, and generates personalized recommendations. The endpoint POST /v1/questionnaire/recommend is designed for this purpose.

Detailed examples, required parameters, and explanations can be found in our sandbox.

This endpoint requires questionnaire_id as input. This identifier helps determine which questionnaire and mapping to consult, and which user response to consider, leading to the generation of personalized results.

You will notice the recommend_count parameter also exist in the mapper. If it is set, the value of this parameter takes priority over recommend_count variable in questionnaire mapper.

The response format of this endpoint can be customized using the previously mentioned GPT settings.

Reviewing Questionnaire Recommendation Logs

By utilizing the endpoint GET /v1/questionnaires/{questionnaire_id}/logs, you're able to access the log of requests dispatched to the v1/questionnaire/recommend endpoint, along with the corresponding generated responses.

Previous
Dynamic Pricing
Next
GPT Project setup
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/02_gpt_integration/gpt_project_setup/
Skip to content
Gigalogy Tutorial
GPT Project setup
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
GPT Project setup
GPT Integration
GPT Feedback
API Reference
Release notes
Glossary
Table of contents
Documents
Datasets
Profiles
Project setup
Dataset
Upload Dataset
How to see uploaded datasets
Delete dataset
Update Dataset
Updating and deleting - Documents
Training
Profile
Setup profile
Default profile setup
GPT Project setup

Gigalogy,s GPT based solutions allows you to make your own GPT solutions, trained with your own data, customized according to your needs. Here are some basics to get you started. Find MyGPT related endpoints in our sandbox.

Documents

Documents are information that GPT will consider as a single piece of information, such as Address of Gigalogy, What is Gigalogy personalization, etc.

Datasets

A Dataset is a collection of documents in a single file. For instance, single dataset may contain documents with information such as Address of Gigalogy, What is Gigalogy personalization, What is Gigalogy's MyGPT.

Profiles

With each request sent to GPT endpoints POST /v1/gpt/ask and POST /v1/gpt/ask/vision, we include a parameter called gpt_profile_id. This parameter's value points to a GPT profile. GPT profiles hold information that tells the GPT how to process the information provided (query) and how to respond. To see more about what is inside a profile, check out the parameters and the example request body of the endpoint POST /v1/gpt/profiles.

There are two types of profiles. One is for the /gpt/ask endpoint, which is a general profile for any model except gpt-4-vision-preview. The other is for the /gpt/ask/vision endpoint, for which we currently support only gpt-4-vision-preview as the model.

Project setup

Project setup for MyGPT involves preparing, uploading and training your data. Additionally, set up the required setting the parameters to suit your requirement.

Dataset
Upload Dataset

Use the endpoint POST /v1/gpt/datasets to upload a dataset that will be used to train your customized GPT bot. Currently, we accept CSV and JSON format. You will find the required parameters and description in the sandbox in the link above.

How to see uploaded datasets

Once the dataset is uploaded, you can use /v1/gpt/datasets to see all your datasets of your project. The response will give you below details along with the datasets ids. This dataset_id will be required to edit, delete, train your data.

yaml { "dataset_id": "a8bf8ddd-b5cb-4bea-a82b-4ac148f01c0a", "created_at": "2023-12-24T20:23:34.992063+09:00", "name": "NAME OF THE DATASET", "description": DESCRIPTION OF THE DATASET, "idx_column_name": "idx", "image_url_column": "images" }

Delete dataset

Use the endpoint DELETE /v1/gpt/datasets/{dataset_id} to delete a dataset or particular documents from a dataset. You can find the expected request body, with required parameters and values in our sandbox here

Update Dataset

To be updated

Updating and deleting - Documents

To be updated

Training

Use endpoint POST /v1/gpt/dataset/train to train your uploaded dataset. This endpoint will take the dataset id and image type. It is good practice to train only what is necessary to optimize the usage of resources.

Profile
Setup profile

Use POST /v1/gpt/profiles to setup GPT profile. You can setup multiple profiles. However you will need to select one as the default profile in the next step.

Here are some detail about "intro", "System" and "Model" parameters.

Intro: This is detailed instruction for the bot on how to respond to a query. For example, if you are creating a customer support bot for an E-commerce site "AAA", this parameter could be: "Answer customer questions based on the catalog information and FAQ documents of our company. If a customer wants a product recommendation, ask up to three questions to understand their needs better, then make three recommendations and ask the customer if they like them or want different recommendations."

System: This tells the bot about the persona it is supposed to adopt. For example, if you are creating a customer support bot for an E-commerce site "AAA", selling cosmetics, this parameter could be: "You are customer support for AAA, an expert on skincare and cosmetic products who values customer needs and provides the right product recommendations."

Model: We support all GPT models of OpenAI, which you can select based on the needs and use cases. Please consider the purpose and the estimated token count when selecting the model, as this can significantly impact costs. You can learn more about OpenAI models from this page. This setting will impact the parameters search_max_token (tokens allocated for data sent to the model) and completion_token (tokens allocated for the reply). Note that intro, system, and query have token costs that are not included in the token size allocation. The selected model's CONTEXT WINDOW should cover the total token allocation. That is CONTEXT WINDOW ≥ search_max_token + completion_token + intro + system + query.

You can use the use the endpoints in our sandbox under "MyGPT Profiles" to Check the profiles and update or delete them.

Default profile setup

Once you have setup the profile(s), decide one of the profiles that should be default and use our endpoint POST /v1/gpt/settings to set your default profile.

with this, your GPT setup is completed.

Previous
Questionnaire
Next
GPT Integration
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/02_gpt_integration/gpt_project_setup/#documents
Skip to content
Gigalogy Tutorial
GPT Project setup
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
GPT Project setup
GPT Integration
GPT Feedback
API Reference
Release notes
Glossary
Table of contents
Documents
Datasets
Profiles
Project setup
Dataset
Upload Dataset
How to see uploaded datasets
Delete dataset
Update Dataset
Updating and deleting - Documents
Training
Profile
Setup profile
Default profile setup
GPT Project setup

Gigalogy,s GPT based solutions allows you to make your own GPT solutions, trained with your own data, customized according to your needs. Here are some basics to get you started. Find MyGPT related endpoints in our sandbox.

Documents

Documents are information that GPT will consider as a single piece of information, such as Address of Gigalogy, What is Gigalogy personalization, etc.

Datasets

A Dataset is a collection of documents in a single file. For instance, single dataset may contain documents with information such as Address of Gigalogy, What is Gigalogy personalization, What is Gigalogy's MyGPT.

Profiles

With each request sent to GPT endpoints POST /v1/gpt/ask and POST /v1/gpt/ask/vision, we include a parameter called gpt_profile_id. This parameter's value points to a GPT profile. GPT profiles hold information that tells the GPT how to process the information provided (query) and how to respond. To see more about what is inside a profile, check out the parameters and the example request body of the endpoint POST /v1/gpt/profiles.

There are two types of profiles. One is for the /gpt/ask endpoint, which is a general profile for any model except gpt-4-vision-preview. The other is for the /gpt/ask/vision endpoint, for which we currently support only gpt-4-vision-preview as the model.

Project setup

Project setup for MyGPT involves preparing, uploading and training your data. Additionally, set up the required setting the parameters to suit your requirement.

Dataset
Upload Dataset

Use the endpoint POST /v1/gpt/datasets to upload a dataset that will be used to train your customized GPT bot. Currently, we accept CSV and JSON format. You will find the required parameters and description in the sandbox in the link above.

How to see uploaded datasets

Once the dataset is uploaded, you can use /v1/gpt/datasets to see all your datasets of your project. The response will give you below details along with the datasets ids. This dataset_id will be required to edit, delete, train your data.

yaml { "dataset_id": "a8bf8ddd-b5cb-4bea-a82b-4ac148f01c0a", "created_at": "2023-12-24T20:23:34.992063+09:00", "name": "NAME OF THE DATASET", "description": DESCRIPTION OF THE DATASET, "idx_column_name": "idx", "image_url_column": "images" }

Delete dataset

Use the endpoint DELETE /v1/gpt/datasets/{dataset_id} to delete a dataset or particular documents from a dataset. You can find the expected request body, with required parameters and values in our sandbox here

Update Dataset

To be updated

Updating and deleting - Documents

To be updated

Training

Use endpoint POST /v1/gpt/dataset/train to train your uploaded dataset. This endpoint will take the dataset id and image type. It is good practice to train only what is necessary to optimize the usage of resources.

Profile
Setup profile

Use POST /v1/gpt/profiles to setup GPT profile. You can setup multiple profiles. However you will need to select one as the default profile in the next step.

Here are some detail about "intro", "System" and "Model" parameters.

Intro: This is detailed instruction for the bot on how to respond to a query. For example, if you are creating a customer support bot for an E-commerce site "AAA", this parameter could be: "Answer customer questions based on the catalog information and FAQ documents of our company. If a customer wants a product recommendation, ask up to three questions to understand their needs better, then make three recommendations and ask the customer if they like them or want different recommendations."

System: This tells the bot about the persona it is supposed to adopt. For example, if you are creating a customer support bot for an E-commerce site "AAA", selling cosmetics, this parameter could be: "You are customer support for AAA, an expert on skincare and cosmetic products who values customer needs and provides the right product recommendations."

Model: We support all GPT models of OpenAI, which you can select based on the needs and use cases. Please consider the purpose and the estimated token count when selecting the model, as this can significantly impact costs. You can learn more about OpenAI models from this page. This setting will impact the parameters search_max_token (tokens allocated for data sent to the model) and completion_token (tokens allocated for the reply). Note that intro, system, and query have token costs that are not included in the token size allocation. The selected model's CONTEXT WINDOW should cover the total token allocation. That is CONTEXT WINDOW ≥ search_max_token + completion_token + intro + system + query.

You can use the use the endpoints in our sandbox under "MyGPT Profiles" to Check the profiles and update or delete them.

Default profile setup

Once you have setup the profile(s), decide one of the profiles that should be default and use our endpoint POST /v1/gpt/settings to set your default profile.

with this, your GPT setup is completed.

Previous
Questionnaire
Next
GPT Integration
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/02_gpt_integration/gpt_project_setup/#datasets
Skip to content
Gigalogy Tutorial
GPT Project setup
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
GPT Project setup
GPT Integration
GPT Feedback
API Reference
Release notes
Glossary
Table of contents
Documents
Datasets
Profiles
Project setup
Dataset
Upload Dataset
How to see uploaded datasets
Delete dataset
Update Dataset
Updating and deleting - Documents
Training
Profile
Setup profile
Default profile setup
GPT Project setup

Gigalogy,s GPT based solutions allows you to make your own GPT solutions, trained with your own data, customized according to your needs. Here are some basics to get you started. Find MyGPT related endpoints in our sandbox.

Documents

Documents are information that GPT will consider as a single piece of information, such as Address of Gigalogy, What is Gigalogy personalization, etc.

Datasets

A Dataset is a collection of documents in a single file. For instance, single dataset may contain documents with information such as Address of Gigalogy, What is Gigalogy personalization, What is Gigalogy's MyGPT.

Profiles

With each request sent to GPT endpoints POST /v1/gpt/ask and POST /v1/gpt/ask/vision, we include a parameter called gpt_profile_id. This parameter's value points to a GPT profile. GPT profiles hold information that tells the GPT how to process the information provided (query) and how to respond. To see more about what is inside a profile, check out the parameters and the example request body of the endpoint POST /v1/gpt/profiles.

There are two types of profiles. One is for the /gpt/ask endpoint, which is a general profile for any model except gpt-4-vision-preview. The other is for the /gpt/ask/vision endpoint, for which we currently support only gpt-4-vision-preview as the model.

Project setup

Project setup for MyGPT involves preparing, uploading and training your data. Additionally, set up the required setting the parameters to suit your requirement.

Dataset
Upload Dataset

Use the endpoint POST /v1/gpt/datasets to upload a dataset that will be used to train your customized GPT bot. Currently, we accept CSV and JSON format. You will find the required parameters and description in the sandbox in the link above.

How to see uploaded datasets

Once the dataset is uploaded, you can use /v1/gpt/datasets to see all your datasets of your project. The response will give you below details along with the datasets ids. This dataset_id will be required to edit, delete, train your data.

yaml { "dataset_id": "a8bf8ddd-b5cb-4bea-a82b-4ac148f01c0a", "created_at": "2023-12-24T20:23:34.992063+09:00", "name": "NAME OF THE DATASET", "description": DESCRIPTION OF THE DATASET, "idx_column_name": "idx", "image_url_column": "images" }

Delete dataset

Use the endpoint DELETE /v1/gpt/datasets/{dataset_id} to delete a dataset or particular documents from a dataset. You can find the expected request body, with required parameters and values in our sandbox here

Update Dataset

To be updated

Updating and deleting - Documents

To be updated

Training

Use endpoint POST /v1/gpt/dataset/train to train your uploaded dataset. This endpoint will take the dataset id and image type. It is good practice to train only what is necessary to optimize the usage of resources.

Profile
Setup profile

Use POST /v1/gpt/profiles to setup GPT profile. You can setup multiple profiles. However you will need to select one as the default profile in the next step.

Here are some detail about "intro", "System" and "Model" parameters.

Intro: This is detailed instruction for the bot on how to respond to a query. For example, if you are creating a customer support bot for an E-commerce site "AAA", this parameter could be: "Answer customer questions based on the catalog information and FAQ documents of our company. If a customer wants a product recommendation, ask up to three questions to understand their needs better, then make three recommendations and ask the customer if they like them or want different recommendations."

System: This tells the bot about the persona it is supposed to adopt. For example, if you are creating a customer support bot for an E-commerce site "AAA", selling cosmetics, this parameter could be: "You are customer support for AAA, an expert on skincare and cosmetic products who values customer needs and provides the right product recommendations."

Model: We support all GPT models of OpenAI, which you can select based on the needs and use cases. Please consider the purpose and the estimated token count when selecting the model, as this can significantly impact costs. You can learn more about OpenAI models from this page. This setting will impact the parameters search_max_token (tokens allocated for data sent to the model) and completion_token (tokens allocated for the reply). Note that intro, system, and query have token costs that are not included in the token size allocation. The selected model's CONTEXT WINDOW should cover the total token allocation. That is CONTEXT WINDOW ≥ search_max_token + completion_token + intro + system + query.

You can use the use the endpoints in our sandbox under "MyGPT Profiles" to Check the profiles and update or delete them.

Default profile setup

Once you have setup the profile(s), decide one of the profiles that should be default and use our endpoint POST /v1/gpt/settings to set your default profile.

with this, your GPT setup is completed.

Previous
Questionnaire
Next
GPT Integration
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/02_gpt_integration/gpt_project_setup/#profiles
Skip to content
Gigalogy Tutorial
GPT Project setup
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
GPT Project setup
GPT Integration
GPT Feedback
API Reference
Release notes
Glossary
Table of contents
Documents
Datasets
Profiles
Project setup
Dataset
Upload Dataset
How to see uploaded datasets
Delete dataset
Update Dataset
Updating and deleting - Documents
Training
Profile
Setup profile
Default profile setup
GPT Project setup

Gigalogy,s GPT based solutions allows you to make your own GPT solutions, trained with your own data, customized according to your needs. Here are some basics to get you started. Find MyGPT related endpoints in our sandbox.

Documents

Documents are information that GPT will consider as a single piece of information, such as Address of Gigalogy, What is Gigalogy personalization, etc.

Datasets

A Dataset is a collection of documents in a single file. For instance, single dataset may contain documents with information such as Address of Gigalogy, What is Gigalogy personalization, What is Gigalogy's MyGPT.

Profiles

With each request sent to GPT endpoints POST /v1/gpt/ask and POST /v1/gpt/ask/vision, we include a parameter called gpt_profile_id. This parameter's value points to a GPT profile. GPT profiles hold information that tells the GPT how to process the information provided (query) and how to respond. To see more about what is inside a profile, check out the parameters and the example request body of the endpoint POST /v1/gpt/profiles.

There are two types of profiles. One is for the /gpt/ask endpoint, which is a general profile for any model except gpt-4-vision-preview. The other is for the /gpt/ask/vision endpoint, for which we currently support only gpt-4-vision-preview as the model.

Project setup

Project setup for MyGPT involves preparing, uploading and training your data. Additionally, set up the required setting the parameters to suit your requirement.

Dataset
Upload Dataset

Use the endpoint POST /v1/gpt/datasets to upload a dataset that will be used to train your customized GPT bot. Currently, we accept CSV and JSON format. You will find the required parameters and description in the sandbox in the link above.

How to see uploaded datasets

Once the dataset is uploaded, you can use /v1/gpt/datasets to see all your datasets of your project. The response will give you below details along with the datasets ids. This dataset_id will be required to edit, delete, train your data.

yaml { "dataset_id": "a8bf8ddd-b5cb-4bea-a82b-4ac148f01c0a", "created_at": "2023-12-24T20:23:34.992063+09:00", "name": "NAME OF THE DATASET", "description": DESCRIPTION OF THE DATASET, "idx_column_name": "idx", "image_url_column": "images" }

Delete dataset

Use the endpoint DELETE /v1/gpt/datasets/{dataset_id} to delete a dataset or particular documents from a dataset. You can find the expected request body, with required parameters and values in our sandbox here

Update Dataset

To be updated

Updating and deleting - Documents

To be updated

Training

Use endpoint POST /v1/gpt/dataset/train to train your uploaded dataset. This endpoint will take the dataset id and image type. It is good practice to train only what is necessary to optimize the usage of resources.

Profile
Setup profile

Use POST /v1/gpt/profiles to setup GPT profile. You can setup multiple profiles. However you will need to select one as the default profile in the next step.

Here are some detail about "intro", "System" and "Model" parameters.

Intro: This is detailed instruction for the bot on how to respond to a query. For example, if you are creating a customer support bot for an E-commerce site "AAA", this parameter could be: "Answer customer questions based on the catalog information and FAQ documents of our company. If a customer wants a product recommendation, ask up to three questions to understand their needs better, then make three recommendations and ask the customer if they like them or want different recommendations."

System: This tells the bot about the persona it is supposed to adopt. For example, if you are creating a customer support bot for an E-commerce site "AAA", selling cosmetics, this parameter could be: "You are customer support for AAA, an expert on skincare and cosmetic products who values customer needs and provides the right product recommendations."

Model: We support all GPT models of OpenAI, which you can select based on the needs and use cases. Please consider the purpose and the estimated token count when selecting the model, as this can significantly impact costs. You can learn more about OpenAI models from this page. This setting will impact the parameters search_max_token (tokens allocated for data sent to the model) and completion_token (tokens allocated for the reply). Note that intro, system, and query have token costs that are not included in the token size allocation. The selected model's CONTEXT WINDOW should cover the total token allocation. That is CONTEXT WINDOW ≥ search_max_token + completion_token + intro + system + query.

You can use the use the endpoints in our sandbox under "MyGPT Profiles" to Check the profiles and update or delete them.

Default profile setup

Once you have setup the profile(s), decide one of the profiles that should be default and use our endpoint POST /v1/gpt/settings to set your default profile.

with this, your GPT setup is completed.

Previous
Questionnaire
Next
GPT Integration
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/02_gpt_integration/gpt_project_setup/#project-setup
Skip to content
Gigalogy Tutorial
GPT Project setup
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
GPT Project setup
GPT Integration
GPT Feedback
API Reference
Release notes
Glossary
Table of contents
Documents
Datasets
Profiles
Project setup
Dataset
Upload Dataset
How to see uploaded datasets
Delete dataset
Update Dataset
Updating and deleting - Documents
Training
Profile
Setup profile
Default profile setup
GPT Project setup

Gigalogy,s GPT based solutions allows you to make your own GPT solutions, trained with your own data, customized according to your needs. Here are some basics to get you started. Find MyGPT related endpoints in our sandbox.

Documents

Documents are information that GPT will consider as a single piece of information, such as Address of Gigalogy, What is Gigalogy personalization, etc.

Datasets

A Dataset is a collection of documents in a single file. For instance, single dataset may contain documents with information such as Address of Gigalogy, What is Gigalogy personalization, What is Gigalogy's MyGPT.

Profiles

With each request sent to GPT endpoints POST /v1/gpt/ask and POST /v1/gpt/ask/vision, we include a parameter called gpt_profile_id. This parameter's value points to a GPT profile. GPT profiles hold information that tells the GPT how to process the information provided (query) and how to respond. To see more about what is inside a profile, check out the parameters and the example request body of the endpoint POST /v1/gpt/profiles.

There are two types of profiles. One is for the /gpt/ask endpoint, which is a general profile for any model except gpt-4-vision-preview. The other is for the /gpt/ask/vision endpoint, for which we currently support only gpt-4-vision-preview as the model.

Project setup

Project setup for MyGPT involves preparing, uploading and training your data. Additionally, set up the required setting the parameters to suit your requirement.

Dataset
Upload Dataset

Use the endpoint POST /v1/gpt/datasets to upload a dataset that will be used to train your customized GPT bot. Currently, we accept CSV and JSON format. You will find the required parameters and description in the sandbox in the link above.

How to see uploaded datasets

Once the dataset is uploaded, you can use /v1/gpt/datasets to see all your datasets of your project. The response will give you below details along with the datasets ids. This dataset_id will be required to edit, delete, train your data.

yaml { "dataset_id": "a8bf8ddd-b5cb-4bea-a82b-4ac148f01c0a", "created_at": "2023-12-24T20:23:34.992063+09:00", "name": "NAME OF THE DATASET", "description": DESCRIPTION OF THE DATASET, "idx_column_name": "idx", "image_url_column": "images" }

Delete dataset

Use the endpoint DELETE /v1/gpt/datasets/{dataset_id} to delete a dataset or particular documents from a dataset. You can find the expected request body, with required parameters and values in our sandbox here

Update Dataset

To be updated

Updating and deleting - Documents

To be updated

Training

Use endpoint POST /v1/gpt/dataset/train to train your uploaded dataset. This endpoint will take the dataset id and image type. It is good practice to train only what is necessary to optimize the usage of resources.

Profile
Setup profile

Use POST /v1/gpt/profiles to setup GPT profile. You can setup multiple profiles. However you will need to select one as the default profile in the next step.

Here are some detail about "intro", "System" and "Model" parameters.

Intro: This is detailed instruction for the bot on how to respond to a query. For example, if you are creating a customer support bot for an E-commerce site "AAA", this parameter could be: "Answer customer questions based on the catalog information and FAQ documents of our company. If a customer wants a product recommendation, ask up to three questions to understand their needs better, then make three recommendations and ask the customer if they like them or want different recommendations."

System: This tells the bot about the persona it is supposed to adopt. For example, if you are creating a customer support bot for an E-commerce site "AAA", selling cosmetics, this parameter could be: "You are customer support for AAA, an expert on skincare and cosmetic products who values customer needs and provides the right product recommendations."

Model: We support all GPT models of OpenAI, which you can select based on the needs and use cases. Please consider the purpose and the estimated token count when selecting the model, as this can significantly impact costs. You can learn more about OpenAI models from this page. This setting will impact the parameters search_max_token (tokens allocated for data sent to the model) and completion_token (tokens allocated for the reply). Note that intro, system, and query have token costs that are not included in the token size allocation. The selected model's CONTEXT WINDOW should cover the total token allocation. That is CONTEXT WINDOW ≥ search_max_token + completion_token + intro + system + query.

You can use the use the endpoints in our sandbox under "MyGPT Profiles" to Check the profiles and update or delete them.

Default profile setup

Once you have setup the profile(s), decide one of the profiles that should be default and use our endpoint POST /v1/gpt/settings to set your default profile.

with this, your GPT setup is completed.

Previous
Questionnaire
Next
GPT Integration
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/02_gpt_integration/gpt_project_setup/#dataset
Skip to content
Gigalogy Tutorial
GPT Project setup
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
GPT Project setup
GPT Integration
GPT Feedback
API Reference
Release notes
Glossary
Table of contents
Documents
Datasets
Profiles
Project setup
Dataset
Upload Dataset
How to see uploaded datasets
Delete dataset
Update Dataset
Updating and deleting - Documents
Training
Profile
Setup profile
Default profile setup
GPT Project setup

Gigalogy,s GPT based solutions allows you to make your own GPT solutions, trained with your own data, customized according to your needs. Here are some basics to get you started. Find MyGPT related endpoints in our sandbox.

Documents

Documents are information that GPT will consider as a single piece of information, such as Address of Gigalogy, What is Gigalogy personalization, etc.

Datasets

A Dataset is a collection of documents in a single file. For instance, single dataset may contain documents with information such as Address of Gigalogy, What is Gigalogy personalization, What is Gigalogy's MyGPT.

Profiles

With each request sent to GPT endpoints POST /v1/gpt/ask and POST /v1/gpt/ask/vision, we include a parameter called gpt_profile_id. This parameter's value points to a GPT profile. GPT profiles hold information that tells the GPT how to process the information provided (query) and how to respond. To see more about what is inside a profile, check out the parameters and the example request body of the endpoint POST /v1/gpt/profiles.

There are two types of profiles. One is for the /gpt/ask endpoint, which is a general profile for any model except gpt-4-vision-preview. The other is for the /gpt/ask/vision endpoint, for which we currently support only gpt-4-vision-preview as the model.

Project setup

Project setup for MyGPT involves preparing, uploading and training your data. Additionally, set up the required setting the parameters to suit your requirement.

Dataset
Upload Dataset

Use the endpoint POST /v1/gpt/datasets to upload a dataset that will be used to train your customized GPT bot. Currently, we accept CSV and JSON format. You will find the required parameters and description in the sandbox in the link above.

How to see uploaded datasets

Once the dataset is uploaded, you can use /v1/gpt/datasets to see all your datasets of your project. The response will give you below details along with the datasets ids. This dataset_id will be required to edit, delete, train your data.

yaml { "dataset_id": "a8bf8ddd-b5cb-4bea-a82b-4ac148f01c0a", "created_at": "2023-12-24T20:23:34.992063+09:00", "name": "NAME OF THE DATASET", "description": DESCRIPTION OF THE DATASET, "idx_column_name": "idx", "image_url_column": "images" }

Delete dataset

Use the endpoint DELETE /v1/gpt/datasets/{dataset_id} to delete a dataset or particular documents from a dataset. You can find the expected request body, with required parameters and values in our sandbox here

Update Dataset

To be updated

Updating and deleting - Documents

To be updated

Training

Use endpoint POST /v1/gpt/dataset/train to train your uploaded dataset. This endpoint will take the dataset id and image type. It is good practice to train only what is necessary to optimize the usage of resources.

Profile
Setup profile

Use POST /v1/gpt/profiles to setup GPT profile. You can setup multiple profiles. However you will need to select one as the default profile in the next step.

Here are some detail about "intro", "System" and "Model" parameters.

Intro: This is detailed instruction for the bot on how to respond to a query. For example, if you are creating a customer support bot for an E-commerce site "AAA", this parameter could be: "Answer customer questions based on the catalog information and FAQ documents of our company. If a customer wants a product recommendation, ask up to three questions to understand their needs better, then make three recommendations and ask the customer if they like them or want different recommendations."

System: This tells the bot about the persona it is supposed to adopt. For example, if you are creating a customer support bot for an E-commerce site "AAA", selling cosmetics, this parameter could be: "You are customer support for AAA, an expert on skincare and cosmetic products who values customer needs and provides the right product recommendations."

Model: We support all GPT models of OpenAI, which you can select based on the needs and use cases. Please consider the purpose and the estimated token count when selecting the model, as this can significantly impact costs. You can learn more about OpenAI models from this page. This setting will impact the parameters search_max_token (tokens allocated for data sent to the model) and completion_token (tokens allocated for the reply). Note that intro, system, and query have token costs that are not included in the token size allocation. The selected model's CONTEXT WINDOW should cover the total token allocation. That is CONTEXT WINDOW ≥ search_max_token + completion_token + intro + system + query.

You can use the use the endpoints in our sandbox under "MyGPT Profiles" to Check the profiles and update or delete them.

Default profile setup

Once you have setup the profile(s), decide one of the profiles that should be default and use our endpoint POST /v1/gpt/settings to set your default profile.

with this, your GPT setup is completed.

Previous
Questionnaire
Next
GPT Integration
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/02_gpt_integration/gpt_project_setup/#upload-dataset
Skip to content
Gigalogy Tutorial
GPT Project setup
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
GPT Project setup
GPT Integration
GPT Feedback
API Reference
Release notes
Glossary
Table of contents
Documents
Datasets
Profiles
Project setup
Dataset
Upload Dataset
How to see uploaded datasets
Delete dataset
Update Dataset
Updating and deleting - Documents
Training
Profile
Setup profile
Default profile setup
GPT Project setup

Gigalogy,s GPT based solutions allows you to make your own GPT solutions, trained with your own data, customized according to your needs. Here are some basics to get you started. Find MyGPT related endpoints in our sandbox.

Documents

Documents are information that GPT will consider as a single piece of information, such as Address of Gigalogy, What is Gigalogy personalization, etc.

Datasets

A Dataset is a collection of documents in a single file. For instance, single dataset may contain documents with information such as Address of Gigalogy, What is Gigalogy personalization, What is Gigalogy's MyGPT.

Profiles

With each request sent to GPT endpoints POST /v1/gpt/ask and POST /v1/gpt/ask/vision, we include a parameter called gpt_profile_id. This parameter's value points to a GPT profile. GPT profiles hold information that tells the GPT how to process the information provided (query) and how to respond. To see more about what is inside a profile, check out the parameters and the example request body of the endpoint POST /v1/gpt/profiles.

There are two types of profiles. One is for the /gpt/ask endpoint, which is a general profile for any model except gpt-4-vision-preview. The other is for the /gpt/ask/vision endpoint, for which we currently support only gpt-4-vision-preview as the model.

Project setup

Project setup for MyGPT involves preparing, uploading and training your data. Additionally, set up the required setting the parameters to suit your requirement.

Dataset
Upload Dataset

Use the endpoint POST /v1/gpt/datasets to upload a dataset that will be used to train your customized GPT bot. Currently, we accept CSV and JSON format. You will find the required parameters and description in the sandbox in the link above.

How to see uploaded datasets

Once the dataset is uploaded, you can use /v1/gpt/datasets to see all your datasets of your project. The response will give you below details along with the datasets ids. This dataset_id will be required to edit, delete, train your data.

yaml { "dataset_id": "a8bf8ddd-b5cb-4bea-a82b-4ac148f01c0a", "created_at": "2023-12-24T20:23:34.992063+09:00", "name": "NAME OF THE DATASET", "description": DESCRIPTION OF THE DATASET, "idx_column_name": "idx", "image_url_column": "images" }

Delete dataset

Use the endpoint DELETE /v1/gpt/datasets/{dataset_id} to delete a dataset or particular documents from a dataset. You can find the expected request body, with required parameters and values in our sandbox here

Update Dataset

To be updated

Updating and deleting - Documents

To be updated

Training

Use endpoint POST /v1/gpt/dataset/train to train your uploaded dataset. This endpoint will take the dataset id and image type. It is good practice to train only what is necessary to optimize the usage of resources.

Profile
Setup profile

Use POST /v1/gpt/profiles to setup GPT profile. You can setup multiple profiles. However you will need to select one as the default profile in the next step.

Here are some detail about "intro", "System" and "Model" parameters.

Intro: This is detailed instruction for the bot on how to respond to a query. For example, if you are creating a customer support bot for an E-commerce site "AAA", this parameter could be: "Answer customer questions based on the catalog information and FAQ documents of our company. If a customer wants a product recommendation, ask up to three questions to understand their needs better, then make three recommendations and ask the customer if they like them or want different recommendations."

System: This tells the bot about the persona it is supposed to adopt. For example, if you are creating a customer support bot for an E-commerce site "AAA", selling cosmetics, this parameter could be: "You are customer support for AAA, an expert on skincare and cosmetic products who values customer needs and provides the right product recommendations."

Model: We support all GPT models of OpenAI, which you can select based on the needs and use cases. Please consider the purpose and the estimated token count when selecting the model, as this can significantly impact costs. You can learn more about OpenAI models from this page. This setting will impact the parameters search_max_token (tokens allocated for data sent to the model) and completion_token (tokens allocated for the reply). Note that intro, system, and query have token costs that are not included in the token size allocation. The selected model's CONTEXT WINDOW should cover the total token allocation. That is CONTEXT WINDOW ≥ search_max_token + completion_token + intro + system + query.

You can use the use the endpoints in our sandbox under "MyGPT Profiles" to Check the profiles and update or delete them.

Default profile setup

Once you have setup the profile(s), decide one of the profiles that should be default and use our endpoint POST /v1/gpt/settings to set your default profile.

with this, your GPT setup is completed.

Previous
Questionnaire
Next
GPT Integration
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/02_gpt_integration/gpt_project_setup/#how-to-see-uploaded-datasets
Skip to content
Gigalogy Tutorial
GPT Project setup
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
GPT Project setup
GPT Integration
GPT Feedback
API Reference
Release notes
Glossary
Table of contents
Documents
Datasets
Profiles
Project setup
Dataset
Upload Dataset
How to see uploaded datasets
Delete dataset
Update Dataset
Updating and deleting - Documents
Training
Profile
Setup profile
Default profile setup
GPT Project setup

Gigalogy,s GPT based solutions allows you to make your own GPT solutions, trained with your own data, customized according to your needs. Here are some basics to get you started. Find MyGPT related endpoints in our sandbox.

Documents

Documents are information that GPT will consider as a single piece of information, such as Address of Gigalogy, What is Gigalogy personalization, etc.

Datasets

A Dataset is a collection of documents in a single file. For instance, single dataset may contain documents with information such as Address of Gigalogy, What is Gigalogy personalization, What is Gigalogy's MyGPT.

Profiles

With each request sent to GPT endpoints POST /v1/gpt/ask and POST /v1/gpt/ask/vision, we include a parameter called gpt_profile_id. This parameter's value points to a GPT profile. GPT profiles hold information that tells the GPT how to process the information provided (query) and how to respond. To see more about what is inside a profile, check out the parameters and the example request body of the endpoint POST /v1/gpt/profiles.

There are two types of profiles. One is for the /gpt/ask endpoint, which is a general profile for any model except gpt-4-vision-preview. The other is for the /gpt/ask/vision endpoint, for which we currently support only gpt-4-vision-preview as the model.

Project setup

Project setup for MyGPT involves preparing, uploading and training your data. Additionally, set up the required setting the parameters to suit your requirement.

Dataset
Upload Dataset

Use the endpoint POST /v1/gpt/datasets to upload a dataset that will be used to train your customized GPT bot. Currently, we accept CSV and JSON format. You will find the required parameters and description in the sandbox in the link above.

How to see uploaded datasets

Once the dataset is uploaded, you can use /v1/gpt/datasets to see all your datasets of your project. The response will give you below details along with the datasets ids. This dataset_id will be required to edit, delete, train your data.

yaml { "dataset_id": "a8bf8ddd-b5cb-4bea-a82b-4ac148f01c0a", "created_at": "2023-12-24T20:23:34.992063+09:00", "name": "NAME OF THE DATASET", "description": DESCRIPTION OF THE DATASET, "idx_column_name": "idx", "image_url_column": "images" }

Delete dataset

Use the endpoint DELETE /v1/gpt/datasets/{dataset_id} to delete a dataset or particular documents from a dataset. You can find the expected request body, with required parameters and values in our sandbox here

Update Dataset

To be updated

Updating and deleting - Documents

To be updated

Training

Use endpoint POST /v1/gpt/dataset/train to train your uploaded dataset. This endpoint will take the dataset id and image type. It is good practice to train only what is necessary to optimize the usage of resources.

Profile
Setup profile

Use POST /v1/gpt/profiles to setup GPT profile. You can setup multiple profiles. However you will need to select one as the default profile in the next step.

Here are some detail about "intro", "System" and "Model" parameters.

Intro: This is detailed instruction for the bot on how to respond to a query. For example, if you are creating a customer support bot for an E-commerce site "AAA", this parameter could be: "Answer customer questions based on the catalog information and FAQ documents of our company. If a customer wants a product recommendation, ask up to three questions to understand their needs better, then make three recommendations and ask the customer if they like them or want different recommendations."

System: This tells the bot about the persona it is supposed to adopt. For example, if you are creating a customer support bot for an E-commerce site "AAA", selling cosmetics, this parameter could be: "You are customer support for AAA, an expert on skincare and cosmetic products who values customer needs and provides the right product recommendations."

Model: We support all GPT models of OpenAI, which you can select based on the needs and use cases. Please consider the purpose and the estimated token count when selecting the model, as this can significantly impact costs. You can learn more about OpenAI models from this page. This setting will impact the parameters search_max_token (tokens allocated for data sent to the model) and completion_token (tokens allocated for the reply). Note that intro, system, and query have token costs that are not included in the token size allocation. The selected model's CONTEXT WINDOW should cover the total token allocation. That is CONTEXT WINDOW ≥ search_max_token + completion_token + intro + system + query.

You can use the use the endpoints in our sandbox under "MyGPT Profiles" to Check the profiles and update or delete them.

Default profile setup

Once you have setup the profile(s), decide one of the profiles that should be default and use our endpoint POST /v1/gpt/settings to set your default profile.

with this, your GPT setup is completed.

Previous
Questionnaire
Next
GPT Integration
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/02_gpt_integration/gpt_project_setup/#delete-dataset
Skip to content
Gigalogy Tutorial
GPT Project setup
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
GPT Project setup
GPT Integration
GPT Feedback
API Reference
Release notes
Glossary
Table of contents
Documents
Datasets
Profiles
Project setup
Dataset
Upload Dataset
How to see uploaded datasets
Delete dataset
Update Dataset
Updating and deleting - Documents
Training
Profile
Setup profile
Default profile setup
GPT Project setup

Gigalogy,s GPT based solutions allows you to make your own GPT solutions, trained with your own data, customized according to your needs. Here are some basics to get you started. Find MyGPT related endpoints in our sandbox.

Documents

Documents are information that GPT will consider as a single piece of information, such as Address of Gigalogy, What is Gigalogy personalization, etc.

Datasets

A Dataset is a collection of documents in a single file. For instance, single dataset may contain documents with information such as Address of Gigalogy, What is Gigalogy personalization, What is Gigalogy's MyGPT.

Profiles

With each request sent to GPT endpoints POST /v1/gpt/ask and POST /v1/gpt/ask/vision, we include a parameter called gpt_profile_id. This parameter's value points to a GPT profile. GPT profiles hold information that tells the GPT how to process the information provided (query) and how to respond. To see more about what is inside a profile, check out the parameters and the example request body of the endpoint POST /v1/gpt/profiles.

There are two types of profiles. One is for the /gpt/ask endpoint, which is a general profile for any model except gpt-4-vision-preview. The other is for the /gpt/ask/vision endpoint, for which we currently support only gpt-4-vision-preview as the model.

Project setup

Project setup for MyGPT involves preparing, uploading and training your data. Additionally, set up the required setting the parameters to suit your requirement.

Dataset
Upload Dataset

Use the endpoint POST /v1/gpt/datasets to upload a dataset that will be used to train your customized GPT bot. Currently, we accept CSV and JSON format. You will find the required parameters and description in the sandbox in the link above.

How to see uploaded datasets

Once the dataset is uploaded, you can use /v1/gpt/datasets to see all your datasets of your project. The response will give you below details along with the datasets ids. This dataset_id will be required to edit, delete, train your data.

yaml { "dataset_id": "a8bf8ddd-b5cb-4bea-a82b-4ac148f01c0a", "created_at": "2023-12-24T20:23:34.992063+09:00", "name": "NAME OF THE DATASET", "description": DESCRIPTION OF THE DATASET, "idx_column_name": "idx", "image_url_column": "images" }

Delete dataset

Use the endpoint DELETE /v1/gpt/datasets/{dataset_id} to delete a dataset or particular documents from a dataset. You can find the expected request body, with required parameters and values in our sandbox here

Update Dataset

To be updated

Updating and deleting - Documents

To be updated

Training

Use endpoint POST /v1/gpt/dataset/train to train your uploaded dataset. This endpoint will take the dataset id and image type. It is good practice to train only what is necessary to optimize the usage of resources.

Profile
Setup profile

Use POST /v1/gpt/profiles to setup GPT profile. You can setup multiple profiles. However you will need to select one as the default profile in the next step.

Here are some detail about "intro", "System" and "Model" parameters.

Intro: This is detailed instruction for the bot on how to respond to a query. For example, if you are creating a customer support bot for an E-commerce site "AAA", this parameter could be: "Answer customer questions based on the catalog information and FAQ documents of our company. If a customer wants a product recommendation, ask up to three questions to understand their needs better, then make three recommendations and ask the customer if they like them or want different recommendations."

System: This tells the bot about the persona it is supposed to adopt. For example, if you are creating a customer support bot for an E-commerce site "AAA", selling cosmetics, this parameter could be: "You are customer support for AAA, an expert on skincare and cosmetic products who values customer needs and provides the right product recommendations."

Model: We support all GPT models of OpenAI, which you can select based on the needs and use cases. Please consider the purpose and the estimated token count when selecting the model, as this can significantly impact costs. You can learn more about OpenAI models from this page. This setting will impact the parameters search_max_token (tokens allocated for data sent to the model) and completion_token (tokens allocated for the reply). Note that intro, system, and query have token costs that are not included in the token size allocation. The selected model's CONTEXT WINDOW should cover the total token allocation. That is CONTEXT WINDOW ≥ search_max_token + completion_token + intro + system + query.

You can use the use the endpoints in our sandbox under "MyGPT Profiles" to Check the profiles and update or delete them.

Default profile setup

Once you have setup the profile(s), decide one of the profiles that should be default and use our endpoint POST /v1/gpt/settings to set your default profile.

with this, your GPT setup is completed.

Previous
Questionnaire
Next
GPT Integration
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/02_gpt_integration/gpt_project_setup/#update-dataset
Skip to content
Gigalogy Tutorial
GPT Project setup
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
GPT Project setup
GPT Integration
GPT Feedback
API Reference
Release notes
Glossary
Table of contents
Documents
Datasets
Profiles
Project setup
Dataset
Upload Dataset
How to see uploaded datasets
Delete dataset
Update Dataset
Updating and deleting - Documents
Training
Profile
Setup profile
Default profile setup
GPT Project setup

Gigalogy,s GPT based solutions allows you to make your own GPT solutions, trained with your own data, customized according to your needs. Here are some basics to get you started. Find MyGPT related endpoints in our sandbox.

Documents

Documents are information that GPT will consider as a single piece of information, such as Address of Gigalogy, What is Gigalogy personalization, etc.

Datasets

A Dataset is a collection of documents in a single file. For instance, single dataset may contain documents with information such as Address of Gigalogy, What is Gigalogy personalization, What is Gigalogy's MyGPT.

Profiles

With each request sent to GPT endpoints POST /v1/gpt/ask and POST /v1/gpt/ask/vision, we include a parameter called gpt_profile_id. This parameter's value points to a GPT profile. GPT profiles hold information that tells the GPT how to process the information provided (query) and how to respond. To see more about what is inside a profile, check out the parameters and the example request body of the endpoint POST /v1/gpt/profiles.

There are two types of profiles. One is for the /gpt/ask endpoint, which is a general profile for any model except gpt-4-vision-preview. The other is for the /gpt/ask/vision endpoint, for which we currently support only gpt-4-vision-preview as the model.

Project setup

Project setup for MyGPT involves preparing, uploading and training your data. Additionally, set up the required setting the parameters to suit your requirement.

Dataset
Upload Dataset

Use the endpoint POST /v1/gpt/datasets to upload a dataset that will be used to train your customized GPT bot. Currently, we accept CSV and JSON format. You will find the required parameters and description in the sandbox in the link above.

How to see uploaded datasets

Once the dataset is uploaded, you can use /v1/gpt/datasets to see all your datasets of your project. The response will give you below details along with the datasets ids. This dataset_id will be required to edit, delete, train your data.

yaml { "dataset_id": "a8bf8ddd-b5cb-4bea-a82b-4ac148f01c0a", "created_at": "2023-12-24T20:23:34.992063+09:00", "name": "NAME OF THE DATASET", "description": DESCRIPTION OF THE DATASET, "idx_column_name": "idx", "image_url_column": "images" }

Delete dataset

Use the endpoint DELETE /v1/gpt/datasets/{dataset_id} to delete a dataset or particular documents from a dataset. You can find the expected request body, with required parameters and values in our sandbox here

Update Dataset

To be updated

Updating and deleting - Documents

To be updated

Training

Use endpoint POST /v1/gpt/dataset/train to train your uploaded dataset. This endpoint will take the dataset id and image type. It is good practice to train only what is necessary to optimize the usage of resources.

Profile
Setup profile

Use POST /v1/gpt/profiles to setup GPT profile. You can setup multiple profiles. However you will need to select one as the default profile in the next step.

Here are some detail about "intro", "System" and "Model" parameters.

Intro: This is detailed instruction for the bot on how to respond to a query. For example, if you are creating a customer support bot for an E-commerce site "AAA", this parameter could be: "Answer customer questions based on the catalog information and FAQ documents of our company. If a customer wants a product recommendation, ask up to three questions to understand their needs better, then make three recommendations and ask the customer if they like them or want different recommendations."

System: This tells the bot about the persona it is supposed to adopt. For example, if you are creating a customer support bot for an E-commerce site "AAA", selling cosmetics, this parameter could be: "You are customer support for AAA, an expert on skincare and cosmetic products who values customer needs and provides the right product recommendations."

Model: We support all GPT models of OpenAI, which you can select based on the needs and use cases. Please consider the purpose and the estimated token count when selecting the model, as this can significantly impact costs. You can learn more about OpenAI models from this page. This setting will impact the parameters search_max_token (tokens allocated for data sent to the model) and completion_token (tokens allocated for the reply). Note that intro, system, and query have token costs that are not included in the token size allocation. The selected model's CONTEXT WINDOW should cover the total token allocation. That is CONTEXT WINDOW ≥ search_max_token + completion_token + intro + system + query.

You can use the use the endpoints in our sandbox under "MyGPT Profiles" to Check the profiles and update or delete them.

Default profile setup

Once you have setup the profile(s), decide one of the profiles that should be default and use our endpoint POST /v1/gpt/settings to set your default profile.

with this, your GPT setup is completed.

Previous
Questionnaire
Next
GPT Integration
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/02_gpt_integration/gpt_project_setup/#updating-and-deleting-documents
Skip to content
Gigalogy Tutorial
GPT Project setup
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
GPT Project setup
GPT Integration
GPT Feedback
API Reference
Release notes
Glossary
Table of contents
Documents
Datasets
Profiles
Project setup
Dataset
Upload Dataset
How to see uploaded datasets
Delete dataset
Update Dataset
Updating and deleting - Documents
Training
Profile
Setup profile
Default profile setup
GPT Project setup

Gigalogy,s GPT based solutions allows you to make your own GPT solutions, trained with your own data, customized according to your needs. Here are some basics to get you started. Find MyGPT related endpoints in our sandbox.

Documents

Documents are information that GPT will consider as a single piece of information, such as Address of Gigalogy, What is Gigalogy personalization, etc.

Datasets

A Dataset is a collection of documents in a single file. For instance, single dataset may contain documents with information such as Address of Gigalogy, What is Gigalogy personalization, What is Gigalogy's MyGPT.

Profiles

With each request sent to GPT endpoints POST /v1/gpt/ask and POST /v1/gpt/ask/vision, we include a parameter called gpt_profile_id. This parameter's value points to a GPT profile. GPT profiles hold information that tells the GPT how to process the information provided (query) and how to respond. To see more about what is inside a profile, check out the parameters and the example request body of the endpoint POST /v1/gpt/profiles.

There are two types of profiles. One is for the /gpt/ask endpoint, which is a general profile for any model except gpt-4-vision-preview. The other is for the /gpt/ask/vision endpoint, for which we currently support only gpt-4-vision-preview as the model.

Project setup

Project setup for MyGPT involves preparing, uploading and training your data. Additionally, set up the required setting the parameters to suit your requirement.

Dataset
Upload Dataset

Use the endpoint POST /v1/gpt/datasets to upload a dataset that will be used to train your customized GPT bot. Currently, we accept CSV and JSON format. You will find the required parameters and description in the sandbox in the link above.

How to see uploaded datasets

Once the dataset is uploaded, you can use /v1/gpt/datasets to see all your datasets of your project. The response will give you below details along with the datasets ids. This dataset_id will be required to edit, delete, train your data.

yaml { "dataset_id": "a8bf8ddd-b5cb-4bea-a82b-4ac148f01c0a", "created_at": "2023-12-24T20:23:34.992063+09:00", "name": "NAME OF THE DATASET", "description": DESCRIPTION OF THE DATASET, "idx_column_name": "idx", "image_url_column": "images" }

Delete dataset

Use the endpoint DELETE /v1/gpt/datasets/{dataset_id} to delete a dataset or particular documents from a dataset. You can find the expected request body, with required parameters and values in our sandbox here

Update Dataset

To be updated

Updating and deleting - Documents

To be updated

Training

Use endpoint POST /v1/gpt/dataset/train to train your uploaded dataset. This endpoint will take the dataset id and image type. It is good practice to train only what is necessary to optimize the usage of resources.

Profile
Setup profile

Use POST /v1/gpt/profiles to setup GPT profile. You can setup multiple profiles. However you will need to select one as the default profile in the next step.

Here are some detail about "intro", "System" and "Model" parameters.

Intro: This is detailed instruction for the bot on how to respond to a query. For example, if you are creating a customer support bot for an E-commerce site "AAA", this parameter could be: "Answer customer questions based on the catalog information and FAQ documents of our company. If a customer wants a product recommendation, ask up to three questions to understand their needs better, then make three recommendations and ask the customer if they like them or want different recommendations."

System: This tells the bot about the persona it is supposed to adopt. For example, if you are creating a customer support bot for an E-commerce site "AAA", selling cosmetics, this parameter could be: "You are customer support for AAA, an expert on skincare and cosmetic products who values customer needs and provides the right product recommendations."

Model: We support all GPT models of OpenAI, which you can select based on the needs and use cases. Please consider the purpose and the estimated token count when selecting the model, as this can significantly impact costs. You can learn more about OpenAI models from this page. This setting will impact the parameters search_max_token (tokens allocated for data sent to the model) and completion_token (tokens allocated for the reply). Note that intro, system, and query have token costs that are not included in the token size allocation. The selected model's CONTEXT WINDOW should cover the total token allocation. That is CONTEXT WINDOW ≥ search_max_token + completion_token + intro + system + query.

You can use the use the endpoints in our sandbox under "MyGPT Profiles" to Check the profiles and update or delete them.

Default profile setup

Once you have setup the profile(s), decide one of the profiles that should be default and use our endpoint POST /v1/gpt/settings to set your default profile.

with this, your GPT setup is completed.

Previous
Questionnaire
Next
GPT Integration
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/02_gpt_integration/gpt_project_setup/#training
Skip to content
Gigalogy Tutorial
GPT Project setup
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
GPT Project setup
GPT Integration
GPT Feedback
API Reference
Release notes
Glossary
Table of contents
Documents
Datasets
Profiles
Project setup
Dataset
Upload Dataset
How to see uploaded datasets
Delete dataset
Update Dataset
Updating and deleting - Documents
Training
Profile
Setup profile
Default profile setup
GPT Project setup

Gigalogy,s GPT based solutions allows you to make your own GPT solutions, trained with your own data, customized according to your needs. Here are some basics to get you started. Find MyGPT related endpoints in our sandbox.

Documents

Documents are information that GPT will consider as a single piece of information, such as Address of Gigalogy, What is Gigalogy personalization, etc.

Datasets

A Dataset is a collection of documents in a single file. For instance, single dataset may contain documents with information such as Address of Gigalogy, What is Gigalogy personalization, What is Gigalogy's MyGPT.

Profiles

With each request sent to GPT endpoints POST /v1/gpt/ask and POST /v1/gpt/ask/vision, we include a parameter called gpt_profile_id. This parameter's value points to a GPT profile. GPT profiles hold information that tells the GPT how to process the information provided (query) and how to respond. To see more about what is inside a profile, check out the parameters and the example request body of the endpoint POST /v1/gpt/profiles.

There are two types of profiles. One is for the /gpt/ask endpoint, which is a general profile for any model except gpt-4-vision-preview. The other is for the /gpt/ask/vision endpoint, for which we currently support only gpt-4-vision-preview as the model.

Project setup

Project setup for MyGPT involves preparing, uploading and training your data. Additionally, set up the required setting the parameters to suit your requirement.

Dataset
Upload Dataset

Use the endpoint POST /v1/gpt/datasets to upload a dataset that will be used to train your customized GPT bot. Currently, we accept CSV and JSON format. You will find the required parameters and description in the sandbox in the link above.

How to see uploaded datasets

Once the dataset is uploaded, you can use /v1/gpt/datasets to see all your datasets of your project. The response will give you below details along with the datasets ids. This dataset_id will be required to edit, delete, train your data.

yaml { "dataset_id": "a8bf8ddd-b5cb-4bea-a82b-4ac148f01c0a", "created_at": "2023-12-24T20:23:34.992063+09:00", "name": "NAME OF THE DATASET", "description": DESCRIPTION OF THE DATASET, "idx_column_name": "idx", "image_url_column": "images" }

Delete dataset

Use the endpoint DELETE /v1/gpt/datasets/{dataset_id} to delete a dataset or particular documents from a dataset. You can find the expected request body, with required parameters and values in our sandbox here

Update Dataset

To be updated

Updating and deleting - Documents

To be updated

Training

Use endpoint POST /v1/gpt/dataset/train to train your uploaded dataset. This endpoint will take the dataset id and image type. It is good practice to train only what is necessary to optimize the usage of resources.

Profile
Setup profile

Use POST /v1/gpt/profiles to setup GPT profile. You can setup multiple profiles. However you will need to select one as the default profile in the next step.

Here are some detail about "intro", "System" and "Model" parameters.

Intro: This is detailed instruction for the bot on how to respond to a query. For example, if you are creating a customer support bot for an E-commerce site "AAA", this parameter could be: "Answer customer questions based on the catalog information and FAQ documents of our company. If a customer wants a product recommendation, ask up to three questions to understand their needs better, then make three recommendations and ask the customer if they like them or want different recommendations."

System: This tells the bot about the persona it is supposed to adopt. For example, if you are creating a customer support bot for an E-commerce site "AAA", selling cosmetics, this parameter could be: "You are customer support for AAA, an expert on skincare and cosmetic products who values customer needs and provides the right product recommendations."

Model: We support all GPT models of OpenAI, which you can select based on the needs and use cases. Please consider the purpose and the estimated token count when selecting the model, as this can significantly impact costs. You can learn more about OpenAI models from this page. This setting will impact the parameters search_max_token (tokens allocated for data sent to the model) and completion_token (tokens allocated for the reply). Note that intro, system, and query have token costs that are not included in the token size allocation. The selected model's CONTEXT WINDOW should cover the total token allocation. That is CONTEXT WINDOW ≥ search_max_token + completion_token + intro + system + query.

You can use the use the endpoints in our sandbox under "MyGPT Profiles" to Check the profiles and update or delete them.

Default profile setup

Once you have setup the profile(s), decide one of the profiles that should be default and use our endpoint POST /v1/gpt/settings to set your default profile.

with this, your GPT setup is completed.

Previous
Questionnaire
Next
GPT Integration
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/02_gpt_integration/gpt_project_setup/#profile
Skip to content
Gigalogy Tutorial
GPT Project setup
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
GPT Project setup
GPT Integration
GPT Feedback
API Reference
Release notes
Glossary
Table of contents
Documents
Datasets
Profiles
Project setup
Dataset
Upload Dataset
How to see uploaded datasets
Delete dataset
Update Dataset
Updating and deleting - Documents
Training
Profile
Setup profile
Default profile setup
GPT Project setup

Gigalogy,s GPT based solutions allows you to make your own GPT solutions, trained with your own data, customized according to your needs. Here are some basics to get you started. Find MyGPT related endpoints in our sandbox.

Documents

Documents are information that GPT will consider as a single piece of information, such as Address of Gigalogy, What is Gigalogy personalization, etc.

Datasets

A Dataset is a collection of documents in a single file. For instance, single dataset may contain documents with information such as Address of Gigalogy, What is Gigalogy personalization, What is Gigalogy's MyGPT.

Profiles

With each request sent to GPT endpoints POST /v1/gpt/ask and POST /v1/gpt/ask/vision, we include a parameter called gpt_profile_id. This parameter's value points to a GPT profile. GPT profiles hold information that tells the GPT how to process the information provided (query) and how to respond. To see more about what is inside a profile, check out the parameters and the example request body of the endpoint POST /v1/gpt/profiles.

There are two types of profiles. One is for the /gpt/ask endpoint, which is a general profile for any model except gpt-4-vision-preview. The other is for the /gpt/ask/vision endpoint, for which we currently support only gpt-4-vision-preview as the model.

Project setup

Project setup for MyGPT involves preparing, uploading and training your data. Additionally, set up the required setting the parameters to suit your requirement.

Dataset
Upload Dataset

Use the endpoint POST /v1/gpt/datasets to upload a dataset that will be used to train your customized GPT bot. Currently, we accept CSV and JSON format. You will find the required parameters and description in the sandbox in the link above.

How to see uploaded datasets

Once the dataset is uploaded, you can use /v1/gpt/datasets to see all your datasets of your project. The response will give you below details along with the datasets ids. This dataset_id will be required to edit, delete, train your data.

yaml { "dataset_id": "a8bf8ddd-b5cb-4bea-a82b-4ac148f01c0a", "created_at": "2023-12-24T20:23:34.992063+09:00", "name": "NAME OF THE DATASET", "description": DESCRIPTION OF THE DATASET, "idx_column_name": "idx", "image_url_column": "images" }

Delete dataset

Use the endpoint DELETE /v1/gpt/datasets/{dataset_id} to delete a dataset or particular documents from a dataset. You can find the expected request body, with required parameters and values in our sandbox here

Update Dataset

To be updated

Updating and deleting - Documents

To be updated

Training

Use endpoint POST /v1/gpt/dataset/train to train your uploaded dataset. This endpoint will take the dataset id and image type. It is good practice to train only what is necessary to optimize the usage of resources.

Profile
Setup profile

Use POST /v1/gpt/profiles to setup GPT profile. You can setup multiple profiles. However you will need to select one as the default profile in the next step.

Here are some detail about "intro", "System" and "Model" parameters.

Intro: This is detailed instruction for the bot on how to respond to a query. For example, if you are creating a customer support bot for an E-commerce site "AAA", this parameter could be: "Answer customer questions based on the catalog information and FAQ documents of our company. If a customer wants a product recommendation, ask up to three questions to understand their needs better, then make three recommendations and ask the customer if they like them or want different recommendations."

System: This tells the bot about the persona it is supposed to adopt. For example, if you are creating a customer support bot for an E-commerce site "AAA", selling cosmetics, this parameter could be: "You are customer support for AAA, an expert on skincare and cosmetic products who values customer needs and provides the right product recommendations."

Model: We support all GPT models of OpenAI, which you can select based on the needs and use cases. Please consider the purpose and the estimated token count when selecting the model, as this can significantly impact costs. You can learn more about OpenAI models from this page. This setting will impact the parameters search_max_token (tokens allocated for data sent to the model) and completion_token (tokens allocated for the reply). Note that intro, system, and query have token costs that are not included in the token size allocation. The selected model's CONTEXT WINDOW should cover the total token allocation. That is CONTEXT WINDOW ≥ search_max_token + completion_token + intro + system + query.

You can use the use the endpoints in our sandbox under "MyGPT Profiles" to Check the profiles and update or delete them.

Default profile setup

Once you have setup the profile(s), decide one of the profiles that should be default and use our endpoint POST /v1/gpt/settings to set your default profile.

with this, your GPT setup is completed.

Previous
Questionnaire
Next
GPT Integration
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/02_gpt_integration/gpt_project_setup/#setup-profile
Skip to content
Gigalogy Tutorial
GPT Project setup
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
GPT Project setup
GPT Integration
GPT Feedback
API Reference
Release notes
Glossary
Table of contents
Documents
Datasets
Profiles
Project setup
Dataset
Upload Dataset
How to see uploaded datasets
Delete dataset
Update Dataset
Updating and deleting - Documents
Training
Profile
Setup profile
Default profile setup
GPT Project setup

Gigalogy,s GPT based solutions allows you to make your own GPT solutions, trained with your own data, customized according to your needs. Here are some basics to get you started. Find MyGPT related endpoints in our sandbox.

Documents

Documents are information that GPT will consider as a single piece of information, such as Address of Gigalogy, What is Gigalogy personalization, etc.

Datasets

A Dataset is a collection of documents in a single file. For instance, single dataset may contain documents with information such as Address of Gigalogy, What is Gigalogy personalization, What is Gigalogy's MyGPT.

Profiles

With each request sent to GPT endpoints POST /v1/gpt/ask and POST /v1/gpt/ask/vision, we include a parameter called gpt_profile_id. This parameter's value points to a GPT profile. GPT profiles hold information that tells the GPT how to process the information provided (query) and how to respond. To see more about what is inside a profile, check out the parameters and the example request body of the endpoint POST /v1/gpt/profiles.

There are two types of profiles. One is for the /gpt/ask endpoint, which is a general profile for any model except gpt-4-vision-preview. The other is for the /gpt/ask/vision endpoint, for which we currently support only gpt-4-vision-preview as the model.

Project setup

Project setup for MyGPT involves preparing, uploading and training your data. Additionally, set up the required setting the parameters to suit your requirement.

Dataset
Upload Dataset

Use the endpoint POST /v1/gpt/datasets to upload a dataset that will be used to train your customized GPT bot. Currently, we accept CSV and JSON format. You will find the required parameters and description in the sandbox in the link above.

How to see uploaded datasets

Once the dataset is uploaded, you can use /v1/gpt/datasets to see all your datasets of your project. The response will give you below details along with the datasets ids. This dataset_id will be required to edit, delete, train your data.

yaml { "dataset_id": "a8bf8ddd-b5cb-4bea-a82b-4ac148f01c0a", "created_at": "2023-12-24T20:23:34.992063+09:00", "name": "NAME OF THE DATASET", "description": DESCRIPTION OF THE DATASET, "idx_column_name": "idx", "image_url_column": "images" }

Delete dataset

Use the endpoint DELETE /v1/gpt/datasets/{dataset_id} to delete a dataset or particular documents from a dataset. You can find the expected request body, with required parameters and values in our sandbox here

Update Dataset

To be updated

Updating and deleting - Documents

To be updated

Training

Use endpoint POST /v1/gpt/dataset/train to train your uploaded dataset. This endpoint will take the dataset id and image type. It is good practice to train only what is necessary to optimize the usage of resources.

Profile
Setup profile

Use POST /v1/gpt/profiles to setup GPT profile. You can setup multiple profiles. However you will need to select one as the default profile in the next step.

Here are some detail about "intro", "System" and "Model" parameters.

Intro: This is detailed instruction for the bot on how to respond to a query. For example, if you are creating a customer support bot for an E-commerce site "AAA", this parameter could be: "Answer customer questions based on the catalog information and FAQ documents of our company. If a customer wants a product recommendation, ask up to three questions to understand their needs better, then make three recommendations and ask the customer if they like them or want different recommendations."

System: This tells the bot about the persona it is supposed to adopt. For example, if you are creating a customer support bot for an E-commerce site "AAA", selling cosmetics, this parameter could be: "You are customer support for AAA, an expert on skincare and cosmetic products who values customer needs and provides the right product recommendations."

Model: We support all GPT models of OpenAI, which you can select based on the needs and use cases. Please consider the purpose and the estimated token count when selecting the model, as this can significantly impact costs. You can learn more about OpenAI models from this page. This setting will impact the parameters search_max_token (tokens allocated for data sent to the model) and completion_token (tokens allocated for the reply). Note that intro, system, and query have token costs that are not included in the token size allocation. The selected model's CONTEXT WINDOW should cover the total token allocation. That is CONTEXT WINDOW ≥ search_max_token + completion_token + intro + system + query.

You can use the use the endpoints in our sandbox under "MyGPT Profiles" to Check the profiles and update or delete them.

Default profile setup

Once you have setup the profile(s), decide one of the profiles that should be default and use our endpoint POST /v1/gpt/settings to set your default profile.

with this, your GPT setup is completed.

Previous
Questionnaire
Next
GPT Integration
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/02_gpt_integration/gpt_project_setup/#default-profile-setup
Skip to content
Gigalogy Tutorial
GPT Project setup
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
GPT Project setup
GPT Integration
GPT Feedback
API Reference
Release notes
Glossary
Table of contents
Documents
Datasets
Profiles
Project setup
Dataset
Upload Dataset
How to see uploaded datasets
Delete dataset
Update Dataset
Updating and deleting - Documents
Training
Profile
Setup profile
Default profile setup
GPT Project setup

Gigalogy,s GPT based solutions allows you to make your own GPT solutions, trained with your own data, customized according to your needs. Here are some basics to get you started. Find MyGPT related endpoints in our sandbox.

Documents

Documents are information that GPT will consider as a single piece of information, such as Address of Gigalogy, What is Gigalogy personalization, etc.

Datasets

A Dataset is a collection of documents in a single file. For instance, single dataset may contain documents with information such as Address of Gigalogy, What is Gigalogy personalization, What is Gigalogy's MyGPT.

Profiles

With each request sent to GPT endpoints POST /v1/gpt/ask and POST /v1/gpt/ask/vision, we include a parameter called gpt_profile_id. This parameter's value points to a GPT profile. GPT profiles hold information that tells the GPT how to process the information provided (query) and how to respond. To see more about what is inside a profile, check out the parameters and the example request body of the endpoint POST /v1/gpt/profiles.

There are two types of profiles. One is for the /gpt/ask endpoint, which is a general profile for any model except gpt-4-vision-preview. The other is for the /gpt/ask/vision endpoint, for which we currently support only gpt-4-vision-preview as the model.

Project setup

Project setup for MyGPT involves preparing, uploading and training your data. Additionally, set up the required setting the parameters to suit your requirement.

Dataset
Upload Dataset

Use the endpoint POST /v1/gpt/datasets to upload a dataset that will be used to train your customized GPT bot. Currently, we accept CSV and JSON format. You will find the required parameters and description in the sandbox in the link above.

How to see uploaded datasets

Once the dataset is uploaded, you can use /v1/gpt/datasets to see all your datasets of your project. The response will give you below details along with the datasets ids. This dataset_id will be required to edit, delete, train your data.

yaml { "dataset_id": "a8bf8ddd-b5cb-4bea-a82b-4ac148f01c0a", "created_at": "2023-12-24T20:23:34.992063+09:00", "name": "NAME OF THE DATASET", "description": DESCRIPTION OF THE DATASET, "idx_column_name": "idx", "image_url_column": "images" }

Delete dataset

Use the endpoint DELETE /v1/gpt/datasets/{dataset_id} to delete a dataset or particular documents from a dataset. You can find the expected request body, with required parameters and values in our sandbox here

Update Dataset

To be updated

Updating and deleting - Documents

To be updated

Training

Use endpoint POST /v1/gpt/dataset/train to train your uploaded dataset. This endpoint will take the dataset id and image type. It is good practice to train only what is necessary to optimize the usage of resources.

Profile
Setup profile

Use POST /v1/gpt/profiles to setup GPT profile. You can setup multiple profiles. However you will need to select one as the default profile in the next step.

Here are some detail about "intro", "System" and "Model" parameters.

Intro: This is detailed instruction for the bot on how to respond to a query. For example, if you are creating a customer support bot for an E-commerce site "AAA", this parameter could be: "Answer customer questions based on the catalog information and FAQ documents of our company. If a customer wants a product recommendation, ask up to three questions to understand their needs better, then make three recommendations and ask the customer if they like them or want different recommendations."

System: This tells the bot about the persona it is supposed to adopt. For example, if you are creating a customer support bot for an E-commerce site "AAA", selling cosmetics, this parameter could be: "You are customer support for AAA, an expert on skincare and cosmetic products who values customer needs and provides the right product recommendations."

Model: We support all GPT models of OpenAI, which you can select based on the needs and use cases. Please consider the purpose and the estimated token count when selecting the model, as this can significantly impact costs. You can learn more about OpenAI models from this page. This setting will impact the parameters search_max_token (tokens allocated for data sent to the model) and completion_token (tokens allocated for the reply). Note that intro, system, and query have token costs that are not included in the token size allocation. The selected model's CONTEXT WINDOW should cover the total token allocation. That is CONTEXT WINDOW ≥ search_max_token + completion_token + intro + system + query.

You can use the use the endpoints in our sandbox under "MyGPT Profiles" to Check the profiles and update or delete them.

Default profile setup

Once you have setup the profile(s), decide one of the profiles that should be default and use our endpoint POST /v1/gpt/settings to set your default profile.

with this, your GPT setup is completed.

Previous
Questionnaire
Next
GPT Integration
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/02_gpt_integration/gpt_integration/
Skip to content
Gigalogy Tutorial
GPT Integration
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
GPT Project setup
GPT Integration
GPT Feedback
API Reference
Release notes
Glossary
Table of contents
Asking question to the bot
Checking conversation history
Using endpoint
From the GAIP UI
GPT Integration

After preparing, uploading and training your data, now you are ready to interact with your bot, by asking it question with texts and images via our endpoints.

Asking question to the bot

To send a text only question/query to GPT, use the endpoint POST /v1/gpt/ask. Find details of the required parameters with explanation and an example request body in our sandbox under "MyGPT"

In the response, you will get the response to the query, along with some other information related, such as the conversation_id, tokens, etc.

For query with image, use the endpoint POST /v1/gpt/ask/vision. There is one additional parameter here, to add the image file. This endpoint will take both the image and the query and respond based on both input. Here you can only use gpt-4-vision-preview model.

Note that if you do not pass any gpt_profile_id, it will use the default profile that was set during the setup.

Checking conversation history

There are two ways to check conversation history 1. Using API endpoint 2. From the GAIP UI.

Using endpoint

Using our endpoint GET /v1/gpt/conversations, you can check conversation history of your project, for a specific date range. By default, (if no time period is specified), this endpoint will give the conversation history for last 7 days. If you want to see the response detail of any particular conversation, you can use your endpoint GET /v1/gpt/conversations/{conversation_id}. This endpoint will take the conversation ID and will give the detail of that particular conversation.

From the GAIP UI

In our platform, navigate to your project's insight page. Here you will find "GPT-Flow History" table. This will show all the conversation during the time range selected above. You can filter this table with user id, query, feedback. You can also see the details of each conversation by clicking on the details icon at the right. To download the history table, use the download button at the top right of the table. This will download the table as CSV. Note that the downloaded file will only contain the information as it is on the table, with the date and other filters applied.

Previous
GPT Project setup
Next
GPT Feedback
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/02_gpt_integration/gpt_integration/#asking-question-to-the-bot
Skip to content
Gigalogy Tutorial
GPT Integration
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
GPT Project setup
GPT Integration
GPT Feedback
API Reference
Release notes
Glossary
Table of contents
Asking question to the bot
Checking conversation history
Using endpoint
From the GAIP UI
GPT Integration

After preparing, uploading and training your data, now you are ready to interact with your bot, by asking it question with texts and images via our endpoints.

Asking question to the bot

To send a text only question/query to GPT, use the endpoint POST /v1/gpt/ask. Find details of the required parameters with explanation and an example request body in our sandbox under "MyGPT"

In the response, you will get the response to the query, along with some other information related, such as the conversation_id, tokens, etc.

For query with image, use the endpoint POST /v1/gpt/ask/vision. There is one additional parameter here, to add the image file. This endpoint will take both the image and the query and respond based on both input. Here you can only use gpt-4-vision-preview model.

Note that if you do not pass any gpt_profile_id, it will use the default profile that was set during the setup.

Checking conversation history

There are two ways to check conversation history 1. Using API endpoint 2. From the GAIP UI.

Using endpoint

Using our endpoint GET /v1/gpt/conversations, you can check conversation history of your project, for a specific date range. By default, (if no time period is specified), this endpoint will give the conversation history for last 7 days. If you want to see the response detail of any particular conversation, you can use your endpoint GET /v1/gpt/conversations/{conversation_id}. This endpoint will take the conversation ID and will give the detail of that particular conversation.

From the GAIP UI

In our platform, navigate to your project's insight page. Here you will find "GPT-Flow History" table. This will show all the conversation during the time range selected above. You can filter this table with user id, query, feedback. You can also see the details of each conversation by clicking on the details icon at the right. To download the history table, use the download button at the top right of the table. This will download the table as CSV. Note that the downloaded file will only contain the information as it is on the table, with the date and other filters applied.

Previous
GPT Project setup
Next
GPT Feedback
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/02_gpt_integration/gpt_integration/#checking-conversation-history
Skip to content
Gigalogy Tutorial
GPT Integration
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
GPT Project setup
GPT Integration
GPT Feedback
API Reference
Release notes
Glossary
Table of contents
Asking question to the bot
Checking conversation history
Using endpoint
From the GAIP UI
GPT Integration

After preparing, uploading and training your data, now you are ready to interact with your bot, by asking it question with texts and images via our endpoints.

Asking question to the bot

To send a text only question/query to GPT, use the endpoint POST /v1/gpt/ask. Find details of the required parameters with explanation and an example request body in our sandbox under "MyGPT"

In the response, you will get the response to the query, along with some other information related, such as the conversation_id, tokens, etc.

For query with image, use the endpoint POST /v1/gpt/ask/vision. There is one additional parameter here, to add the image file. This endpoint will take both the image and the query and respond based on both input. Here you can only use gpt-4-vision-preview model.

Note that if you do not pass any gpt_profile_id, it will use the default profile that was set during the setup.

Checking conversation history

There are two ways to check conversation history 1. Using API endpoint 2. From the GAIP UI.

Using endpoint

Using our endpoint GET /v1/gpt/conversations, you can check conversation history of your project, for a specific date range. By default, (if no time period is specified), this endpoint will give the conversation history for last 7 days. If you want to see the response detail of any particular conversation, you can use your endpoint GET /v1/gpt/conversations/{conversation_id}. This endpoint will take the conversation ID and will give the detail of that particular conversation.

From the GAIP UI

In our platform, navigate to your project's insight page. Here you will find "GPT-Flow History" table. This will show all the conversation during the time range selected above. You can filter this table with user id, query, feedback. You can also see the details of each conversation by clicking on the details icon at the right. To download the history table, use the download button at the top right of the table. This will download the table as CSV. Note that the downloaded file will only contain the information as it is on the table, with the date and other filters applied.

Previous
GPT Project setup
Next
GPT Feedback
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/02_gpt_integration/gpt_integration/#using-endpoint
Skip to content
Gigalogy Tutorial
GPT Integration
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
GPT Project setup
GPT Integration
GPT Feedback
API Reference
Release notes
Glossary
Table of contents
Asking question to the bot
Checking conversation history
Using endpoint
From the GAIP UI
GPT Integration

After preparing, uploading and training your data, now you are ready to interact with your bot, by asking it question with texts and images via our endpoints.

Asking question to the bot

To send a text only question/query to GPT, use the endpoint POST /v1/gpt/ask. Find details of the required parameters with explanation and an example request body in our sandbox under "MyGPT"

In the response, you will get the response to the query, along with some other information related, such as the conversation_id, tokens, etc.

For query with image, use the endpoint POST /v1/gpt/ask/vision. There is one additional parameter here, to add the image file. This endpoint will take both the image and the query and respond based on both input. Here you can only use gpt-4-vision-preview model.

Note that if you do not pass any gpt_profile_id, it will use the default profile that was set during the setup.

Checking conversation history

There are two ways to check conversation history 1. Using API endpoint 2. From the GAIP UI.

Using endpoint

Using our endpoint GET /v1/gpt/conversations, you can check conversation history of your project, for a specific date range. By default, (if no time period is specified), this endpoint will give the conversation history for last 7 days. If you want to see the response detail of any particular conversation, you can use your endpoint GET /v1/gpt/conversations/{conversation_id}. This endpoint will take the conversation ID and will give the detail of that particular conversation.

From the GAIP UI

In our platform, navigate to your project's insight page. Here you will find "GPT-Flow History" table. This will show all the conversation during the time range selected above. You can filter this table with user id, query, feedback. You can also see the details of each conversation by clicking on the details icon at the right. To download the history table, use the download button at the top right of the table. This will download the table as CSV. Note that the downloaded file will only contain the information as it is on the table, with the date and other filters applied.

Previous
GPT Project setup
Next
GPT Feedback
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/02_gpt_integration/gpt_integration/#from-the-gaip-ui
Skip to content
Gigalogy Tutorial
GPT Integration
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
GPT Project setup
GPT Integration
GPT Feedback
API Reference
Release notes
Glossary
Table of contents
Asking question to the bot
Checking conversation history
Using endpoint
From the GAIP UI
GPT Integration

After preparing, uploading and training your data, now you are ready to interact with your bot, by asking it question with texts and images via our endpoints.

Asking question to the bot

To send a text only question/query to GPT, use the endpoint POST /v1/gpt/ask. Find details of the required parameters with explanation and an example request body in our sandbox under "MyGPT"

In the response, you will get the response to the query, along with some other information related, such as the conversation_id, tokens, etc.

For query with image, use the endpoint POST /v1/gpt/ask/vision. There is one additional parameter here, to add the image file. This endpoint will take both the image and the query and respond based on both input. Here you can only use gpt-4-vision-preview model.

Note that if you do not pass any gpt_profile_id, it will use the default profile that was set during the setup.

Checking conversation history

There are two ways to check conversation history 1. Using API endpoint 2. From the GAIP UI.

Using endpoint

Using our endpoint GET /v1/gpt/conversations, you can check conversation history of your project, for a specific date range. By default, (if no time period is specified), this endpoint will give the conversation history for last 7 days. If you want to see the response detail of any particular conversation, you can use your endpoint GET /v1/gpt/conversations/{conversation_id}. This endpoint will take the conversation ID and will give the detail of that particular conversation.

From the GAIP UI

In our platform, navigate to your project's insight page. Here you will find "GPT-Flow History" table. This will show all the conversation during the time range selected above. You can filter this table with user id, query, feedback. You can also see the details of each conversation by clicking on the details icon at the right. To download the history table, use the download button at the top right of the table. This will download the table as CSV. Note that the downloaded file will only contain the information as it is on the table, with the date and other filters applied.

Previous
GPT Project setup
Next
GPT Feedback
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/02_gpt_integration/gpt_feedbacks/
Skip to content
Gigalogy Tutorial
GPT Feedback
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
GPT Project setup
GPT Integration
GPT Feedback
API Reference
Release notes
Glossary
Table of contents
Providing Feedback via API
Sharing Feedback via the User Interface
GPT Feedback

After establishing your GPT project and engaging with your bot, you may want to provide response on the dialogue quality.

Providing Feedback via API

To facilitate this, the endpoint PUT /v1/gpt/conversations would be of use. This endpoint allows you to designate specific conversations through their IDs, assign either positive or negative feedback, and optionally include a message conveying your review details. Such feedback is instrumental for refining the model's training and future results optimization.

Sharing Feedback via the User Interface

Under your GPT project's insight tab, you can see the history of your conversations in the the "GPT-Flow History" table. From the "Training data" column, you can edit the query, response and provide feedback. You can also select here which data should be added for future training and under which dataset.

Previous
GPT Integration
Next
API Reference
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/02_gpt_integration/gpt_feedbacks/#providing-feedback-via-api
Skip to content
Gigalogy Tutorial
GPT Feedback
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
GPT Project setup
GPT Integration
GPT Feedback
API Reference
Release notes
Glossary
Table of contents
Providing Feedback via API
Sharing Feedback via the User Interface
GPT Feedback

After establishing your GPT project and engaging with your bot, you may want to provide response on the dialogue quality.

Providing Feedback via API

To facilitate this, the endpoint PUT /v1/gpt/conversations would be of use. This endpoint allows you to designate specific conversations through their IDs, assign either positive or negative feedback, and optionally include a message conveying your review details. Such feedback is instrumental for refining the model's training and future results optimization.

Sharing Feedback via the User Interface

Under your GPT project's insight tab, you can see the history of your conversations in the the "GPT-Flow History" table. From the "Training data" column, you can edit the query, response and provide feedback. You can also select here which data should be added for future training and under which dataset.

Previous
GPT Integration
Next
API Reference
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/02_gpt_integration/gpt_feedbacks/#sharing-feedback-via-the-user-interface
Skip to content
Gigalogy Tutorial
GPT Feedback
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
GPT Project setup
GPT Integration
GPT Feedback
API Reference
Release notes
Glossary
Table of contents
Providing Feedback via API
Sharing Feedback via the User Interface
GPT Feedback

After establishing your GPT project and engaging with your bot, you may want to provide response on the dialogue quality.

Providing Feedback via API

To facilitate this, the endpoint PUT /v1/gpt/conversations would be of use. This endpoint allows you to designate specific conversations through their IDs, assign either positive or negative feedback, and optionally include a message conveying your review details. Such feedback is instrumental for refining the model's training and future results optimization.

Sharing Feedback via the User Interface

Under your GPT project's insight tab, you can see the history of your conversations in the the "GPT-Flow History" table. From the "Training data" column, you can edit the query, response and provide feedback. You can also select here which data should be added for future training and under which dataset.

Previous
GPT Integration
Next
API Reference
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/api_reference/
Gigalogy Tutorial
API Reference
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
API Reference

API Documentation URL

Sandbox URL

Previous
GPT Feedback
Next
Release notes
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/release_notes/
Skip to content
Gigalogy Tutorial
Release notes
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
Table of contents
Apr 5, 2024
Mar 27, 2024
Mar 27, 2024
Mar 26, 2024
Mar 20, 2024
Mar 16, 2024
Mar 3, 2024
Jan 25, 2024
GAIP Release notes
Apr 5, 2024
New feature
questionnaire mapper now supports result format. User can setup how they want to see the questionnaire result. User can setup required categories and optional categories. Recommended items from POST /v1/questionnaire/recommend will be categorized based on that. Optional categories come with their inclusion condition as well.
Mar 27, 2024
New feature
From the GPT-Flow History table, you can now select which data (Query, response, feedback) you want to include in the training for future conversations and in which dataset.
Mar 27, 2024
New feature
Now POST /v1/gpt/ask API have multiple options to generate answer. This can be set as the value of the key conversation_type, question type does not consider any previous conversation history. chat type considers n previous conversation history while generating an answer. The length of the converstaion is defined by the parameter chat_history_length of GPT settings.
Mar 26, 2024
New features
In the same project, /gpt/ask API can now answer from both personalizer catalog and MyGPT datasets. Previously it could handle only one of the data sources.
Introducing "Profiles" for our GPT solutions! You can now create and save multiple profiles, each with its own settings . Users no longer have to manually adjust the GPT settings every time they switch between different tasks or needs.
Mar 20, 2024
New features
keyword extractor changed for japanese language. This is done to improve the quality of japanese language search capabilities. top_k can control how wide or controlled search a customer wants to do in terms of keywords.
questionnaire/recommend end point now has a new optional parameter called recommend_count. If it is set, the value of this parameter takes priority over recommend_count variable in questionnaire mapper.
Mar 16, 2024
New feature
gpt/ask and questionnaire/recommend endpoints, when providing response with product IDs, now can include any key that was set in the mapper.
Now it is easier to set how many product questionnaire/recommend will give, with the new key "recommend_count" in the mapper.
introducing top_k to items/search, gpt/ask, giving ability to controls the number of keywords to consider for document searching.
Bug Fix
questionnaire/recommend endpoint can now take Null values.
Mar 3, 2024
New feature
UI-Based Project Setup: Now set up Personalization Projects directly via the UI, including file uploads, data mapping, and model training.
Training History: View detailed training history in Project Settings for improved tracking and insights.
Manual Training: Initiate model training manually from the UI for greater control over your personalization strategies.
Jan 25, 2024
New features
Add an export button at the bottom of the GPT flow history table. Exports the Table as a CSV file for the specified time range
GAIP sidebar improved: Solutions are accessible directly now.
Big fix
Bug at trial project status and upgrade flow
Previous
API Reference
Next
Glossary
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/release_notes/#gaip-release-notes
Skip to content
Gigalogy Tutorial
Release notes
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
Table of contents
Apr 5, 2024
Mar 27, 2024
Mar 27, 2024
Mar 26, 2024
Mar 20, 2024
Mar 16, 2024
Mar 3, 2024
Jan 25, 2024
GAIP Release notes
Apr 5, 2024
New feature
questionnaire mapper now supports result format. User can setup how they want to see the questionnaire result. User can setup required categories and optional categories. Recommended items from POST /v1/questionnaire/recommend will be categorized based on that. Optional categories come with their inclusion condition as well.
Mar 27, 2024
New feature
From the GPT-Flow History table, you can now select which data (Query, response, feedback) you want to include in the training for future conversations and in which dataset.
Mar 27, 2024
New feature
Now POST /v1/gpt/ask API have multiple options to generate answer. This can be set as the value of the key conversation_type, question type does not consider any previous conversation history. chat type considers n previous conversation history while generating an answer. The length of the converstaion is defined by the parameter chat_history_length of GPT settings.
Mar 26, 2024
New features
In the same project, /gpt/ask API can now answer from both personalizer catalog and MyGPT datasets. Previously it could handle only one of the data sources.
Introducing "Profiles" for our GPT solutions! You can now create and save multiple profiles, each with its own settings . Users no longer have to manually adjust the GPT settings every time they switch between different tasks or needs.
Mar 20, 2024
New features
keyword extractor changed for japanese language. This is done to improve the quality of japanese language search capabilities. top_k can control how wide or controlled search a customer wants to do in terms of keywords.
questionnaire/recommend end point now has a new optional parameter called recommend_count. If it is set, the value of this parameter takes priority over recommend_count variable in questionnaire mapper.
Mar 16, 2024
New feature
gpt/ask and questionnaire/recommend endpoints, when providing response with product IDs, now can include any key that was set in the mapper.
Now it is easier to set how many product questionnaire/recommend will give, with the new key "recommend_count" in the mapper.
introducing top_k to items/search, gpt/ask, giving ability to controls the number of keywords to consider for document searching.
Bug Fix
questionnaire/recommend endpoint can now take Null values.
Mar 3, 2024
New feature
UI-Based Project Setup: Now set up Personalization Projects directly via the UI, including file uploads, data mapping, and model training.
Training History: View detailed training history in Project Settings for improved tracking and insights.
Manual Training: Initiate model training manually from the UI for greater control over your personalization strategies.
Jan 25, 2024
New features
Add an export button at the bottom of the GPT flow history table. Exports the Table as a CSV file for the specified time range
GAIP sidebar improved: Solutions are accessible directly now.
Big fix
Bug at trial project status and upgrade flow
Previous
API Reference
Next
Glossary
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/release_notes/#apr-5-2024
Skip to content
Gigalogy Tutorial
Release notes
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
Table of contents
Apr 5, 2024
Mar 27, 2024
Mar 27, 2024
Mar 26, 2024
Mar 20, 2024
Mar 16, 2024
Mar 3, 2024
Jan 25, 2024
GAIP Release notes
Apr 5, 2024
New feature
questionnaire mapper now supports result format. User can setup how they want to see the questionnaire result. User can setup required categories and optional categories. Recommended items from POST /v1/questionnaire/recommend will be categorized based on that. Optional categories come with their inclusion condition as well.
Mar 27, 2024
New feature
From the GPT-Flow History table, you can now select which data (Query, response, feedback) you want to include in the training for future conversations and in which dataset.
Mar 27, 2024
New feature
Now POST /v1/gpt/ask API have multiple options to generate answer. This can be set as the value of the key conversation_type, question type does not consider any previous conversation history. chat type considers n previous conversation history while generating an answer. The length of the converstaion is defined by the parameter chat_history_length of GPT settings.
Mar 26, 2024
New features
In the same project, /gpt/ask API can now answer from both personalizer catalog and MyGPT datasets. Previously it could handle only one of the data sources.
Introducing "Profiles" for our GPT solutions! You can now create and save multiple profiles, each with its own settings . Users no longer have to manually adjust the GPT settings every time they switch between different tasks or needs.
Mar 20, 2024
New features
keyword extractor changed for japanese language. This is done to improve the quality of japanese language search capabilities. top_k can control how wide or controlled search a customer wants to do in terms of keywords.
questionnaire/recommend end point now has a new optional parameter called recommend_count. If it is set, the value of this parameter takes priority over recommend_count variable in questionnaire mapper.
Mar 16, 2024
New feature
gpt/ask and questionnaire/recommend endpoints, when providing response with product IDs, now can include any key that was set in the mapper.
Now it is easier to set how many product questionnaire/recommend will give, with the new key "recommend_count" in the mapper.
introducing top_k to items/search, gpt/ask, giving ability to controls the number of keywords to consider for document searching.
Bug Fix
questionnaire/recommend endpoint can now take Null values.
Mar 3, 2024
New feature
UI-Based Project Setup: Now set up Personalization Projects directly via the UI, including file uploads, data mapping, and model training.
Training History: View detailed training history in Project Settings for improved tracking and insights.
Manual Training: Initiate model training manually from the UI for greater control over your personalization strategies.
Jan 25, 2024
New features
Add an export button at the bottom of the GPT flow history table. Exports the Table as a CSV file for the specified time range
GAIP sidebar improved: Solutions are accessible directly now.
Big fix
Bug at trial project status and upgrade flow
Previous
API Reference
Next
Glossary
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/release_notes/#mar-27-2024
Skip to content
Gigalogy Tutorial
Release notes
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
Table of contents
Apr 5, 2024
Mar 27, 2024
Mar 27, 2024
Mar 26, 2024
Mar 20, 2024
Mar 16, 2024
Mar 3, 2024
Jan 25, 2024
GAIP Release notes
Apr 5, 2024
New feature
questionnaire mapper now supports result format. User can setup how they want to see the questionnaire result. User can setup required categories and optional categories. Recommended items from POST /v1/questionnaire/recommend will be categorized based on that. Optional categories come with their inclusion condition as well.
Mar 27, 2024
New feature
From the GPT-Flow History table, you can now select which data (Query, response, feedback) you want to include in the training for future conversations and in which dataset.
Mar 27, 2024
New feature
Now POST /v1/gpt/ask API have multiple options to generate answer. This can be set as the value of the key conversation_type, question type does not consider any previous conversation history. chat type considers n previous conversation history while generating an answer. The length of the converstaion is defined by the parameter chat_history_length of GPT settings.
Mar 26, 2024
New features
In the same project, /gpt/ask API can now answer from both personalizer catalog and MyGPT datasets. Previously it could handle only one of the data sources.
Introducing "Profiles" for our GPT solutions! You can now create and save multiple profiles, each with its own settings . Users no longer have to manually adjust the GPT settings every time they switch between different tasks or needs.
Mar 20, 2024
New features
keyword extractor changed for japanese language. This is done to improve the quality of japanese language search capabilities. top_k can control how wide or controlled search a customer wants to do in terms of keywords.
questionnaire/recommend end point now has a new optional parameter called recommend_count. If it is set, the value of this parameter takes priority over recommend_count variable in questionnaire mapper.
Mar 16, 2024
New feature
gpt/ask and questionnaire/recommend endpoints, when providing response with product IDs, now can include any key that was set in the mapper.
Now it is easier to set how many product questionnaire/recommend will give, with the new key "recommend_count" in the mapper.
introducing top_k to items/search, gpt/ask, giving ability to controls the number of keywords to consider for document searching.
Bug Fix
questionnaire/recommend endpoint can now take Null values.
Mar 3, 2024
New feature
UI-Based Project Setup: Now set up Personalization Projects directly via the UI, including file uploads, data mapping, and model training.
Training History: View detailed training history in Project Settings for improved tracking and insights.
Manual Training: Initiate model training manually from the UI for greater control over your personalization strategies.
Jan 25, 2024
New features
Add an export button at the bottom of the GPT flow history table. Exports the Table as a CSV file for the specified time range
GAIP sidebar improved: Solutions are accessible directly now.
Big fix
Bug at trial project status and upgrade flow
Previous
API Reference
Next
Glossary
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/release_notes/#mar-27-2024_1
Skip to content
Gigalogy Tutorial
Release notes
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
Table of contents
Apr 5, 2024
Mar 27, 2024
Mar 27, 2024
Mar 26, 2024
Mar 20, 2024
Mar 16, 2024
Mar 3, 2024
Jan 25, 2024
GAIP Release notes
Apr 5, 2024
New feature
questionnaire mapper now supports result format. User can setup how they want to see the questionnaire result. User can setup required categories and optional categories. Recommended items from POST /v1/questionnaire/recommend will be categorized based on that. Optional categories come with their inclusion condition as well.
Mar 27, 2024
New feature
From the GPT-Flow History table, you can now select which data (Query, response, feedback) you want to include in the training for future conversations and in which dataset.
Mar 27, 2024
New feature
Now POST /v1/gpt/ask API have multiple options to generate answer. This can be set as the value of the key conversation_type, question type does not consider any previous conversation history. chat type considers n previous conversation history while generating an answer. The length of the converstaion is defined by the parameter chat_history_length of GPT settings.
Mar 26, 2024
New features
In the same project, /gpt/ask API can now answer from both personalizer catalog and MyGPT datasets. Previously it could handle only one of the data sources.
Introducing "Profiles" for our GPT solutions! You can now create and save multiple profiles, each with its own settings . Users no longer have to manually adjust the GPT settings every time they switch between different tasks or needs.
Mar 20, 2024
New features
keyword extractor changed for japanese language. This is done to improve the quality of japanese language search capabilities. top_k can control how wide or controlled search a customer wants to do in terms of keywords.
questionnaire/recommend end point now has a new optional parameter called recommend_count. If it is set, the value of this parameter takes priority over recommend_count variable in questionnaire mapper.
Mar 16, 2024
New feature
gpt/ask and questionnaire/recommend endpoints, when providing response with product IDs, now can include any key that was set in the mapper.
Now it is easier to set how many product questionnaire/recommend will give, with the new key "recommend_count" in the mapper.
introducing top_k to items/search, gpt/ask, giving ability to controls the number of keywords to consider for document searching.
Bug Fix
questionnaire/recommend endpoint can now take Null values.
Mar 3, 2024
New feature
UI-Based Project Setup: Now set up Personalization Projects directly via the UI, including file uploads, data mapping, and model training.
Training History: View detailed training history in Project Settings for improved tracking and insights.
Manual Training: Initiate model training manually from the UI for greater control over your personalization strategies.
Jan 25, 2024
New features
Add an export button at the bottom of the GPT flow history table. Exports the Table as a CSV file for the specified time range
GAIP sidebar improved: Solutions are accessible directly now.
Big fix
Bug at trial project status and upgrade flow
Previous
API Reference
Next
Glossary
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/release_notes/#mar-26-2024
Skip to content
Gigalogy Tutorial
Release notes
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
Table of contents
Apr 5, 2024
Mar 27, 2024
Mar 27, 2024
Mar 26, 2024
Mar 20, 2024
Mar 16, 2024
Mar 3, 2024
Jan 25, 2024
GAIP Release notes
Apr 5, 2024
New feature
questionnaire mapper now supports result format. User can setup how they want to see the questionnaire result. User can setup required categories and optional categories. Recommended items from POST /v1/questionnaire/recommend will be categorized based on that. Optional categories come with their inclusion condition as well.
Mar 27, 2024
New feature
From the GPT-Flow History table, you can now select which data (Query, response, feedback) you want to include in the training for future conversations and in which dataset.
Mar 27, 2024
New feature
Now POST /v1/gpt/ask API have multiple options to generate answer. This can be set as the value of the key conversation_type, question type does not consider any previous conversation history. chat type considers n previous conversation history while generating an answer. The length of the converstaion is defined by the parameter chat_history_length of GPT settings.
Mar 26, 2024
New features
In the same project, /gpt/ask API can now answer from both personalizer catalog and MyGPT datasets. Previously it could handle only one of the data sources.
Introducing "Profiles" for our GPT solutions! You can now create and save multiple profiles, each with its own settings . Users no longer have to manually adjust the GPT settings every time they switch between different tasks or needs.
Mar 20, 2024
New features
keyword extractor changed for japanese language. This is done to improve the quality of japanese language search capabilities. top_k can control how wide or controlled search a customer wants to do in terms of keywords.
questionnaire/recommend end point now has a new optional parameter called recommend_count. If it is set, the value of this parameter takes priority over recommend_count variable in questionnaire mapper.
Mar 16, 2024
New feature
gpt/ask and questionnaire/recommend endpoints, when providing response with product IDs, now can include any key that was set in the mapper.
Now it is easier to set how many product questionnaire/recommend will give, with the new key "recommend_count" in the mapper.
introducing top_k to items/search, gpt/ask, giving ability to controls the number of keywords to consider for document searching.
Bug Fix
questionnaire/recommend endpoint can now take Null values.
Mar 3, 2024
New feature
UI-Based Project Setup: Now set up Personalization Projects directly via the UI, including file uploads, data mapping, and model training.
Training History: View detailed training history in Project Settings for improved tracking and insights.
Manual Training: Initiate model training manually from the UI for greater control over your personalization strategies.
Jan 25, 2024
New features
Add an export button at the bottom of the GPT flow history table. Exports the Table as a CSV file for the specified time range
GAIP sidebar improved: Solutions are accessible directly now.
Big fix
Bug at trial project status and upgrade flow
Previous
API Reference
Next
Glossary
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/release_notes/#mar-20-2024
Skip to content
Gigalogy Tutorial
Release notes
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
Table of contents
Apr 5, 2024
Mar 27, 2024
Mar 27, 2024
Mar 26, 2024
Mar 20, 2024
Mar 16, 2024
Mar 3, 2024
Jan 25, 2024
GAIP Release notes
Apr 5, 2024
New feature
questionnaire mapper now supports result format. User can setup how they want to see the questionnaire result. User can setup required categories and optional categories. Recommended items from POST /v1/questionnaire/recommend will be categorized based on that. Optional categories come with their inclusion condition as well.
Mar 27, 2024
New feature
From the GPT-Flow History table, you can now select which data (Query, response, feedback) you want to include in the training for future conversations and in which dataset.
Mar 27, 2024
New feature
Now POST /v1/gpt/ask API have multiple options to generate answer. This can be set as the value of the key conversation_type, question type does not consider any previous conversation history. chat type considers n previous conversation history while generating an answer. The length of the converstaion is defined by the parameter chat_history_length of GPT settings.
Mar 26, 2024
New features
In the same project, /gpt/ask API can now answer from both personalizer catalog and MyGPT datasets. Previously it could handle only one of the data sources.
Introducing "Profiles" for our GPT solutions! You can now create and save multiple profiles, each with its own settings . Users no longer have to manually adjust the GPT settings every time they switch between different tasks or needs.
Mar 20, 2024
New features
keyword extractor changed for japanese language. This is done to improve the quality of japanese language search capabilities. top_k can control how wide or controlled search a customer wants to do in terms of keywords.
questionnaire/recommend end point now has a new optional parameter called recommend_count. If it is set, the value of this parameter takes priority over recommend_count variable in questionnaire mapper.
Mar 16, 2024
New feature
gpt/ask and questionnaire/recommend endpoints, when providing response with product IDs, now can include any key that was set in the mapper.
Now it is easier to set how many product questionnaire/recommend will give, with the new key "recommend_count" in the mapper.
introducing top_k to items/search, gpt/ask, giving ability to controls the number of keywords to consider for document searching.
Bug Fix
questionnaire/recommend endpoint can now take Null values.
Mar 3, 2024
New feature
UI-Based Project Setup: Now set up Personalization Projects directly via the UI, including file uploads, data mapping, and model training.
Training History: View detailed training history in Project Settings for improved tracking and insights.
Manual Training: Initiate model training manually from the UI for greater control over your personalization strategies.
Jan 25, 2024
New features
Add an export button at the bottom of the GPT flow history table. Exports the Table as a CSV file for the specified time range
GAIP sidebar improved: Solutions are accessible directly now.
Big fix
Bug at trial project status and upgrade flow
Previous
API Reference
Next
Glossary
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/release_notes/#mar-16-2024
Skip to content
Gigalogy Tutorial
Release notes
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
Table of contents
Apr 5, 2024
Mar 27, 2024
Mar 27, 2024
Mar 26, 2024
Mar 20, 2024
Mar 16, 2024
Mar 3, 2024
Jan 25, 2024
GAIP Release notes
Apr 5, 2024
New feature
questionnaire mapper now supports result format. User can setup how they want to see the questionnaire result. User can setup required categories and optional categories. Recommended items from POST /v1/questionnaire/recommend will be categorized based on that. Optional categories come with their inclusion condition as well.
Mar 27, 2024
New feature
From the GPT-Flow History table, you can now select which data (Query, response, feedback) you want to include in the training for future conversations and in which dataset.
Mar 27, 2024
New feature
Now POST /v1/gpt/ask API have multiple options to generate answer. This can be set as the value of the key conversation_type, question type does not consider any previous conversation history. chat type considers n previous conversation history while generating an answer. The length of the converstaion is defined by the parameter chat_history_length of GPT settings.
Mar 26, 2024
New features
In the same project, /gpt/ask API can now answer from both personalizer catalog and MyGPT datasets. Previously it could handle only one of the data sources.
Introducing "Profiles" for our GPT solutions! You can now create and save multiple profiles, each with its own settings . Users no longer have to manually adjust the GPT settings every time they switch between different tasks or needs.
Mar 20, 2024
New features
keyword extractor changed for japanese language. This is done to improve the quality of japanese language search capabilities. top_k can control how wide or controlled search a customer wants to do in terms of keywords.
questionnaire/recommend end point now has a new optional parameter called recommend_count. If it is set, the value of this parameter takes priority over recommend_count variable in questionnaire mapper.
Mar 16, 2024
New feature
gpt/ask and questionnaire/recommend endpoints, when providing response with product IDs, now can include any key that was set in the mapper.
Now it is easier to set how many product questionnaire/recommend will give, with the new key "recommend_count" in the mapper.
introducing top_k to items/search, gpt/ask, giving ability to controls the number of keywords to consider for document searching.
Bug Fix
questionnaire/recommend endpoint can now take Null values.
Mar 3, 2024
New feature
UI-Based Project Setup: Now set up Personalization Projects directly via the UI, including file uploads, data mapping, and model training.
Training History: View detailed training history in Project Settings for improved tracking and insights.
Manual Training: Initiate model training manually from the UI for greater control over your personalization strategies.
Jan 25, 2024
New features
Add an export button at the bottom of the GPT flow history table. Exports the Table as a CSV file for the specified time range
GAIP sidebar improved: Solutions are accessible directly now.
Big fix
Bug at trial project status and upgrade flow
Previous
API Reference
Next
Glossary
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/release_notes/#mar-3-2024
Skip to content
Gigalogy Tutorial
Release notes
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
Table of contents
Apr 5, 2024
Mar 27, 2024
Mar 27, 2024
Mar 26, 2024
Mar 20, 2024
Mar 16, 2024
Mar 3, 2024
Jan 25, 2024
GAIP Release notes
Apr 5, 2024
New feature
questionnaire mapper now supports result format. User can setup how they want to see the questionnaire result. User can setup required categories and optional categories. Recommended items from POST /v1/questionnaire/recommend will be categorized based on that. Optional categories come with their inclusion condition as well.
Mar 27, 2024
New feature
From the GPT-Flow History table, you can now select which data (Query, response, feedback) you want to include in the training for future conversations and in which dataset.
Mar 27, 2024
New feature
Now POST /v1/gpt/ask API have multiple options to generate answer. This can be set as the value of the key conversation_type, question type does not consider any previous conversation history. chat type considers n previous conversation history while generating an answer. The length of the converstaion is defined by the parameter chat_history_length of GPT settings.
Mar 26, 2024
New features
In the same project, /gpt/ask API can now answer from both personalizer catalog and MyGPT datasets. Previously it could handle only one of the data sources.
Introducing "Profiles" for our GPT solutions! You can now create and save multiple profiles, each with its own settings . Users no longer have to manually adjust the GPT settings every time they switch between different tasks or needs.
Mar 20, 2024
New features
keyword extractor changed for japanese language. This is done to improve the quality of japanese language search capabilities. top_k can control how wide or controlled search a customer wants to do in terms of keywords.
questionnaire/recommend end point now has a new optional parameter called recommend_count. If it is set, the value of this parameter takes priority over recommend_count variable in questionnaire mapper.
Mar 16, 2024
New feature
gpt/ask and questionnaire/recommend endpoints, when providing response with product IDs, now can include any key that was set in the mapper.
Now it is easier to set how many product questionnaire/recommend will give, with the new key "recommend_count" in the mapper.
introducing top_k to items/search, gpt/ask, giving ability to controls the number of keywords to consider for document searching.
Bug Fix
questionnaire/recommend endpoint can now take Null values.
Mar 3, 2024
New feature
UI-Based Project Setup: Now set up Personalization Projects directly via the UI, including file uploads, data mapping, and model training.
Training History: View detailed training history in Project Settings for improved tracking and insights.
Manual Training: Initiate model training manually from the UI for greater control over your personalization strategies.
Jan 25, 2024
New features
Add an export button at the bottom of the GPT flow history table. Exports the Table as a CSV file for the specified time range
GAIP sidebar improved: Solutions are accessible directly now.
Big fix
Bug at trial project status and upgrade flow
Previous
API Reference
Next
Glossary
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/release_notes/#jan-25-2024
Skip to content
Gigalogy Tutorial
Release notes
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
Table of contents
Apr 5, 2024
Mar 27, 2024
Mar 27, 2024
Mar 26, 2024
Mar 20, 2024
Mar 16, 2024
Mar 3, 2024
Jan 25, 2024
GAIP Release notes
Apr 5, 2024
New feature
questionnaire mapper now supports result format. User can setup how they want to see the questionnaire result. User can setup required categories and optional categories. Recommended items from POST /v1/questionnaire/recommend will be categorized based on that. Optional categories come with their inclusion condition as well.
Mar 27, 2024
New feature
From the GPT-Flow History table, you can now select which data (Query, response, feedback) you want to include in the training for future conversations and in which dataset.
Mar 27, 2024
New feature
Now POST /v1/gpt/ask API have multiple options to generate answer. This can be set as the value of the key conversation_type, question type does not consider any previous conversation history. chat type considers n previous conversation history while generating an answer. The length of the converstaion is defined by the parameter chat_history_length of GPT settings.
Mar 26, 2024
New features
In the same project, /gpt/ask API can now answer from both personalizer catalog and MyGPT datasets. Previously it could handle only one of the data sources.
Introducing "Profiles" for our GPT solutions! You can now create and save multiple profiles, each with its own settings . Users no longer have to manually adjust the GPT settings every time they switch between different tasks or needs.
Mar 20, 2024
New features
keyword extractor changed for japanese language. This is done to improve the quality of japanese language search capabilities. top_k can control how wide or controlled search a customer wants to do in terms of keywords.
questionnaire/recommend end point now has a new optional parameter called recommend_count. If it is set, the value of this parameter takes priority over recommend_count variable in questionnaire mapper.
Mar 16, 2024
New feature
gpt/ask and questionnaire/recommend endpoints, when providing response with product IDs, now can include any key that was set in the mapper.
Now it is easier to set how many product questionnaire/recommend will give, with the new key "recommend_count" in the mapper.
introducing top_k to items/search, gpt/ask, giving ability to controls the number of keywords to consider for document searching.
Bug Fix
questionnaire/recommend endpoint can now take Null values.
Mar 3, 2024
New feature
UI-Based Project Setup: Now set up Personalization Projects directly via the UI, including file uploads, data mapping, and model training.
Training History: View detailed training history in Project Settings for improved tracking and insights.
Manual Training: Initiate model training manually from the UI for greater control over your personalization strategies.
Jan 25, 2024
New features
Add an export button at the bottom of the GPT flow history table. Exports the Table as a CSV file for the specified time range
GAIP sidebar improved: Solutions are accessible directly now.
Big fix
Bug at trial project status and upgrade flow
Previous
API Reference
Next
Glossary
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/glossary/
Skip to content
Gigalogy Tutorial
Glossary
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
Table of contents
Index
Mapper
Reindex
Index Settings
Index Mappings
Glossary
Index

Index are the data organizing mechanism which are similar to the database of relational database system.

Mapper

Mapper is the system to map recommender key to data source key. Therefore, solution can understand data from any data source generate intelligence from them.

Reindex

Reindexing is a process to copy documents from one index to another, optionally filtering the source documents by a query, fetching the documents from a remote cluster. You will about to change the settings and mappings of a destination index with reindex.

Index Settings

Index settings are one of the index modules created per index and control all aspects of the index.

Index Mappings

Index mapping is the process of defining how a document and the fields it contained are sorted and indexed.

Previous
Release notes
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/glossary/#index
Skip to content
Gigalogy Tutorial
Glossary
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
Table of contents
Index
Mapper
Reindex
Index Settings
Index Mappings
Glossary
Index

Index are the data organizing mechanism which are similar to the database of relational database system.

Mapper

Mapper is the system to map recommender key to data source key. Therefore, solution can understand data from any data source generate intelligence from them.

Reindex

Reindexing is a process to copy documents from one index to another, optionally filtering the source documents by a query, fetching the documents from a remote cluster. You will about to change the settings and mappings of a destination index with reindex.

Index Settings

Index settings are one of the index modules created per index and control all aspects of the index.

Index Mappings

Index mapping is the process of defining how a document and the fields it contained are sorted and indexed.

Previous
Release notes
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/glossary/#mapper
Skip to content
Gigalogy Tutorial
Glossary
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
Table of contents
Index
Mapper
Reindex
Index Settings
Index Mappings
Glossary
Index

Index are the data organizing mechanism which are similar to the database of relational database system.

Mapper

Mapper is the system to map recommender key to data source key. Therefore, solution can understand data from any data source generate intelligence from them.

Reindex

Reindexing is a process to copy documents from one index to another, optionally filtering the source documents by a query, fetching the documents from a remote cluster. You will about to change the settings and mappings of a destination index with reindex.

Index Settings

Index settings are one of the index modules created per index and control all aspects of the index.

Index Mappings

Index mapping is the process of defining how a document and the fields it contained are sorted and indexed.

Previous
Release notes
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/glossary/#reindex
Skip to content
Gigalogy Tutorial
Glossary
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
Table of contents
Index
Mapper
Reindex
Index Settings
Index Mappings
Glossary
Index

Index are the data organizing mechanism which are similar to the database of relational database system.

Mapper

Mapper is the system to map recommender key to data source key. Therefore, solution can understand data from any data source generate intelligence from them.

Reindex

Reindexing is a process to copy documents from one index to another, optionally filtering the source documents by a query, fetching the documents from a remote cluster. You will about to change the settings and mappings of a destination index with reindex.

Index Settings

Index settings are one of the index modules created per index and control all aspects of the index.

Index Mappings

Index mapping is the process of defining how a document and the fields it contained are sorted and indexed.

Previous
Release notes
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/glossary/#index-settings
Skip to content
Gigalogy Tutorial
Glossary
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
Table of contents
Index
Mapper
Reindex
Index Settings
Index Mappings
Glossary
Index

Index are the data organizing mechanism which are similar to the database of relational database system.

Mapper

Mapper is the system to map recommender key to data source key. Therefore, solution can understand data from any data source generate intelligence from them.

Reindex

Reindexing is a process to copy documents from one index to another, optionally filtering the source documents by a query, fetching the documents from a remote cluster. You will about to change the settings and mappings of a destination index with reindex.

Index Settings

Index settings are one of the index modules created per index and control all aspects of the index.

Index Mappings

Index mapping is the process of defining how a document and the fields it contained are sorted and indexed.

Previous
Release notes
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/glossary/#index-mappings
Skip to content
Gigalogy Tutorial
Glossary
English
Japanese
Type to start searching
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
Table of contents
Index
Mapper
Reindex
Index Settings
Index Mappings
Glossary
Index

Index are the data organizing mechanism which are similar to the database of relational database system.

Mapper

Mapper is the system to map recommender key to data source key. Therefore, solution can understand data from any data source generate intelligence from them.

Reindex

Reindexing is a process to copy documents from one index to another, optionally filtering the source documents by a query, fetching the documents from a remote cluster. You will about to change the settings and mappings of a destination index with reindex.

Index Settings

Index settings are one of the index modules created per index and control all aspects of the index.

Index Mappings

Index mapping is the process of defining how a document and the fields it contained are sorted and indexed.

Previous
Release notes
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/api_reference/None/v1/documentation
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/api_reference/None/v1/sandbox
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/02_gpt_integration/gpt_integration/None/v1/sandbox
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/02_gpt_integration/gpt_project_setup/None/v1/sandbox
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/02_gpt_integration/gpt_project_setup/None/v1/sandbox#/MyGPT%20Dataset/post_gpt_dataset_v1_gpt_datasets_post
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/02_gpt_integration/gpt_project_setup/None/v1/sandbox#/MyGPT%20Dataset/gpt_dataset_delete_v1_gpt_datasets__dataset_id__delete
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/01_Integration/Questionnaire/None/v1/sandbox#/questionnaire
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/01_Integration/Questionnaire/None/v1/sandbox#/Recommendation/post_questionnaire_recommend_v1_questionnaire_recommend_post
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/01_Integration/purchased_together/None/v1/sandbox#/Recommendation/item_purchased_together_v1_items_purchased_together_post
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/01_Integration/purchased_together/None/v1/documentation#tag/Recommendation/operation/item_purchased_together_v1_items_purchased_together_post
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/01_Integration/04_Recommend_Similar_Products_on_Details_Page/None/v1/sandbox#/Recommendation/item_recommend_details_v1_items_recommend_post
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/01_Integration/04_Recommend_Similar_Products_on_Details_Page/None/v1/documentation#tag/Recommendation/operation/item_recommend_details_v1_items_recommend_post
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/01_Integration/03_Trending_Items_Recommendation_Engine/None/v1/sandbox#/Training%20Settings/save_rank_settings_v1_ranks_settings_post
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/01_Integration/03_Trending_Items_Recommendation_Engine/None/v1/sandbox#/Recommendation/rank_recommend_v1_items_trending_post
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/01_Integration/03_Trending_Items_Recommendation_Engine/None/v1/documentation#tag/Recommendation/operation/rank_recommend_v1_items_trending_post
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/01_Integration/02_Personalized_Feed/None/v1/sandbox#/Recommendation/behavior_recommend_details_v1_users_recommend_post
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/01_Integration/02_Personalized_Feed/None/v1/documentation#tag/Recommendation/operation/behavior_recommend_details_v1_users_recommend_post
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/01_Integration/02_Personalized_Feed/None/v1/sandbox#/Recommendation/search_recommend_details_v1_users_search_recommend_post
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/01_Integration/02_Personalized_Feed/None/v1/documentation#tag/Recommendation/operation/search_recommend_details_v1_users_search_recommend_post
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/01_Integration/06_Image_Search_Engine/None/v1/sandbox#/Search/image_search_v1_images_search_post
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/01_Integration/06_Image_Search_Engine/None/v1/documentation#tag/Search/operation/image_search_v1_images_search_post
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/01_Integration/00_Personalized_search_engine/None/v1/sandbox#/search/item_search_details_v1_item_search_post
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/01_Integration/00_Personalized_search_engine/None/v1/documentation#tag/search/operation/item_search_details_v1_item_search_post
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/mkdocs/docs/tutorial/00_Setup_and_training/03_Data_integration_&_user_behavior_collection.md##Import-user-behavior-data
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/04_Training/None/v1/sandbox#/task
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/03_Data_integration_%26_user_behavior_collection/None/v1/documentation#tag/Catalog-Integration/operation/api_save_v1_items_save_remote_post
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/03_Data_integration_%26_user_behavior_collection/None/v1/sandbox#/task/get_task_v1_tasks__task_id__get
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/03_Data_integration_%26_user_behavior_collection/None/v1/sandbox#/User%20Data%20Collection
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/03_Data_integration_%26_user_behavior_collection/##Import-user-behavior-data
Skip to content
Gigalogy Tutorial
Integration of Catalogue information and user behavior data
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
Project setup
Environment setup
Integration of Catalogue information and user behavior data
Training your data
Personalized search
Personalized Image Search
Personalized Feed
Recommend trending items
Recommend similar items
Recommend items purchase together
Dynamic Pricing
Questionnaire
MyGPT
API Reference
Release notes
Glossary
Table of contents
Catalogue information integration.
Uploading data using a file
Fetch item from external API
Search Items in GAIP after import
User behavior data collection integration
Comparison of each approach.
Google Tag manager
Generating user ID
Collecting and sending user browsing data
Collecting and sending user purchase data
Collecting and sending user rating data
Collecting and sending user data
Server to server integration
User information
Product browse
Product purchase
Product rating
Client to server integration
Import user behavior data
Integration of Catalogue information and user behavior data

This tutorial will cover how to integrate your catalogue information into GAIP and how to set up user behavior tracking and integrate with GAIP.

Catalogue information integration.

Prerequisite: Mapping creation and the index creation is done.

Info

This step can also be done from our platform (GAIP). Refer here for detail.

There are two ways to import your catalogue information into GAIP.

Upload you catalogue information as a CSV or JSON file using endpoint POST /v1/item/save.
Fetch data from your API (or any external API) using endpoint POST /v1/item/save/remote.
Uploading data using a file

For uploading the data using a CSV file or JSON file, please use the POST /v1/item/save endpoint. Simply upload the file and confirm the server response is success. Confirm the task was successful using the GET/v1/tasks/{task_id} endpoint.

Info

This will throw an error and task will fail, if the keys during the Mapping creation step does not match with the keys in the file, OR if the indices were not created succesfully.

Fetch item from external API

To fetch data from external API, use the POST /v1/item/save/remote endpoint. The key and value types and an example request body for the endpoint can be found here in our sandbox.

After hitting either of the endpoint above to import your data into GAIP, you will get a task ID in the response. Use this task ID and hit the /v1/tasks/{task_id} endpoint to confirm the operation was successful. In case it fails, you can also find the details there. For API documentation, please refer <>

Search Items in GAIP after import

You can search items by passing list of item ids fromPOST /v1/items/search endpoint. This endpoint will return searched items with item details.

It is recommended to use this endpoint to confirm that the item catalogue is successfully imported into your project.

User behavior data collection integration

GAIP can collect different user behavior related information to optimize the recommendation for the user. Types of data collected are listed below with their endpoints.

Data type	Endpoint
Product browsing: When user browse products.	/v1/items/browse or /v1/items/browse/client
Product Purchase: When user purchase a product with its quantity.	/v1/items/purchase or /v1/items/purchase/client
Rating: When a user rates a product.	/v1/items/rating or /v1/items/rating/client
User: User information such as age, gender and other customized attributes depending on your website.	/v1/users or /v1/users/client

You will find these endpoints listed in our Sandbox under section "User Data Collection". Please check the required parameters, value types and example request bodies for all the endpoints there.

There are 3 ways to integrate user behavior data collection with GAIP

Google Tag Manager
Server to server integration
Client to server integration

You can also bulk upload user behavior data from the past. For that, please refer to Import user behavior data section.

Comparison of each approach.
Approach	GTM	Server to Server	Client to Server
Description	Use Google Tag Manager (GTM) to collect data (User behavior) from your website and send it to GAIP via endpoint.	The data is captured in the backend server of your application and then sent to GAIP via endpoint.	The data is directly sent from your front end (Client side) to GAIP via endpoint.
Pros	Easy to implement, Minimum coding required, Flexible configuration	More secure, Data integrity, Controlled environment	Real-time data, Less server dependency
Cons	Limited customization, need to have basic knowledge about GTM, Dependency on third-party service, Might not work for certain browsers and plugins like AdBlockers	More complex to set up, potential latency, maintenance required	Less secure, Potential for inconsistent data, dependency on client-side behavior

Below we will show the implementation of each approach.

BE ADVISED: The following is a general guideline, and it may vary across different websites, contingent upon the specific implementation of your website.

Google Tag manager

Prerequisite: Your website must have GTM setup. If you do not have GTM setup, you can easily do the setup by following the guidelines here.

If you are not familiar with basic GTM concepts, such as Tags, Triggers and Variables, please familiarize yourself first with these concepts before proceeding with this approach. You can find more resources related to this here.

Generating user ID

In our sandbox, Notice that in the user data collection endpoints, every endpoint has a parameter called user_id, and member_id. These are vital to identify each user so that you can personalize their experience. user_id is generated by GAIP for each user of your site. The endpoint to generate and user_id is GET /v1/users/generate/id. You can generate the user_id using GTM using below code. This code can be used with every Tag, which checks if there is a user_id and creates one if there is none.

js // Function to get or generate 'gaip_user_id' using a function expression var getGaipUser = function() { return new Promise(function(resolve, reject) { if (gaipUser !== null) { resolve(gaipUser); } else { var idHeaders = new Headers(); idHeaders.append("project-key", "{{ YOUR_PROJECT_KEY_HERE }}"); idHeaders.append("api-key", "{{ YOUR_API_KEY_HERE }}"); fetch("None/v1/users/generate/id", { headers: idHeaders }) .then(function(response) { return response.json(); }) .then(function(data) { localStorage.setItem('gaip_user_id', data.detail.response); resolve(data.detail.response); }) .catch(reject); } }); };

member_id can be set by you depending on how your site identifies unique users such as user ID, phone number, email address etc.

Collecting and sending user browsing data

Set up a variable to capture the product name/title/ID, when the user goes a product details page or clicks on a product to enlarge it or open a pop-up etc.

Set up a trigger so that the tag would fire when the user goes browses an item (Go to product detail page or quick view options etc.).

Create a custom HTML Tag with the above trigger and variable and put the below code in the tag.

Collecting and sending user purchase data

Set up a variable to capture the all the purchase detail, when the user makes a purchase. This could be from the purchase confirmation page etc.

Set up a trigger so that the tag would fire when the user make the purchase.

Create a custom HTML Tag with the above trigger to send the information to endpoint POST /v1/purchase or POST /v1/purchase/client

Collecting and sending user rating data

Setup variables to capture the product name/title/ID and the rating, when the user rates an item positively or negatively. We can also consider an item is positively rated when user adds the item to wishlist.

Set up a trigger for the tag to fire when the user rates a product.

Create a custom HTML Tag with the above trigger and variables to send the information to the endpoint POST /v1/rating or POST /v1/rating/client

Collecting and sending user data

Setup variable to capture the user information.

For this, the trigger could be setup up when the user logs or update their information.

Create a custom HTML Tag with the above trigger and variables to send the information to the endpoint POST /v1/user or POST /v1/user/client

Server to server integration

For server to server integration, you will need to generate Project key and API as mentioned in the credentials section.

User information

The below request path, takes user information, such as name, age, gender, address and saves them in the gaip database.

POST /v1/user

Here is an example request body

json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "user_info": { "address": "string", "gender": "integer --> 1 for male or 2 for female or 3 for others", "age": 25, "user_type": [ { "key_name1": "value1_value2", "separator": "_" }, { "key_name2": "value3" } ] } } You can find the sample code for implementation here

Product browse

You can use the below endpoint to capture user browsing information and save them in GAIP database js POST /v1/items/browse

It takes user_id and item_id as required parameters.

Here is an example value of the request body JSON { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_id": "1000764491" }

You can find sample code here

Product purchase

You can use the below endpoint to capture user's product purchase information and save them in GAIP database

POST /v1/purchase It takes user_id, item_list which includes item_id, price, quantity for a specific item as required parameters.

Here is an example request body json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_list": [ { "item_id": "1000757666", "price": 5000, "quantity": 1 }, { "item_id": "1000764491", "price": 400, "quantity": 7 } ] }

You a find sample code for this implementation here

Product rating

You can use the below endpoint to capture user's product rating information and save them in GAIP database

POST /v1/rating

It takes user_id, item_id, and rating for the specific item as required parameters.

Here is a sample request body

json { "user_id": "a0cc6beb-2909-459b-be55-62196af78ce4", "member_id": "df3456tg-2909-459b-be55-62196afedf85", "item_id": "1000764491", "rating": "1" }

You can find the sample code for this implementation here

If you want to save your data with bulk upload you can use above-mentioned endpoint.

Client to server integration

For client to server integration, you will need to generate client key as described in the Credentials sections. Once the client key is ready, you can directly send the request from your client side to GAIP, using the client key provided.

Note that while generating client key, you can add whitelisted domains, which whitelists the request origin. This is recommended to enhance security.

The rest of the implementation method is same as server to server integration.

Import user behavior data

Similar to data integration, all 4 kinds of user information (browse, purchase, rating, user) can be bulk uploaded. This could be useful if you already have this information from the past and want to import it into GAIP.

To import user behavior and user information in bulk, first you need to create mapper to match the keys with GAIP.

To create the mapper, the endpoints with the example request bodies can be found here in the gigalogy recommender page. You can also find the sample codes for mapper creation here in the API documentation page

Next we will use the below 4 endpoints to upload each category of data in bulk

Request path for product browsing history: POST /v1/items/browse/save
Request path for purchase history: POST /v1/items/purchase/save
Request path for rating history: POST /v1/items/rating/save
Request path to upload user information in bulk: POST /v1/users/save

You can find these endpoints with the example request bodies here. The sample code can be found here in the API documentation page

Previous
Environment setup
Next
Training your data
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/03_Data_integration_%26_user_behavior_collection/None/v1/documentation#tag/behavior/operation/post_v1_user_post
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/03_Data_integration_%26_user_behavior_collection/None/v1/documentation#tag/behavior/operation/post_v1_browse_post
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/03_Data_integration_%26_user_behavior_collection/None/v1/documentation#tag/behavior/operation/purchase_v1_purchase_post
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/03_Data_integration_%26_user_behavior_collection/None/v1/documentation#tag/behavior/operation/post_v1_rating_post
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/03_Data_integration_%26_user_behavior_collection/None/v1/sandbox#/behavior%20mapper
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/03_Data_integration_%26_user_behavior_collection/None/v1/documentation#tag/behavior-mapper
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/03_Data_integration_%26_user_behavior_collection/None/v1/documentation
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/03_Data_integration_%26_user_behavior_collection/None/v1/sandbox#/Historical%20User%20Data%20Collection/post_browse_csv_v1_items_browse_save_post
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/03_Data_integration_%26_user_behavior_collection/None/v1/sandbox#/Historical%20User%20Data%20Collection/post_purchase_csv_v1_items_purchase_save_post
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/03_Data_integration_%26_user_behavior_collection/None/v1/sandbox#/Historical%20User%20Data%20Collection/post_rating_csv_v1_items_rating_save_post
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/03_Data_integration_%26_user_behavior_collection/None/v1/sandbox#/Historical%20User%20Data%20Collection/post_user_csv_v1_users_save_post
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/03_Data_integration_%26_user_behavior_collection/None/v1/sandbox#/behavior
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/03_Data_integration_%26_user_behavior_collection/None/v1/documentation#tag/behavior
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/02_environment_setup/None/v1/sandbox
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/02_environment_setup/None/v1/sandbox#/
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/02_environment_setup/None/v1/sandbox#/Catalog%20Mapping/post_mapper_v1_mappers_post
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/tutorial/00_Setup_and_training/02_environment_setup/None/v1/documentation#tag/item-mapper/operation/post_mapper_v1_mapper_post
Gigalogy Tutorial
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
404 - Not found
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/#tutorial
Skip to content
Gigalogy Tutorial
Overview
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
Table of contents
Tutorial
API Reference
Overview

The Gigalogy AI platform (GAIP) introduces cutting-edge solutions, designed to craft personalized product offerings. These solutions address specific user needs while propelling business growth. The platform provides solutions for personalization, Custom GPT creation and CV based solution. Within this documentation, you will discover comprehensive guidance on integrating the solutions, understanding its operational framework, and leveraging its capabilities within your application.

Let's have a rundown of the documentation:

Tutorial

To seamlessly integrate your product with our solutions, this section serves as your comprehensive guide. It provides a detailed, step-by-step walkthrough for integrating an API with our solutions. Additionally, you'll gain insights into how the solution function and discover strategies for effectively building your application with it. For an optimal integration process, it is advisable to thoroughly review this section prior to integrating your product.

Get started

API Reference

This section offers developer-friendly API documentation designed to guide you through accessing and utilizing the recommender API effectively.

API Reference

Next
Account and Project creation
Made with Material for MkDocs
================================================================================
URL: https://tutorial.gigalogy.com/#api-reference
Skip to content
Gigalogy Tutorial
Overview
English
Japanese
Initializing search
Gigalogy Tutorial
Overview
Account and Project creation
Credentials
Personalizer
MyGPT
API Reference
Release notes
Glossary
Table of contents
Tutorial
API Reference
Overview

The Gigalogy AI platform (GAIP) introduces cutting-edge solutions, designed to craft personalized product offerings. These solutions address specific user needs while propelling business growth. The platform provides solutions for personalization, Custom GPT creation and CV based solution. Within this documentation, you will discover comprehensive guidance on integrating the solutions, understanding its operational framework, and leveraging its capabilities within your application.

Let's have a rundown of the documentation:

Tutorial

To seamlessly integrate your product with our solutions, this section serves as your comprehensive guide. It provides a detailed, step-by-step walkthrough for integrating an API with our solutions. Additionally, you'll gain insights into how the solution function and discover strategies for effectively building your application with it. For an optimal integration process, it is advisable to thoroughly review this section prior to integrating your product.

Get started

API Reference

This section offers developer-friendly API documentation designed to guide you through accessing and utilizing the recommender API effectively.

API Reference

Next
Account and Project creation
Made with Material for MkDocs
================================================================================
